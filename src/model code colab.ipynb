{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 907
        },
        "id": "BOpmp1ADUkak",
        "outputId": "988fd2e3-7b54-4445-93b3-e165e55f5f1c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python: 3.12.11 (main, Jun  4 2025, 08:56:18) [GCC 11.4.0]\n",
            "PyTorch: 2.8.0+cu126\n",
            "CUDA available: True\n",
            "CUDA device: Tesla T4\n",
            "Mounted at /content/drive\n",
            "Project root: /content/nmt_urdu_roman\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m60.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[33m  DEPRECATION: Building 'pandarallel' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'pandarallel'. Discussion can be found at https://github.com/pypa/pip/issues/6334\u001b[0m\u001b[33m\n",
            "\u001b[0m  Building wheel for pandarallel (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Seed fixed to 42\n",
            "âš ï¸ Could not find your local poet folders in Drive.\n",
            "Attempting to clone GitHub dataset into project data ...\n",
            "Saved config: /content/nmt_urdu_roman/project_config.json\n",
            "Found 3 poet folders (showing up to 30):\n",
            " - .git\n",
            " - dataset\n",
            " - sample_dataset\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2607418669.py:112: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  \"created_at\": datetime.utcnow().isoformat() + \"Z\",\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'caas_jupyter_tools'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2607418669.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    156\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msamples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATASET_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpoets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nâœ… Setup complete. If the scan table opened, review the paths & sizes.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2607418669.py\u001b[0m in \u001b[0;36msample_files\u001b[0;34m(base, poets_list, n_per_poet)\u001b[0m\n\u001b[1;32m    150\u001b[0m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m         \u001b[0;32mfrom\u001b[0m \u001b[0mcaas_jupyter_tools\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdisplay_dataframe_to_user\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m         \u001b[0mdisplay_dataframe_to_user\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Dataset quick scan\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'caas_jupyter_tools'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "# ============================================================\n",
        "# Colab Cell #1 â€” Setup, Mount, Paths, Installs, Quick Scan\n",
        "# ============================================================\n",
        "import sys, os, re, random, json, math, unicodedata, shutil, glob\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "\n",
        "# --------------------------\n",
        "# 0) Basic environment info\n",
        "# --------------------------\n",
        "print(\"Python:\", sys.version)\n",
        "try:\n",
        "    import torch\n",
        "    print(\"PyTorch:\", torch.__version__)\n",
        "    print(\"CUDA available:\", torch.cuda.is_available())\n",
        "    if torch.cuda.is_available():\n",
        "        print(\"CUDA device:\", torch.cuda.get_device_name(0))\n",
        "except Exception as e:\n",
        "    print(\"Torch not yet available:\", e)\n",
        "\n",
        "# --------------------------\n",
        "# 1) Mount Google Drive\n",
        "# --------------------------\n",
        "IN_COLAB = \"google.colab\" in sys.modules\n",
        "if IN_COLAB:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "    ROOT = Path(\"/content\")\n",
        "    GDRIVE = Path(\"/content/drive/MyDrive\")\n",
        "else:\n",
        "    # Fallback for local dev (optional)\n",
        "    ROOT = Path.cwd()\n",
        "    GDRIVE = ROOT\n",
        "\n",
        "PROJECT_DIR = ROOT / \"nmt_urdu_roman\"\n",
        "for p in [\"data\",\"artifacts\",\"models\",\"logs\",\"src\",\"runs\"]:\n",
        "    (PROJECT_DIR / p).mkdir(parents=True, exist_ok=True)\n",
        "print(\"Project root:\", PROJECT_DIR)\n",
        "\n",
        "# --------------------------\n",
        "# 2) Pip installs\n",
        "# --------------------------\n",
        "# Keep installs minimal in first cell; add others later when needed\n",
        "!pip -q install --upgrade pip\n",
        "!pip -q install sacrebleu==2.4.2 jiwer==3.0.4 python-Levenshtein==0.25.1 sentencepiece==0.2.0 pyarrow==17.0.0 pandas==2.2.2 pandarallel==1.6.5\n",
        "\n",
        "import pandas as pd\n",
        "import sacrebleu, sentencepiece as spm\n",
        "from jiwer import cer\n",
        "import Levenshtein\n",
        "\n",
        "# --------------------------\n",
        "# 3) Reproducibility\n",
        "# --------------------------\n",
        "import numpy as np\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "try:\n",
        "    import torch\n",
        "    torch.manual_seed(SEED)\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "except:\n",
        "    pass\n",
        "print(\"Seed fixed to\", SEED)\n",
        "\n",
        "# --------------------------\n",
        "# 4) Dataset wiring\n",
        "# --------------------------\n",
        "# >>>> IMPORTANT: Set this to your Drive path where poet folders live <<<<\n",
        "# Example: /content/drive/MyDrive/marwah/dataset\n",
        "CANDIDATE_PATHS = [\n",
        "    GDRIVE / \"marwah\" / \"dataset\",\n",
        "    GDRIVE / \"dataset\" / \"urdu_ghazals_rekhta\",\n",
        "    PROJECT_DIR / \"data\" / \"urdu_ghazals_rekhta\"\n",
        "]\n",
        "\n",
        "DATASET_DIR = None\n",
        "for p in CANDIDATE_PATHS:\n",
        "    if p.exists() and any((p / d).exists() for d in [\n",
        "        \"ahmad-faraz\",\"akbar-allahabadi\",\"allama-iqbal\",\"ameer-khusrau\",\n",
        "        \"mirza-ghalib\",\"parveen-shakir\",\"faiz-ahmad-faiz\"\n",
        "    ]):\n",
        "        DATASET_DIR = p\n",
        "        break\n",
        "\n",
        "if DATASET_DIR is None:\n",
        "    print(\"âš ï¸ Could not find your local poet folders in Drive.\")\n",
        "    print(\"Attempting to clone GitHub dataset into project data ...\")\n",
        "    !rm -rf /content/urdu_ghazals_rekhta\n",
        "    !git clone -q https://github.com/amir9ume/urdu_ghazals_rekhta.git /content/urdu_ghazals_rekhta\n",
        "    src = Path(\"/content/urdu_ghazals_rekhta\")\n",
        "    if src.exists():\n",
        "        # try to locate poet folders\n",
        "        poet_dirs = [d for d in src.iterdir() if d.is_dir()]\n",
        "        if poet_dirs:\n",
        "            target = PROJECT_DIR / \"data\" / \"urdu_ghazals_rekhta\"\n",
        "            target.mkdir(parents=True, exist_ok=True)\n",
        "            # Copy structure lightly to our data dir\n",
        "            for d in poet_dirs:\n",
        "                shutil.copytree(d, target / d.name, dirs_exist_ok=True)\n",
        "            DATASET_DIR = target\n",
        "        else:\n",
        "            print(\"âŒ Clone succeeded but poet directories not found. Please set DATASET_DIR manually.\")\n",
        "else:\n",
        "    print(\"âœ… Found dataset at:\", DATASET_DIR)\n",
        "\n",
        "# Persist a config file\n",
        "cfg = {\n",
        "    \"created_at\": datetime.utcnow().isoformat() + \"Z\",\n",
        "    \"seed\": SEED,\n",
        "    \"dataset_dir\": str(DATASET_DIR) if DATASET_DIR else None,\n",
        "    \"device\": \"cuda\" if (\"torch\" in sys.modules and torch.cuda.is_available()) else \"cpu\",\n",
        "    \"project_dir\": str(PROJECT_DIR)\n",
        "}\n",
        "with open(PROJECT_DIR / \"project_config.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(cfg, f, indent=2, ensure_ascii=False)\n",
        "print(\"Saved config:\", PROJECT_DIR / \"project_config.json\")\n",
        "\n",
        "# --------------------------\n",
        "# 5) Quick scan of poets\n",
        "# --------------------------\n",
        "def list_poets(base: Path, limit=30):\n",
        "    if base is None or not base.exists():\n",
        "        print(\"Dataset path not set. Please update CANDIDATE_PATHS.\")\n",
        "        return []\n",
        "    poets = sorted([d.name for d in base.iterdir() if d.is_dir()])\n",
        "    print(f\"Found {len(poets)} poet folders (showing up to {limit}):\")\n",
        "    for name in poets[:limit]:\n",
        "        print(\" -\", name)\n",
        "    return poets\n",
        "\n",
        "poets = list_poets(DATASET_DIR)\n",
        "\n",
        "# Probe a few files inside first 2 poets to understand file patterns\n",
        "def sample_files(base: Path, poets_list, n_per_poet=3):\n",
        "    samples = []\n",
        "    for poet in poets_list[:2]:  # keep light\n",
        "        folder = base / poet\n",
        "        if not folder.exists():\n",
        "            continue\n",
        "        files = sorted([*folder.rglob(\"*\")])\n",
        "        text_like = [p for p in files if p.suffix.lower() in (\".txt\", \".csv\", \".json\", \".md\")]\n",
        "        chosen = text_like[:n_per_poet] if text_like else files[:n_per_poet]\n",
        "        for f in chosen:\n",
        "            size_kb = os.path.getsize(f) / 1024.0\n",
        "            samples.append({\"poet\": poet, \"path\": str(f), \"size_kb\": round(size_kb, 2)})\n",
        "    df = pd.DataFrame(samples)\n",
        "    if not df.empty:\n",
        "        from caas_jupyter_tools import display_dataframe_to_user\n",
        "        display_dataframe_to_user(\"Dataset quick scan\", df)\n",
        "    else:\n",
        "        print(\"No text-like files found in first two poets; structure may be nested differently.\")\n",
        "    return samples\n",
        "\n",
        "_ = sample_files(DATASET_DIR, poets)\n",
        "\n",
        "print(\"\\nâœ… Setup complete. If the scan table opened, review the paths & sizes.\")\n",
        "print(\"If your dataset lives elsewhere in Drive, update CANDIDATE_PATHS above and re-run this cell.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install --force-reinstall \"sentencepiece==0.1.99\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xjZfKo2ej0JY",
        "outputId": "6e378b7c-acab-480f-ac87-32de883c3222"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[33m  DEPRECATION: Building 'sentencepiece' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'sentencepiece'. Discussion can be found at https://github.com/pypa/pip/issues/6334\u001b[0m\u001b[33m\n",
            "\u001b[0m  Building wheel for sentencepiece (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Colab Cell #1B â€” Point to /MyDrive/dataset + safe quick scan\n",
        "# ============================================================\n",
        "import json, os\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "\n",
        "# If you changed your folder name, update this path:\n",
        "DATASET_DIR = Path(\"/content/drive/MyDrive/dataset\")\n",
        "\n",
        "assert DATASET_DIR.exists(), f\"Dataset path not found: {DATASET_DIR}\"\n",
        "\n",
        "# Basic sanity: look for a few poet folders\n",
        "EXPECTED = {\n",
        "    \"ahmad-faraz\",\"akbar-allahabadi\",\"allama-iqbal\",\"altaf-hussain-hali\",\n",
        "    \"ameer-khusrau\",\"bahadur-shah-zafar\",\"dagh-dehlvi\",\"fahmida-riaz\",\n",
        "    \"faiz-ahmad-faiz\",\"firaq-gorakhpuri\",\"gulzar\",\"habib-jalib\",\n",
        "    \"jaan-nisar-akhtar\",\"jaun-eliya\",\"javed-akhtar\",\"meer-taqi-meer\",\n",
        "    \"mirza-ghalib\",\"parveen-shakir\"\n",
        "}\n",
        "present = {d.name for d in DATASET_DIR.iterdir() if d.is_dir()}\n",
        "print(f\"âœ… DATASET_DIR set to: {DATASET_DIR}\")\n",
        "print(f\"Found {len(present)} top-level folders.\")\n",
        "\n",
        "missing = sorted(list(EXPECTED - present))\n",
        "if missing:\n",
        "    print(\"Note: some expected poet folders not found (ok if your dump differs). Example missing:\", missing[:8])\n",
        "\n",
        "# Show a tiny table-like listing (no special display tools needed)\n",
        "def sample_files(base: Path, n_per_poet=3, poets_limit=5):\n",
        "    rows = []\n",
        "    poets = sorted([d for d in base.iterdir() if d.is_dir()])[:poets_limit]\n",
        "    for poet_dir in poets:\n",
        "        files = sorted([*poet_dir.rglob(\"*\")])\n",
        "        text_like = [p for p in files if p.suffix.lower() in (\".txt\", \".csv\", \".json\", \".md\")]\n",
        "        chosen = text_like[:n_per_poet] if text_like else files[:n_per_poet]\n",
        "        for f in chosen:\n",
        "            try:\n",
        "                size_kb = os.path.getsize(f) / 1024.0\n",
        "            except Exception:\n",
        "                size_kb = -1\n",
        "            rows.append((poet_dir.name, str(f.relative_to(base)), round(size_kb, 2)))\n",
        "    print(\"\\nSample of discovered files:\")\n",
        "    print(\"poet\\t\\trelative_path\\t\\tsize_kb\")\n",
        "    for poet, rel, sz in rows[:30]:\n",
        "        print(f\"{poet}\\t{rel}\\t{sz}\")\n",
        "\n",
        "sample_files(DATASET_DIR)\n",
        "\n",
        "# Persist the config with the corrected path\n",
        "PROJECT_DIR = Path(\"/content/nmt_urdu_roman\")\n",
        "cfg_path = PROJECT_DIR / \"project_config.json\"\n",
        "try:\n",
        "    with open(cfg_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        cfg = json.load(f)\n",
        "except Exception:\n",
        "    cfg = {}\n",
        "\n",
        "cfg.update({\n",
        "    \"dataset_dir\": str(DATASET_DIR),\n",
        "    \"updated_at\": datetime.utcnow().isoformat() + \"Z\",\n",
        "})\n",
        "with open(cfg_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(cfg, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "print(\"\\nğŸ”§ Config updated at:\", cfg_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u4BukGwZVCYZ",
        "outputId": "98deb798-e0ac-4acd-ed6d-0c6d48e8c229"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… DATASET_DIR set to: /content/drive/MyDrive/dataset\n",
            "Found 30 top-level folders.\n",
            "\n",
            "Sample of discovered files:\n",
            "poet\t\trelative_path\t\tsize_kb\n",
            "ahmad-faraz\tahmad-faraz/.DS_Store\t6.0\n",
            "ahmad-faraz\tahmad-faraz/en\t4.0\n",
            "ahmad-faraz\tahmad-faraz/en/aankh-se-duur-na-ho-dil-se-utar-jaaegaa-ahmad-faraz-ghazals\t0.46\n",
            "akbar-allahabadi\takbar-allahabadi/en\t4.0\n",
            "akbar-allahabadi\takbar-allahabadi/en/aah-jo-dil-se-nikaalii-jaaegii-akbar-allahabadi-ghazals\t0.38\n",
            "akbar-allahabadi\takbar-allahabadi/en/aaj-aaraaish-e-gesuu-e-dotaa-hotii-hai-akbar-allahabadi-ghazals\t1.22\n",
            "allama-iqbal\tallama-iqbal/.DS_Store\t6.0\n",
            "allama-iqbal\tallama-iqbal/en\t4.0\n",
            "allama-iqbal\tallama-iqbal/en/aalam-e-aab-o-khaak-o-baad-sirr-e-ayaan-hai-tuu-ki-main-allama-iqbal-ghazals\t0.46\n",
            "altaf-hussain-hali\taltaf-hussain-hali/en\t4.0\n",
            "altaf-hussain-hali\taltaf-hussain-hali/en/aage-badhe-na-qissa-e-ishq-e-butaan-se-ham-altaf-hussain-hali-ghazals-3\t0.48\n",
            "altaf-hussain-hali\taltaf-hussain-hali/en/ab-vo-aglaa-saa-iltifaat-nahiin-altaf-hussain-hali-ghazals\t0.49\n",
            "ameer-khusrau\tameer-khusrau/en\t4.0\n",
            "ameer-khusrau\tameer-khusrau/en/ze-haal-e-miskiin-makun-tagaaful-duraae-nainaan-banaae-batiyaan-ameer-khusrau-ghazals\t0.7\n",
            "ameer-khusrau\tameer-khusrau/hi\t4.0\n",
            "\n",
            "ğŸ”§ Config updated at: /content/nmt_urdu_roman/project_config.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2404578599.py:61: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  \"updated_at\": datetime.utcnow().isoformat() + \"Z\",\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Colab Cell #2 â€” Parse Rekhta dump â†’ Urdu/Roman pairs (raw)\n",
        "# ============================================================\n",
        "from pathlib import Path\n",
        "import os, io, re, json, unicodedata, itertools\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Load config written earlier\n",
        "import json\n",
        "cfg_path = Path(\"/content/nmt_urdu_roman/project_config.json\")\n",
        "with open(cfg_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    CFG = json.load(f)\n",
        "PROJECT_DIR = Path(CFG[\"project_dir\"])\n",
        "DATASET_DIR = Path(CFG[\"dataset_dir\"])\n",
        "OUT_DIR = PROJECT_DIR / \"data\"\n",
        "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(\"Using DATASET_DIR =\", DATASET_DIR)\n",
        "\n",
        "# --------------------------\n",
        "# 1) Normalization helpers\n",
        "# --------------------------\n",
        "ZW_CHARS = \"\".join([\n",
        "    \"\\u200b\", \"\\u200c\", \"\\u200d\", \"\\ufeff\"  # ZW space, joiners, BOM\n",
        "])\n",
        "\n",
        "URDU_MAP = {\n",
        "    # Common confusables â†’ canonical\n",
        "    \"ÙŠ\": \"ÛŒ\",   # Arabic yeh â†’ Farsi yeh\n",
        "    \"Ùƒ\": \"Ú©\",   # Arabic kaf â†’ Farsi kaf\n",
        "    \"Û\": \"Û\",   # already Urdu heh-doachashmee\n",
        "    \"Û€\": \"Û\",   # heh with hamza â†’ heh\n",
        "    \"Ú¾\": \"Û\",   # heh goal â†’ heh (approx)\n",
        "    \"Ûƒ\": \"Û\",\n",
        "    \"Ø£\": \"Ø§\", \"Ø¥\": \"Ø§\", \"Ø¢\": \"Ø¢\", \"Ù±\": \"Ø§\",\n",
        "    \"Ø¤\": \"Ùˆ\", \"Ø¦\": \"ÛŒ\",\n",
        "    \"Ù”\": \"\", \"Ù°\": \"\", \"ÙŒ\": \"\", \"Ù‹\": \"\", \"Ù\": \"\", \"Ù’\": \"\", \"Ù‘\": \"\",  # remove tashkeel\n",
        "    \"Ù€\": \"\",  # tatweel\n",
        "}\n",
        "\n",
        "PUNCT_KEEP = set(list(\"ØŒØ›ØŸ!.,:;!?â€¦â€”â€“-()[]{}\\\"'`â€™â€œâ€Â«Â»\"))\n",
        "\n",
        "def normalize_urdu(s: str) -> str:\n",
        "    if not isinstance(s, str): return \"\"\n",
        "    s = unicodedata.normalize(\"NFC\", s)\n",
        "    # Remove zero-width & BOM\n",
        "    s = re.sub(f\"[{re.escape(ZW_CHARS)}]\", \"\", s)\n",
        "    # Map confusables\n",
        "    s = \"\".join(URDU_MAP.get(ch, ch) for ch in s)\n",
        "    # Collapse spaces\n",
        "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
        "    return s\n",
        "\n",
        "def normalize_roman(s: str) -> str:\n",
        "    if not isinstance(s, str): return \"\"\n",
        "    s = unicodedata.normalize(\"NFC\", s)\n",
        "    s = s.replace(\"â€™\", \"'\").replace(\"â€œ\", '\"').replace(\"â€\", '\"')\n",
        "    s = re.sub(f\"[{re.escape(ZW_CHARS)}]\", \"\", s)\n",
        "    # unify long vowels a bit (gentle)\n",
        "    s = re.sub(r\"\\baa\\b\", \"aa\", s, flags=re.I)\n",
        "    s = re.sub(r\"\\bee\\b\", \"ee\", s, flags=re.I)\n",
        "    s = re.sub(r\"\\boo\\b\", \"oo\", s, flags=re.I)\n",
        "    # collapse spaces\n",
        "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
        "    return s\n",
        "\n",
        "def looks_like_title(line: str) -> bool:\n",
        "    # Heuristic: titles are often long slugs or very short with no Urdu letters\n",
        "    urdu_chars = re.findall(r\"[\\u0600-\\u06FF]\", line)\n",
        "    if not line: return True\n",
        "    if len(urdu_chars) == 0 and len(line.split()) <= 3:\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "# --------------------------\n",
        "# 2) File readers\n",
        "# --------------------------\n",
        "def read_text_file(p: Path) -> list[str]:\n",
        "    \"\"\"Read a text file with unknown extension; try utf-8 then latin-1 as fallback.\"\"\"\n",
        "    try_enc = [\"utf-8\", \"utf-8-sig\", \"cp1256\", \"latin-1\"]\n",
        "    for enc in try_enc:\n",
        "        try:\n",
        "            with open(p, \"r\", encoding=enc, errors=\"ignore\") as f:\n",
        "                raw = f.read()\n",
        "            break\n",
        "        except Exception:\n",
        "            continue\n",
        "    else:\n",
        "        return []\n",
        "\n",
        "    lines = [ln.strip() for ln in raw.splitlines()]\n",
        "    # remove empty & very short decoration lines\n",
        "    lines = [ln for ln in lines if ln.strip() != \"\"]\n",
        "    return lines\n",
        "\n",
        "def collect_language_folder(poet_dir: Path, lang_code: str) -> dict[str, list[str]]:\n",
        "    \"\"\"\n",
        "    Returns a dict: { ghazal_key -> list_of_lines } for given language subfolder.\n",
        "    lang_code in {\"ur\",\"en\",\"hi\"} (en ~ English transliteration ~ Roman Urdu).\n",
        "    Handles files WITH or WITHOUT extensions.\n",
        "    \"\"\"\n",
        "    base = poet_dir / lang_code\n",
        "    if not base.exists():\n",
        "        return {}\n",
        "    files = sorted([p for p in base.rglob(\"*\") if p.is_file()])\n",
        "    out = {}\n",
        "    for f in files:\n",
        "        key = f.name  # keep full filename (with/without ext) to maximize pairing\n",
        "        # Also allow pairing by stem if we need later\n",
        "        lines = read_text_file(f)\n",
        "        # remove leading title-ish junk line if it looks like a header\n",
        "        if lines and looks_like_title(lines[0]) and len(lines) > 1:\n",
        "            lines = lines[1:]\n",
        "        out[key] = lines\n",
        "    return out\n",
        "\n",
        "# --------------------------\n",
        "# 3) Pairing Urdu â†” Roman per ghazal file\n",
        "# --------------------------\n",
        "def align_lines(ur_lines: list[str], rom_lines: list[str]) -> list[tuple[str, str]]:\n",
        "    \"\"\"\n",
        "    Align lines by index. If lengths differ, cut to min length.\n",
        "    Apply normalization per line.\n",
        "    Drop pairs that become empty after normalization.\n",
        "    \"\"\"\n",
        "    n = min(len(ur_lines), len(rom_lines))\n",
        "    pairs = []\n",
        "    for i in range(n):\n",
        "        ur = normalize_urdu(ur_lines[i])\n",
        "        rom = normalize_roman(rom_lines[i])\n",
        "        # filter trivial separators\n",
        "        if ur and rom and not re.fullmatch(r\"[-â€“â€”â€¦\\.\\*]+\", ur) and not re.fullmatch(r\"[-â€“â€”â€¦\\.\\*]+\", rom):\n",
        "            pairs.append((ur, rom))\n",
        "    return pairs\n",
        "\n",
        "def best_key_match(key: str, candidates: set[str]) -> str | None:\n",
        "    \"\"\"\n",
        "    Try exact key, then stem match, then relaxed slug match.\n",
        "    \"\"\"\n",
        "    if key in candidates:\n",
        "        return key\n",
        "    stem = Path(key).stem\n",
        "    by_stem = {Path(c).stem: c for c in candidates}\n",
        "    if stem in by_stem:\n",
        "        return by_stem[stem]\n",
        "    # relaxed: remove trailing numerals like -1/-2, etc.\n",
        "    stem_relaxed = re.sub(r\"[-_]{0,1}\\d+$\", \"\", stem)\n",
        "    for s, full in by_stem.items():\n",
        "        if re.sub(r\"[-_]{0,1}\\d+$\", \"\", s) == stem_relaxed:\n",
        "            return full\n",
        "    return None\n",
        "\n",
        "# --------------------------\n",
        "# 4) Walk poets and build a dataframe of pairs\n",
        "# --------------------------\n",
        "rows = []\n",
        "poets = sorted([d for d in DATASET_DIR.iterdir() if d.is_dir()])\n",
        "\n",
        "for poet_dir in tqdm(poets, desc=\"Poets\"):\n",
        "    poet = poet_dir.name\n",
        "    ur_map = collect_language_folder(poet_dir, \"ur\")\n",
        "    en_map = collect_language_folder(poet_dir, \"en\")  # Roman Urdu (English transliteration)\n",
        "    if not en_map:\n",
        "        # Some dumps may use 'roman' instead of 'en'; try that too\n",
        "        en_map = collect_language_folder(poet_dir, \"roman\")\n",
        "    if not ur_map or not en_map:\n",
        "        # Keep note but continue\n",
        "        print(f\"Note: skipping '{poet}' (ur={len(ur_map)}, en/roman={len(en_map)})\")\n",
        "        continue\n",
        "\n",
        "    en_keys = set(en_map.keys())\n",
        "    for key_ur, ur_lines in ur_map.items():\n",
        "        key_en = best_key_match(key_ur, en_keys)\n",
        "        if key_en is None:\n",
        "            # attempt match by stem across all en files quickly\n",
        "            continue\n",
        "        rom_lines = en_map.get(key_en, [])\n",
        "        pairs = align_lines(ur_lines, rom_lines)\n",
        "        ghazal_id = Path(key_ur).stem\n",
        "        for idx, (src_ur, tgt_rom) in enumerate(pairs):\n",
        "            rows.append({\n",
        "                \"poet\": poet,\n",
        "                \"ghazal_id\": ghazal_id,\n",
        "                \"line_idx\": idx,\n",
        "                \"src_ur\": src_ur,\n",
        "                \"tgt_rom_gold\": tgt_rom,  # gold transliteration when present\n",
        "                \"tgt_rom_rule\": None,     # placeholder (weâ€™ll fill later if needed)\n",
        "            })\n",
        "\n",
        "df = pd.DataFrame(rows)\n",
        "print(\"\\nCollected pairs:\", len(df))\n",
        "print(\"Unique poets parsed:\", df[\"poet\"].nunique() if not df.empty else 0)\n",
        "\n",
        "# Basic sanity stats\n",
        "if not df.empty:\n",
        "    lens_src = df[\"src_ur\"].str.len()\n",
        "    lens_tgt = df[\"tgt_rom_gold\"].str.len()\n",
        "    print(\"Avg src len:\", round(lens_src.mean(), 1), \"| 95p:\", int(lens_src.quantile(0.95)))\n",
        "    print(\"Avg tgt len:\", round(lens_tgt.mean(), 1), \"| 95p:\", int(lens_tgt.quantile(0.95)))\n",
        "\n",
        "# Save raw (gold-present) pairs\n",
        "RAW_PATH = OUT_DIR / \"train_raw_gold.parquet\"\n",
        "if not df.empty:\n",
        "    df.to_parquet(RAW_PATH, index=False)\n",
        "    print(\"âœ… Saved:\", RAW_PATH)\n",
        "else:\n",
        "    print(\"âš ï¸ No aligned pairs found yet. We may need to handle alternative layouts.\")\n",
        "\n",
        "# Peek a few rows\n",
        "print(\"\\nSample rows:\")\n",
        "print(df.sample(min(10, len(df)), random_state=42) if not df.empty else \"No data\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "78dmXRQMYQoV",
        "outputId": "691eaba7-970c-42ad-aae4-f8db28b3de33"
      },
      "execution_count": 4,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using DATASET_DIR = /content/drive/MyDrive/dataset\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Poets: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [27:45<00:00, 55.53s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Collected pairs: 20983\n",
            "Unique poets parsed: 30\n",
            "Avg src len: 33.2 | 95p: 43\n",
            "Avg tgt len: 40.3 | 95p: 52\n",
            "âœ… Saved: /content/nmt_urdu_roman/data/train_raw_gold.parquet\n",
            "\n",
            "Sample rows:\n",
            "                     poet                                          ghazal_id  \\\n",
            "10172        javed-akhtar  mujh-ko-yaqiin-hai-sach-kahtii-thiin-jo-bhii-a...   \n",
            "14968        mirza-ghalib  maze-jahaan-ke-apnii-nazar-men-khaak-nahiin-mi...   \n",
            "13406        mirza-ghalib  dost-gam-khvaarii-men-merii-saii-farmaavenge-k...   \n",
            "3211   bahadur-shah-zafar  khvaah-kar-insaaf-zaalim-khvaah-kar-bedaad-tuu...   \n",
            "6914     firaq-gorakhpuri  samajhtaa-huun-ki-tuu-mujh-se-judaa-hai-firaq-...   \n",
            "18471          nida-fazli  safar-men-dhuup-to-hogii-jo-chal-sako-to-chalo...   \n",
            "11114    jigar-moradabadi  kuchh-is-adaa-se-aaj-vo-pahluu-nashiin-rahe-ji...   \n",
            "3725   bahadur-shah-zafar  zulf-jo-rukh-par-tire-ai-mehr-e-talat-khul-gai...   \n",
            "19854  wali-mohammad-wali  aaj-distaa-hai-haal-kuchh-kaa-kuchh-wali-moham...   \n",
            "10594    jigar-moradabadi  allaah-agar-taufiiq-na-de-insaan-ke-bas-kaa-ka...   \n",
            "\n",
            "       line_idx                                          src_ur  \\\n",
            "10172        12  Ø§ÛŒÚ© ÛŒÛ Ú¯ÛØ± Ø¬Ø³ Ú¯ÛØ± Ù…ÛŒÚº Ù…ÛŒØ±Ø§ Ø³Ø§Ø² Ùˆ Ø³Ø§Ù…Ø§Úº Ø±ÛØªØ§ ÛÛ’   \n",
            "14968         0               Ù…Ø²Û’ Ø¬ÛØ§Ù† Ú©Û’ Ø§Ù¾Ù†ÛŒ Ù†Ø¸Ø± Ù…ÛŒÚº Ø®Ø§Ú© Ù†ÛÛŒÚº   \n",
            "13406         4             Ø­Ø¶Ø±Øª Ù†Ø§ØµØ­ Ú¯Ø± Ø¢ÙˆÛŒÚº Ø¯ÛŒØ¯Û Ùˆ Ø¯Ù„ ÙØ±Ø´ Ø±Ø§Û   \n",
            "3211          1        Ù¾Ø± Ø¬Ùˆ ÙØ±ÛŒØ§Ø¯ÛŒ ÛÛŒÚº Ø§Ù† Ú©ÛŒ Ø³Ù† ØªÙˆ Ù„Û’ ÙØ±ÛŒØ§Ø¯ ØªÙˆ   \n",
            "6914         21                      Ú©ÙˆÛŒÛŒ Ø§Ø³ Ø±Ù†Ú¯ Ø³Û’ Ø´Ø±Ù…Ø§ Ø±ÛØ§ ÛÛ’   \n",
            "18471         1         Ø³Ø¨ÛÛŒ ÛÛŒÚº Ø¨ÛÛŒÚ‘ Ù…ÛŒÚº ØªÙ… Ø¨ÛÛŒ Ù†Ú©Ù„ Ø³Ú©Ùˆ ØªÙˆ Ú†Ù„Ùˆ   \n",
            "11114        32                    Ø§Ø³ Ø¹Ø´Ù‚ Ú©ÛŒ ØªÙ„Ø§ÙÛŒ Ù…Ø§ÙØ§Øª Ø¯ÛŒÚ©ÛÙ†Ø§   \n",
            "3725         13           Ø§Ø¨ Ú¯Ø±Û Ø¯Ù„ Ú©ÛŒ ÛÙ…Ø§Ø±Û’ ÙÛŒ Ø§Ù„Ø­Ù‚ÛŒÙ‚Øª Ú©ÛÙ„ Ú¯ÛŒÛŒ   \n",
            "19854         8                       Ø§Û’ ÙˆÙ„ÛŒØ” Ø¯Ù„ Ú©ÙˆÚº Ø¢Ø¬ Ú©Ø±ØªÛŒ ÛÛ’   \n",
            "10594         2   ÛŒÛ ØªÙˆ Ù†Û’ Ú©ÛØ§ Ú©ÛŒØ§ Ø§Û’ Ù†Ø§Ø¯Ø§Úº ÙÛŒØ§Ø¶ÛŒ Ù‚Ø¯Ø±Øª Ø¹Ø§Ù… Ù†ÛÛŒÚº   \n",
            "\n",
            "                                            tgt_rom_gold tgt_rom_rule  \n",
            "10172  ek ye ghar jis ghar meÃ± merÄ sÄz-o-sÄmÄÃ± rahtÄ...         None  \n",
            "14968           maze jahÄn ke apnÄ« nazar meÃ± á¸³haak nahÄ«Ã±         None  \n",
            "13406   hazrat-e-nÄseh gar aaveÃ± diida o dil farsh-e-rÄh         None  \n",
            "3211     par jo fariyÄdÄ« haiÃ± un kÄ« sun to le fariyÄd tÅ«         None  \n",
            "6914                      koÄ« is raÃ±g se sharmÄ rahÄ hai         None  \n",
            "18471   sabhÄ« haiÃ± bhiiá¸Œ meÃ± tum bhÄ« nikal sako to chalo         None  \n",
            "11114                   is ishq kÄ« talÄfi-e-mÄfÄt dekhnÄ         None  \n",
            "3725        ab girah dil kÄ« hamÄre fil-haqÄ«qat khul ga.Ä«         None  \n",
            "19854                   ai 'valÄ«' dil kuuÃ± aaj kartÄ« hai         None  \n",
            "10594  ye tÅ« ne kahÄ kyÄ ai nÄdÄÃ± fayyÄzÄ«-e-qudrat aa...         None  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Colab Cell #3 â€” Rule translit, unified corpus, 50/25/25 split,\n",
        "#                  char vocabs saved to artifacts/\n",
        "# ============================================================\n",
        "from pathlib import Path\n",
        "import json, re, math, random\n",
        "import pandas as pd\n",
        "from collections import Counter, defaultdict\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load config and paths\n",
        "cfg = json.load(open(\"/content/nmt_urdu_roman/project_config.json\", \"r\", encoding=\"utf-8\"))\n",
        "PROJECT_DIR = Path(cfg[\"project_dir\"])\n",
        "DATA_DIR = PROJECT_DIR / \"data\"\n",
        "ARTI = PROJECT_DIR / \"artifacts\"\n",
        "ARTI.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "RAW_GOLD = DATA_DIR / \"train_raw_gold.parquet\"\n",
        "assert RAW_GOLD.exists(), f\"Missing: {RAW_GOLD}\"\n",
        "df = pd.read_parquet(RAW_GOLD)\n",
        "print(\"Loaded gold pairs:\", len(df))\n",
        "\n",
        "# --------------------------\n",
        "# 1) Rule-based Urduâ†’Roman\n",
        "# --------------------------\n",
        "# This is a light transliteration (not phonetic-perfect), good as a fallback.\n",
        "UR2ROM = {\n",
        "    \"Ø§\":\"a\", \"Ø¢\":\"aa\", \"Ø£\":\"a\", \"Ø¥\":\"i\", \"Ù±\":\"a\", \"Ø¡\":\"'\", \"Ø¤\":\"o\", \"Ø¦\":\"i\",\n",
        "    \"Ø¨\":\"b\", \"Ù¾\":\"p\", \"Øª\":\"t\", \"Ù¹\":\"t\", \"Ø«\":\"s\", \"Ø¬\":\"j\", \"Ú†\":\"ch\", \"Ø­\":\"h\",\n",
        "    \"Ø®\":\"kh\", \"Ø¯\":\"d\", \"Úˆ\":\"d\", \"Ø°\":\"z\", \"Ø±\":\"r\", \"Ú‘\":\"r\", \"Ø²\":\"z\", \"Ú˜\":\"zh\",\n",
        "    \"Ø³\":\"s\", \"Ø´\":\"sh\", \"Øµ\":\"s\", \"Ø¶\":\"z\", \"Ø·\":\"t\", \"Ø¸\":\"z\", \"Ø¹\":\"'\", \"Øº\":\"gh\",\n",
        "    \"Ù\":\"f\", \"Ù‚\":\"q\", \"Ú©\":\"k\", \"Ú¯\":\"g\", \"Ù„\":\"l\", \"Ù…\":\"m\", \"Ù†\":\"n\", \"Úº\":\"n\",\n",
        "    \"Ùˆ\":\"v\",  # will handle vowels via context rules below\n",
        "    \"Û\":\"h\", \"Ú¾\":\"h\", \"Ø¡\":\"'\", \"Ù”\":\"\", \"Ù°\":\"\", \"Ù‰\":\"a\", \"ÛŒ\":\"y\", \"Û’\":\"e\",\n",
        "    \"ØŒ\":\",\", \"Ø›\":\";\", \"ØŸ\":\"?\", \"Û”\":\".\", \"â€”\":\"-\", \"â€“\":\"-\", \"Ù’\":\"\", \"Ù‘\":\"\"\n",
        "}\n",
        "# Digits and spaces\n",
        "for d_ar, d_en in zip(\"Ù Ù¡Ù¢Ù£Ù¤Ù¥Ù¦Ù§Ù¨Ù©\", \"0123456789\"):\n",
        "    UR2ROM[d_ar] = d_en\n",
        "\n",
        "# Simple vowel context rules\n",
        "def urdu_to_roman_rule(text: str) -> str:\n",
        "    if not isinstance(text, str): return \"\"\n",
        "    out = []\n",
        "    for i, ch in enumerate(text):\n",
        "        rom = UR2ROM.get(ch)\n",
        "        if rom is None:\n",
        "            # Urdu whitespace / Latin punctuation passthrough\n",
        "            if re.match(r\"[\\u0600-\\u06FF]\", ch):\n",
        "                rom = \"\"  # unknown Urdu char â†’ empty\n",
        "            else:\n",
        "                rom = ch\n",
        "        out.append(rom)\n",
        "    s = \"\".join(out)\n",
        "\n",
        "    # Heuristics:\n",
        "    #   - handle long vowels around y/w (ÛŒ/Ùˆ became y/v)\n",
        "    #   - basic combos\n",
        "    s = re.sub(r\"\\bkh\", \"kh\", s)  # keep\n",
        "    s = re.sub(r\"gh\", \"gh\", s)\n",
        "\n",
        "    # Normalize multiple apostrophes/spaces\n",
        "    s = re.sub(r\"'+\", \"'\", s)\n",
        "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
        "\n",
        "    # Light tidy for v/w and y/i/u heuristics (very rough):\n",
        "    s = re.sub(r\"\\bv\", \"w\", s)  # often Ùˆ is 'w' in roman urdu\n",
        "    s = re.sub(r\"([aeiou])y\\b\", r\"\\1i\", s)\n",
        "    return s\n",
        "\n",
        "# Fill rule transliteration only where gold missing (future-proof)\n",
        "needs_rule = df[\"tgt_rom_gold\"].isna() | df[\"tgt_rom_gold\"].eq(\"\")\n",
        "if needs_rule.any():\n",
        "    df.loc[needs_rule, \"tgt_rom_rule\"] = df.loc[needs_rule, \"src_ur\"].apply(urdu_to_roman_rule)\n",
        "\n",
        "# Choose final target: prefer gold, else rule\n",
        "df[\"tgt_rom\"] = df[\"tgt_rom_gold\"].fillna(\"\").replace(\"\", pd.NA)\n",
        "df[\"tgt_rom\"] = df[\"tgt_rom\"].fillna(df[\"tgt_rom_rule\"].fillna(\"\"))\n",
        "\n",
        "# Drop empty/degenerate rows\n",
        "before = len(df)\n",
        "df = df[(df[\"src_ur\"].str.len() >= 2) & (df[\"tgt_rom\"].str.len() >= 2)]\n",
        "df = df[~df[\"src_ur\"].str.fullmatch(r\"[-â€“â€”â€¦\\.\\*]+\")]\n",
        "df = df[~df[\"tgt_rom\"].str.fullmatch(r\"[-â€“â€”â€¦\\.\\*]+\")]\n",
        "after = len(df)\n",
        "print(f\"Filtered: {before} â†’ {after}\")\n",
        "\n",
        "# --------------------------\n",
        "# 2) Ghazal-level split (50/25/25)\n",
        "#    to avoid same ghazal leaking across sets\n",
        "# --------------------------\n",
        "key = df[\"poet\"].astype(str) + \"Â§\" + df[\"ghazal_id\"].astype(str)\n",
        "unique_gh = key.drop_duplicates().tolist()\n",
        "random.Random(42).shuffle(unique_gh)\n",
        "\n",
        "n = len(unique_gh)\n",
        "n_train = int(0.50 * n)\n",
        "n_val   = int(0.25 * n)\n",
        "train_gh = set(unique_gh[:n_train])\n",
        "val_gh   = set(unique_gh[n_train:n_train+n_val])\n",
        "test_gh  = set(unique_gh[n_train+n_val:])\n",
        "\n",
        "def assign_split(row):\n",
        "    k = f\"{row.poet}Â§{row.ghazal_id}\"\n",
        "    if k in train_gh: return \"train\"\n",
        "    if k in val_gh:   return \"val\"\n",
        "    return \"test\"\n",
        "\n",
        "df[\"split\"] = df.apply(assign_split, axis=1)\n",
        "\n",
        "print(df[\"split\"].value_counts())\n",
        "\n",
        "# Save split files\n",
        "for sp in [\"train\", \"val\", \"test\"]:\n",
        "    outp = DATA_DIR / f\"pairs_{sp}.parquet\"\n",
        "    df[df[\"split\"] == sp][[\"poet\",\"ghazal_id\",\"line_idx\",\"src_ur\",\"tgt_rom\"]].to_parquet(outp, index=False)\n",
        "    print(\"âœ… Saved:\", outp)\n",
        "\n",
        "# --------------------------\n",
        "# 3) Build character-level vocabs\n",
        "# --------------------------\n",
        "SPECIAL = [\"<pad>\", \"<s>\", \"</s>\", \"<unk>\"]\n",
        "\n",
        "def build_char_vocab(series: pd.Series, extra_chars=None):\n",
        "    counter = Counter()\n",
        "    for s in series.astype(str).tolist():\n",
        "        for ch in s:\n",
        "            counter[ch] += 1\n",
        "    chars = sorted(counter.keys())\n",
        "    if extra_chars:\n",
        "        for ch in extra_chars:\n",
        "            if ch not in chars: chars.append(ch)\n",
        "    # index mapping\n",
        "    itos = SPECIAL + chars\n",
        "    stoi = {ch:i for i,ch in enumerate(itos)}\n",
        "    meta = {\"size\": len(itos), \"num_special\": len(SPECIAL)}\n",
        "    return {\"itos\": itos, \"stoi\": stoi, \"meta\": meta}\n",
        "\n",
        "train_df = df[df[\"split\"]==\"train\"]\n",
        "\n",
        "src_vocab = build_char_vocab(train_df[\"src_ur\"])\n",
        "tgt_vocab = build_char_vocab(train_df[\"tgt_rom\"])\n",
        "\n",
        "with open(ARTI / \"vocab_src_char.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(src_vocab, f, ensure_ascii=False, indent=2)\n",
        "with open(ARTI / \"vocab_tgt_char.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(tgt_vocab, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "print(\"\\nVocab sizes â€” src:\", src_vocab[\"meta\"][\"size\"], \"tgt:\", tgt_vocab[\"meta\"][\"size\"])\n",
        "print(\"Special tokens:\", SPECIAL)\n",
        "\n",
        "# Quick peek\n",
        "print(\"\\nSamples:\")\n",
        "print(df[df[\"split\"]==\"train\"][[\"src_ur\",\"tgt_rom\"]].head(5))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FdkWFjv3YfPk",
        "outputId": "cc3fdf66-3916-45f6-ef04-a335a050c42d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded gold pairs: 20983\n",
            "Filtered: 20983 â†’ 20983\n",
            "split\n",
            "train    10493\n",
            "test      5255\n",
            "val       5235\n",
            "Name: count, dtype: int64\n",
            "âœ… Saved: /content/nmt_urdu_roman/data/pairs_train.parquet\n",
            "âœ… Saved: /content/nmt_urdu_roman/data/pairs_val.parquet\n",
            "âœ… Saved: /content/nmt_urdu_roman/data/pairs_test.parquet\n",
            "\n",
            "Vocab sizes â€” src: 50 tgt: 46\n",
            "Special tokens: ['<pad>', '<s>', '</s>', '<unk>']\n",
            "\n",
            "Samples:\n",
            "                                    src_ur  \\\n",
            "24      Ø§Ø¨ Ø§ÙˆØ± Ú©ÛŒØ§ Ú©Ø³ÛŒ Ø³Û’ Ù…Ø±Ø§Ø³Ù… Ø¨Ú‘ÛØ§ÛŒÛŒÚº ÛÙ…   \n",
            "25  ÛŒÛ Ø¨ÛÛŒ Ø¨ÛØª ÛÛ’ ØªØ¬Û Ú©Ùˆ Ø§Ú¯Ø± Ø¨ÛÙˆÙ„ Ø¬Ø§ÛŒÛŒÚº ÛÙ…   \n",
            "26      ØµØ­Ø±Ø§ÛŒÛ’ Ø²Ù†Ø¯Ú¯ÛŒ Ù…ÛŒÚº Ú©ÙˆÛŒÛŒ Ø¯ÙˆØ³Ø±Ø§ Ù†Û ØªÛØ§   \n",
            "27       Ø³Ù†ØªÛ’ Ø±ÛÛ’ ÛÛŒÚº Ø¢Ù¾ ÛÛŒ Ø§Ù¾Ù†ÛŒ ØµØ¯Ø§ÛŒÛŒÚº ÛÙ…   \n",
            "28        Ø§Ø³ Ø²Ù†Ø¯Ú¯ÛŒ Ù…ÛŒÚº Ø§ØªÙ†ÛŒ ÙØ±Ø§ØºØª Ú©Ø³Û’ Ù†ØµÛŒØ¨   \n",
            "\n",
            "                                           tgt_rom  \n",
            "24         ab aur kyÄ kisÄ« se marÄsim baá¸ŒhÄ.eÃ± ham  \n",
            "25  ye bhÄ« bahut hai tujh ko agar bhuul jaa.eÃ± ham  \n",
            "26            sahrÄ-e-zindagÄ« meÃ± koÄ« dÅ«srÄ na thÄ  \n",
            "27         sunte rahe haiÃ± aap hÄ« apnÄ« sadÄ.eÃ± ham  \n",
            "28         is zindagÄ« meÃ± itnÄ« farÄÄ¡hat kise nasÄ«b  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Colab Cell #4 â€” Train SentencePiece (patched) + Dataset/Loaders\n",
        "#   - Trains separate SPM models for src (Urdu) & tgt (Roman)\n",
        "#   - Uses normalization_rule_name=\"identity\" to avoid nmt_nfkc error\n",
        "#   - Builds PyTorch Dataset (char/spm), DataLoaders, and saves exp config\n",
        "# ============================================================\n",
        "from pathlib import Path\n",
        "import json, os, io, random\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import sentencepiece as spm\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# ----- Paths & config\n",
        "cfg = json.load(open(\"/content/nmt_urdu_roman/project_config.json\",\"r\",encoding=\"utf-8\"))\n",
        "PROJECT_DIR = Path(cfg[\"project_dir\"])\n",
        "DATA_DIR    = PROJECT_DIR / \"data\"\n",
        "ARTI        = PROJECT_DIR / \"artifacts\"\n",
        "ARTI.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "TRAIN_PQ = DATA_DIR / \"pairs_train.parquet\"\n",
        "VAL_PQ   = DATA_DIR / \"pairs_val.parquet\"\n",
        "TEST_PQ  = DATA_DIR / \"pairs_test.parquet\"\n",
        "\n",
        "df_train = pd.read_parquet(TRAIN_PQ)\n",
        "df_val   = pd.read_parquet(VAL_PQ)\n",
        "df_test  = pd.read_parquet(TEST_PQ)\n",
        "\n",
        "# ============================================================\n",
        "# (A) Train SentencePiece tokenizers (patched)\n",
        "#     Separate models for src/tgt. Reserve: pad=0, bos=1, eos=2, unk=3\n",
        "# ============================================================\n",
        "SRC_TXT = ARTI / \"spm_src_train.txt\"\n",
        "TGT_TXT = ARTI / \"spm_tgt_train.txt\"\n",
        "SRC_MODEL = ARTI / \"spm_src.model\"\n",
        "TGT_MODEL = ARTI / \"spm_tgt.model\"\n",
        "\n",
        "if not SRC_TXT.exists():\n",
        "    SRC_TXT.write_text(\"\\n\".join(df_train[\"src_ur\"].astype(str).tolist()), encoding=\"utf-8\")\n",
        "if not TGT_TXT.exists():\n",
        "    TGT_TXT.write_text(\"\\n\".join(df_train[\"tgt_rom\"].astype(str).tolist()), encoding=\"utf-8\")\n",
        "\n",
        "SPM_SRC_VOCAB = 2000\n",
        "SPM_TGT_VOCAB = 2000\n",
        "\n",
        "def _clean_partial_models(model_prefix_str: str):\n",
        "    # delete tiny/corrupt artifacts to allow clean retrain\n",
        "    for ext in (\".model\", \".vocab\"):\n",
        "        f = Path(model_prefix_str + ext)\n",
        "        if f.exists() and f.stat().st_size < 1024:\n",
        "            f.unlink(missing_ok=True)\n",
        "\n",
        "def train_spm(input_path, model_path, vocab_size, character_coverage=0.9995, model_type=\"bpe\"):\n",
        "    model_prefix = str(Path(model_path).with_suffix(\"\"))\n",
        "    _clean_partial_models(model_prefix)\n",
        "    if Path(model_prefix + \".model\").exists():\n",
        "        return  # already trained\n",
        "\n",
        "    # Key fix: normalization_rule_name=\"identity\" (prevents 'nmt_nfkc' missing error)\n",
        "    spm.SentencePieceTrainer.Train(\n",
        "        input=str(input_path),\n",
        "        model_prefix=model_prefix,\n",
        "        vocab_size=int(vocab_size),\n",
        "        model_type=model_type,\n",
        "        character_coverage=float(character_coverage),\n",
        "        pad_id=0, bos_id=1, eos_id=2, unk_id=3,\n",
        "        input_sentence_size=1_000_000,\n",
        "        shuffle_input_sentence=True,\n",
        "        normalization_rule_name=\"identity\",\n",
        "    )\n",
        "\n",
        "# Train patched SPM models\n",
        "train_spm(SRC_TXT, SRC_MODEL, SPM_SRC_VOCAB, character_coverage=0.9995, model_type=\"bpe\")\n",
        "train_spm(TGT_TXT, TGT_MODEL, SPM_TGT_VOCAB, character_coverage=0.9995, model_type=\"bpe\")\n",
        "\n",
        "# Load processors\n",
        "sp_src = spm.SentencePieceProcessor()\n",
        "sp_tgt = spm.SentencePieceProcessor()\n",
        "sp_src.load(str(SRC_MODEL))\n",
        "sp_tgt.load(str(TGT_MODEL))\n",
        "\n",
        "print(\"SPM src size:\", sp_src.get_piece_size(), \"| tgt size:\", sp_tgt.get_piece_size())\n",
        "\n",
        "# ============================================================\n",
        "# (B) Tokenization utilities\n",
        "#     - CHAR mode uses vocab jsons from Cell #3\n",
        "#     - SPM mode uses the SentencePiece models above\n",
        "# ============================================================\n",
        "SPECIAL = [\"<pad>\", \"<s>\", \"</s>\", \"<unk>\"]\n",
        "v_src_char = json.load(open(ARTI / \"vocab_src_char.json\", \"r\", encoding=\"utf-8\"))\n",
        "v_tgt_char = json.load(open(ARTI / \"vocab_tgt_char.json\", \"r\", encoding=\"utf-8\"))\n",
        "stoi_src_char = v_src_char[\"stoi\"]; itos_src_char = v_src_char[\"itos\"]\n",
        "stoi_tgt_char = v_tgt_char[\"stoi\"]; itos_tgt_char = v_tgt_char[\"itos\"]\n",
        "\n",
        "PAD_ID = 0; BOS_ID = 1; EOS_ID = 2; UNK_ID = 3\n",
        "\n",
        "def encode_char_src(s: str):\n",
        "    ids = [BOS_ID]\n",
        "    for ch in s:\n",
        "        ids.append(stoi_src_char.get(ch, UNK_ID))\n",
        "    ids.append(EOS_ID)\n",
        "    return ids\n",
        "\n",
        "def encode_char_tgt(s: str):\n",
        "    ids = [BOS_ID]\n",
        "    for ch in s:\n",
        "        ids.append(stoi_tgt_char.get(ch, UNK_ID))\n",
        "    ids.append(EOS_ID)\n",
        "    return ids\n",
        "\n",
        "def encode_spm_src(s: str):\n",
        "    return [BOS_ID] + sp_src.encode(s, out_type=int, enable_sampling=False) + [EOS_ID]\n",
        "\n",
        "def encode_spm_tgt(s: str):\n",
        "    return [BOS_ID] + sp_tgt.encode(s, out_type=int, enable_sampling=False) + [EOS_ID]\n",
        "\n",
        "# ============================================================\n",
        "# (C) PyTorch Dataset + Collate (supports 'char' or 'spm')\n",
        "# ============================================================\n",
        "class NMTDataset(Dataset):\n",
        "    def __init__(self, df: pd.DataFrame, mode=\"char\"):\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.mode = mode\n",
        "        assert mode in (\"char\", \"spm\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        src = str(row[\"src_ur\"])\n",
        "        tgt = str(row[\"tgt_rom\"])\n",
        "        if self.mode == \"char\":\n",
        "            src_ids = encode_char_src(src)\n",
        "            tgt_ids = encode_char_tgt(tgt)\n",
        "        else:\n",
        "            src_ids = encode_spm_src(src)\n",
        "            tgt_ids = encode_spm_tgt(tgt)\n",
        "        return {\n",
        "            \"src_ids\": torch.tensor(src_ids, dtype=torch.long),\n",
        "            \"tgt_ids\": torch.tensor(tgt_ids, dtype=torch.long),\n",
        "            \"len_src\": len(src_ids),\n",
        "            \"len_tgt\": len(tgt_ids),\n",
        "        }\n",
        "\n",
        "def collate_fn(batch):\n",
        "    # Sort by src length (desc) for minor efficiency\n",
        "    batch = sorted(batch, key=lambda x: x[\"len_src\"], reverse=True)\n",
        "    src_lens = [b[\"len_src\"] for b in batch]\n",
        "    tgt_lens = [b[\"len_tgt\"] for b in batch]\n",
        "    max_src = max(src_lens)\n",
        "    max_tgt = max(tgt_lens)\n",
        "\n",
        "    def pad_seq(seq, max_len):\n",
        "        out = torch.full((max_len,), PAD_ID, dtype=torch.long)\n",
        "        out[:len(seq)] = seq\n",
        "        return out\n",
        "\n",
        "    src_pad = torch.stack([pad_seq(b[\"src_ids\"], max_src) for b in batch], dim=0)\n",
        "    tgt_pad = torch.stack([pad_seq(b[\"tgt_ids\"], max_tgt) for b in batch], dim=0)\n",
        "\n",
        "    return {\n",
        "        \"src_ids\": src_pad,    # [B, Tsrc]\n",
        "        \"tgt_ids\": tgt_pad,    # [B, Ttgt]\n",
        "        \"src_lens\": torch.tensor(src_lens, dtype=torch.long),\n",
        "        \"tgt_lens\": torch.tensor(tgt_lens, dtype=torch.long),\n",
        "    }\n",
        "\n",
        "# ============================================================\n",
        "# (D) Build DataLoaders (both modes so you can A/B later)\n",
        "# ============================================================\n",
        "def make_loaders(mode=\"char\", batch_size=64, num_workers=2, shuffle_train=True):\n",
        "    ds_train = NMTDataset(df_train, mode=mode)\n",
        "    ds_val   = NMTDataset(df_val,   mode=mode)\n",
        "    ds_test  = NMTDataset(df_test,  mode=mode)\n",
        "\n",
        "    # pin_memory only helps on CUDA; harmless on CPU but can warn\n",
        "    dl_train = DataLoader(ds_train, batch_size=batch_size, shuffle=shuffle_train,\n",
        "                          num_workers=num_workers, collate_fn=collate_fn, pin_memory=torch.cuda.is_available())\n",
        "    dl_val   = DataLoader(ds_val,   batch_size=batch_size, shuffle=False,\n",
        "                          num_workers=num_workers, collate_fn=collate_fn, pin_memory=torch.cuda.is_available())\n",
        "    dl_test  = DataLoader(ds_test,  batch_size=batch_size, shuffle=False,\n",
        "                          num_workers=num_workers, collate_fn=collate_fn, pin_memory=torch.cuda.is_available())\n",
        "    return ds_train, ds_val, ds_test, dl_train, dl_val, dl_test\n",
        "\n",
        "# Quick smoke test (char mode)\n",
        "_, _, _, dl_tr_char, dl_v_char, _ = make_loaders(mode=\"char\", batch_size=32)\n",
        "batch_char = next(iter(dl_tr_char))\n",
        "print(\"CHAR batch shapes:\",\n",
        "      batch_char[\"src_ids\"].shape, batch_char[\"tgt_ids\"].shape)\n",
        "\n",
        "# Quick smoke test (spm mode)\n",
        "_, _, _, dl_tr_spm, dl_v_spm, _ = make_loaders(mode=\"spm\", batch_size=32)\n",
        "batch_spm = next(iter(dl_tr_spm))\n",
        "print(\"SPM  batch shapes:\",\n",
        "      batch_spm[\"src_ids\"].shape, batch_spm[\"tgt_ids\"].shape)\n",
        "\n",
        "# ============================================================\n",
        "# (E) Save a default experiment config\n",
        "# ============================================================\n",
        "exp_cfg = {\n",
        "    \"tokenization\": \"char\",   # or \"spm\"\n",
        "    \"embedding_dim\": 256,\n",
        "    \"hidden_size\": 256,\n",
        "    \"enc_layers\": 2,\n",
        "    \"dec_layers\": 4,\n",
        "    \"dropout\": 0.3,\n",
        "    \"learning_rate\": 5e-4,\n",
        "    \"batch_size\": 64,\n",
        "    \"teacher_forcing_start\": 1.0,\n",
        "    \"teacher_forcing_end\": 0.5,\n",
        "    \"epochs\": 20,\n",
        "    \"grad_clip\": 1.0,\n",
        "    \"beam_size\": 5\n",
        "}\n",
        "with open(ARTI / \"exp_default.json\",\"w\",encoding=\"utf-8\") as f:\n",
        "    json.dump(exp_cfg, f, indent=2)\n",
        "print(\"Saved default exp config:\", ARTI / \"exp_default.json\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B29Cie8lbHVu",
        "outputId": "d336506b-ea0d-4332-f455-f12914690ef1"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SPM src size: 2000 | tgt size: 2000\n",
            "CHAR batch shapes: torch.Size([32, 41]) torch.Size([32, 49])\n",
            "SPM  batch shapes: torch.Size([32, 17]) torch.Size([32, 25])\n",
            "Saved default exp config: /content/nmt_urdu_roman/artifacts/exp_default.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Colab Cell #5 â€” BiLSTM Encoder + 4-Layer LSTM Decoder (Luong)\n",
        "#   â€¢ Restart-safe: redefines everything needed for training\n",
        "#   â€¢ Works with tokenization = \"char\" or \"spm\"\n",
        "#   â€¢ Trains, evaluates on val (loss, ppl, BLEU, CER), saves best\n",
        "# ============================================================\n",
        "import os, json, math, time, numpy as np\n",
        "from pathlib import Path\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.utils import clip_grad_norm_\n",
        "import sentencepiece as spm\n",
        "import sacrebleu\n",
        "from jiwer import cer as jiwer_cer\n",
        "\n",
        "# ---------- Paths / config ----------\n",
        "cfg = json.load(open(\"/content/nmt_urdu_roman/project_config.json\",\"r\",encoding=\"utf-8\"))\n",
        "PROJECT_DIR = Path(cfg[\"project_dir\"])\n",
        "ARTI        = PROJECT_DIR / \"artifacts\"\n",
        "MODELS_DIR  = PROJECT_DIR / \"models\"\n",
        "MODELS_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Load experiment base (you can override when calling train_model)\n",
        "exp = json.load(open(ARTI / \"exp_default.json\",\"r\",encoding=\"utf-8\"))\n",
        "\n",
        "# Special tokens\n",
        "PAD_ID = 0; BOS_ID = 1; EOS_ID = 2; UNK_ID = 3\n",
        "\n",
        "# Device\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", DEVICE)\n",
        "\n",
        "# ---------- Bring in loaders from Cell #4 (and validate) ----------\n",
        "try:\n",
        "    make_loaders\n",
        "except NameError:\n",
        "    raise RuntimeError(\"âŒ make_loaders is undefined. Please re-run Cell #4 first.\")\n",
        "\n",
        "def get_loaders(mode, batch):\n",
        "    return make_loaders(mode=mode, batch_size=batch, num_workers=2, shuffle_train=True)\n",
        "\n",
        "# ---------- Tokenizers / decoders ----------\n",
        "def _load_tokenizers(tokenization):\n",
        "    if tokenization == \"char\":\n",
        "        v_src = json.load(open(ARTI / \"vocab_src_char.json\",\"r\",encoding=\"utf-8\"))\n",
        "        v_tgt = json.load(open(ARTI / \"vocab_tgt_char.json\",\"r\",encoding=\"utf-8\"))\n",
        "        SRC_VSIZE = len(v_src[\"itos\"]); TGT_VSIZE = len(v_tgt[\"itos\"])\n",
        "        itos_tgt  = v_tgt[\"itos\"]\n",
        "        def decode_char(ids):\n",
        "            out=[]\n",
        "            for i in ids:\n",
        "                if i in (PAD_ID, BOS_ID, EOS_ID): continue\n",
        "                out.append(itos_tgt[i] if 0 <= i < len(itos_tgt) else \"\")\n",
        "            return \"\".join(out).strip()\n",
        "        return {\"mode\":\"char\",\"SRC_V\":SRC_VSIZE,\"TGT_V\":TGT_VSIZE,\n",
        "                \"decode\":decode_char, \"sp_src\":None, \"sp_tgt\":None, \"v_tgt\":v_tgt}\n",
        "    else:\n",
        "        sp_src = spm.SentencePieceProcessor(); sp_src.load(str(ARTI / \"spm_src.model\"))\n",
        "        sp_tgt = spm.SentencePieceProcessor(); sp_tgt.load(str(ARTI / \"spm_tgt.model\"))\n",
        "        SRC_VSIZE = sp_src.get_piece_size(); TGT_VSIZE = sp_tgt.get_piece_size()\n",
        "        def decode_spm(ids):\n",
        "            ids = [i for i in ids if i not in (PAD_ID, BOS_ID, EOS_ID)]\n",
        "            try: return sp_tgt.decode(ids).strip()\n",
        "            except: return sp_tgt.decode_pieces([sp_tgt.id_to_piece(i) for i in ids]).strip()\n",
        "        return {\"mode\":\"spm\",\"SRC_V\":SRC_VSIZE,\"TGT_V\":TGT_VSIZE,\n",
        "                \"decode\":decode_spm, \"sp_src\":sp_src, \"sp_tgt\":sp_tgt, \"v_tgt\":None}\n",
        "\n",
        "# ---------- Model ----------\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, vocab_size, emb_dim, hid_size, num_layers=2, dropout=0.3, bidirectional=True):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, emb_dim, padding_idx=PAD_ID)\n",
        "        self.bilstm = nn.LSTM(\n",
        "            emb_dim, hid_size, num_layers=num_layers, dropout=dropout,\n",
        "            bidirectional=bidirectional, batch_first=True\n",
        "        )\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.bidirectional = bidirectional\n",
        "        self.hid_size = hid_size\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "    def forward(self, src_ids):\n",
        "        emb = self.dropout(self.embedding(src_ids))          # [B, T, E]\n",
        "        outputs, (hn, cn) = self.bilstm(emb)                 # outputs: [B, T, 2H] if bi\n",
        "        return outputs, (hn, cn)\n",
        "\n",
        "class LuongAttention(nn.Module):\n",
        "    # general: s_t^T * W * h_i\n",
        "    def __init__(self, dec_hid, enc_hid_bi):\n",
        "        super().__init__()\n",
        "        self.W = nn.Linear(dec_hid, enc_hid_bi, bias=False)\n",
        "    def forward(self, dec_h_t, enc_outputs, mask=None):\n",
        "        # dec_h_t: [B,H], enc_outputs: [B,Tsrc,Henc]\n",
        "        score = torch.bmm(self.W(dec_h_t).unsqueeze(1), enc_outputs.transpose(1,2))  # [B,1,Tsrc]\n",
        "        if mask is not None:\n",
        "            score = score.masked_fill(mask.unsqueeze(1), -1e9)\n",
        "        attn = torch.softmax(score, dim=-1)       # [B,1,Tsrc]\n",
        "        ctx  = torch.bmm(attn, enc_outputs).squeeze(1)  # [B,Henc]\n",
        "        return ctx, attn.squeeze(1)\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, vocab_size, emb_dim, hid_size, enc_hid_bi, num_layers=4, dropout=0.3):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, emb_dim, padding_idx=PAD_ID)\n",
        "        self.input_size = emb_dim + enc_hid_bi\n",
        "        self.lstm = nn.LSTM(self.input_size, hid_size, num_layers=num_layers,\n",
        "                            dropout=dropout, batch_first=True)\n",
        "        self.attn = LuongAttention(hid_size, enc_hid_bi)\n",
        "        self.fc_out = nn.Linear(hid_size + enc_hid_bi, vocab_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "    def forward_step(self, y_prev, h_c, enc_outputs, enc_pad_mask, ctx_prev):\n",
        "        # y_prev: [B]\n",
        "        emb = self.dropout(self.embedding(y_prev))           # [B,E]\n",
        "        lstm_in = torch.cat([emb, ctx_prev], dim=-1).unsqueeze(1)  # [B,1,E+Henc]\n",
        "        out, h_c = self.lstm(lstm_in, h_c)                   # out: [B,1,Hdec]\n",
        "        h_t = out.squeeze(1)                                 # [B,Hdec]\n",
        "        ctx_t, attn = self.attn(h_t, enc_outputs, enc_pad_mask)    # [B,Henc]\n",
        "        logits = self.fc_out(torch.cat([h_t, ctx_t], dim=-1))      # [B,V]\n",
        "        return logits, h_c, ctx_t, attn\n",
        "\n",
        "    # explicit forward so Module is happy\n",
        "    def forward(self, y_prev, h_c, enc_outputs, enc_pad_mask, ctx_prev):\n",
        "        return self.forward_step(y_prev, h_c, enc_outputs, enc_pad_mask, ctx_prev)\n",
        "\n",
        "class Bridge(nn.Module):\n",
        "    def __init__(self, enc_hid, dec_hid, dec_layers, bidirectional=True):\n",
        "        super().__init__()\n",
        "        mul = 2 if bidirectional else 1\n",
        "        self.h_proj = nn.Linear(enc_hid*mul, dec_hid)\n",
        "        self.c_proj = nn.Linear(enc_hid*mul, dec_hid)\n",
        "        self.dec_layers = dec_layers\n",
        "    def forward(self, enc_hn, enc_cn):\n",
        "        # enc_hn: [L*mul, B, H]\n",
        "        top_h = torch.cat([enc_hn[-2], enc_hn[-1]], dim=-1)  # [B, 2H]\n",
        "        top_c = torch.cat([enc_cn[-2], enc_cn[-1]], dim=-1)  # [B, 2H]\n",
        "        h0 = torch.tanh(self.h_proj(top_h))                  # [B,Hd]\n",
        "        c0 = torch.tanh(self.c_proj(top_c))                  # [B,Hd]\n",
        "        h0 = h0.unsqueeze(0).repeat(self.dec_layers, 1, 1)   # [Ldec,B,Hd]\n",
        "        c0 = c0.unsqueeze(0).repeat(self.dec_layers, 1, 1)\n",
        "        return (h0, c0)\n",
        "\n",
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, src_vsize, tgt_vsize, emb_dim, hid_size, enc_layers, dec_layers, dropout=0.3):\n",
        "        super().__init__()\n",
        "        self.encoder = Encoder(src_vsize, emb_dim, hid_size, enc_layers, dropout, bidirectional=True)\n",
        "        enc_hid_bi = hid_size * 2\n",
        "        self.decoder = Decoder(tgt_vsize, emb_dim, hid_size, enc_hid_bi, dec_layers, dropout)\n",
        "        self.bridge  = Bridge(hid_size, hid_size, dec_layers, bidirectional=True)\n",
        "\n",
        "    def forward(self, src_ids, tgt_ids, teacher_forcing=1.0):\n",
        "        # src_ids: [B,Tsrc], tgt_ids: [B,Ttgt]\n",
        "        B, Tt = tgt_ids.size()\n",
        "        enc_outputs, (hn, cn) = self.encoder(src_ids)            # enc_outputs: [B,Tsrc,2H]\n",
        "        enc_pad_mask = (src_ids == PAD_ID)\n",
        "\n",
        "        dec_hc = self.bridge(hn, cn)\n",
        "        ctx = torch.zeros(src_ids.size(0), enc_outputs.size(-1), device=src_ids.device)\n",
        "        logits_all = []\n",
        "        y_prev = tgt_ids[:,0]  # BOS\n",
        "\n",
        "        for t in range(1, Tt):\n",
        "            use_tf = (np.random.rand() < teacher_forcing)\n",
        "            logits, dec_hc, ctx, _ = self.decoder(y_prev, dec_hc, enc_outputs, enc_pad_mask, ctx)\n",
        "            logits_all.append(logits.unsqueeze(1))\n",
        "            y_prev = tgt_ids[:,t] if use_tf else torch.argmax(logits, dim=-1)\n",
        "\n",
        "        return torch.cat(logits_all, dim=1)  # [B, Tt-1, V]\n",
        "\n",
        "# ---------- Training / Evaluation ----------\n",
        "def sequence_nll(logits, tgt_ids):\n",
        "    # logits: [B,T-1,V], tgt_ids: [B,T]\n",
        "    B, Tm1, V = logits.shape\n",
        "    gold = tgt_ids[:,1:1+Tm1]                      # [B,T-1]\n",
        "    loss = F.cross_entropy(\n",
        "        logits.reshape(B*Tm1, V),\n",
        "        gold.reshape(B*Tm1),\n",
        "        ignore_index=PAD_ID\n",
        "    )\n",
        "    return loss\n",
        "\n",
        "def greedy_decode(model, batch, max_len=150):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        src = batch[\"src_ids\"].to(DEVICE)\n",
        "        enc_outputs, (hn, cn) = model.encoder(src)\n",
        "        enc_mask = (src == PAD_ID)\n",
        "        dec_hc = model.bridge(hn, cn)\n",
        "        ctx = torch.zeros(src.size(0), enc_outputs.size(-1), device=src.device)\n",
        "        y_prev = torch.full((src.size(0),), BOS_ID, dtype=torch.long, device=src.device)\n",
        "        outs = []\n",
        "        for _ in range(max_len):\n",
        "            logits, dec_hc, ctx, _ = model.decoder(y_prev, dec_hc, enc_outputs, enc_mask, ctx)\n",
        "            y_prev = torch.argmax(logits, dim=-1)\n",
        "            outs.append(y_prev.unsqueeze(1))\n",
        "        return torch.cat(outs, dim=1)  # [B, L]\n",
        "\n",
        "def _ids_to_strs_tgt(batch_ids, decode_fn):\n",
        "    # batch_ids: Tensor [B,L] or list[list[int]]\n",
        "    arr = batch_ids.cpu().tolist() if isinstance(batch_ids, torch.Tensor) else batch_ids\n",
        "    return [decode_fn(ids) for ids in arr]\n",
        "\n",
        "def eval_on_loader(model, dl, decode_fn, mode):\n",
        "    model.eval()\n",
        "    total_loss, total_tokens = 0.0, 0\n",
        "    preds_text, refs_text = [], []\n",
        "    with torch.no_grad():\n",
        "        for batch in dl:\n",
        "            src = batch[\"src_ids\"].to(DEVICE)\n",
        "            tgt = batch[\"tgt_ids\"].to(DEVICE)\n",
        "\n",
        "            # teacher-forced loss\n",
        "            logits = model(src, tgt, teacher_forcing=1.0)\n",
        "            loss = sequence_nll(logits, tgt)\n",
        "\n",
        "            gold = tgt[:, 1:1+logits.size(1)]\n",
        "            ntok = (gold != PAD_ID).sum().item()\n",
        "            total_loss += loss.item() * ntok\n",
        "            total_tokens += ntok\n",
        "\n",
        "            # greedy decode for metrics\n",
        "            gen = greedy_decode(model, batch, max_len=tgt.size(1)-1)\n",
        "            preds_text.extend(_ids_to_strs_tgt(gen, decode_fn))\n",
        "            refs_text.extend(_ids_to_strs_tgt(tgt, decode_fn))\n",
        "\n",
        "    avg_nll = total_loss / max(total_tokens, 1)\n",
        "    ppl = math.exp(avg_nll)\n",
        "    bleu = sacrebleu.corpus_bleu(preds_text, [refs_text]).score\n",
        "    cer_scores = [jiwer_cer(r, p) for p, r in zip(preds_text, refs_text)]\n",
        "    cer_mean = float(np.mean(cer_scores)) if cer_scores else 1.0\n",
        "    return avg_nll, ppl, bleu, cer_mean, preds_text[:5], refs_text[:5]\n",
        "\n",
        "def train_model(exp_overrides=None):\n",
        "    # Merge overrides with default exp\n",
        "    cfg_local = dict(exp)\n",
        "    if exp_overrides: cfg_local.update(exp_overrides)\n",
        "\n",
        "    # Tokenizers & decode function\n",
        "    tok = _load_tokenizers(cfg_local[\"tokenization\"])\n",
        "    SRC_VSIZE, TGT_VSIZE = tok[\"SRC_V\"], tok[\"TGT_V\"]\n",
        "    decode_fn = tok[\"decode\"]\n",
        "\n",
        "    # Data\n",
        "    _, _, _, dl_train, dl_val, _ = get_loaders(cfg_local[\"tokenization\"], batch=cfg_local[\"batch_size\"])\n",
        "\n",
        "    # Model + opt\n",
        "    model = Seq2Seq(\n",
        "        src_vsize=SRC_VSIZE, tgt_vsize=TGT_VSIZE,\n",
        "        emb_dim=cfg_local[\"embedding_dim\"], hid_size=cfg_local[\"hidden_size\"],\n",
        "        enc_layers=cfg_local[\"enc_layers\"], dec_layers=cfg_local[\"dec_layers\"],\n",
        "        dropout=cfg_local[\"dropout\"]\n",
        "    ).to(DEVICE)\n",
        "    opt = torch.optim.Adam(model.parameters(), lr=cfg_local[\"learning_rate\"])\n",
        "\n",
        "    # Schedules\n",
        "    EPOCHS   = cfg_local[\"epochs\"]\n",
        "    TF_START = cfg_local[\"teacher_forcing_start\"]\n",
        "    TF_END   = cfg_local[\"teacher_forcing_end\"]\n",
        "    def tf_ratio(epoch_idx):\n",
        "        if EPOCHS <= 1: return TF_END\n",
        "        return TF_START + (TF_END - TF_START) * (epoch_idx / (EPOCHS-1))\n",
        "\n",
        "    best_bleu, best_path = -1.0, None\n",
        "\n",
        "    for ep in range(1, EPOCHS+1):\n",
        "        model.train()\n",
        "        t0 = time.time()\n",
        "        tf = tf_ratio(ep-1)\n",
        "        total_loss, total_tok = 0.0, 0\n",
        "\n",
        "        for batch in dl_train:\n",
        "            src = batch[\"src_ids\"].to(DEVICE)\n",
        "            tgt = batch[\"tgt_ids\"].to(DEVICE)\n",
        "            logits = model(src, tgt, teacher_forcing=tf)\n",
        "            loss = sequence_nll(logits, tgt)\n",
        "\n",
        "            opt.zero_grad(set_to_none=True)\n",
        "            loss.backward()\n",
        "            clip_grad_norm_(model.parameters(), cfg_local[\"grad_clip\"])\n",
        "            opt.step()\n",
        "\n",
        "            gold = tgt[:,1:1+logits.size(1)]\n",
        "            ntok = (gold != PAD_ID).sum().item()\n",
        "            total_loss += loss.item() * ntok\n",
        "            total_tok += ntok\n",
        "\n",
        "        train_nll = total_loss / max(total_tok,1)\n",
        "        train_ppl = math.exp(train_nll)\n",
        "\n",
        "        # Validate\n",
        "        val_nll, val_ppl, val_bleu, val_cer, samp_pred, samp_ref = eval_on_loader(model, dl_val, decode_fn, tok[\"mode\"])\n",
        "\n",
        "        dt = time.time()-t0\n",
        "        print(f\"[Ep {ep:02d}] tf={tf:.2f} | train ppl={train_ppl:.2f} | \"\n",
        "              f\"val ppl={val_ppl:.2f} | BLEU={val_bleu:.2f} | CER={val_cer:.3f} | {dt:.1f}s\")\n",
        "\n",
        "        # Save best by BLEU\n",
        "        if val_bleu > best_bleu:\n",
        "            best_bleu = val_bleu\n",
        "            label = f\"{tok['mode']}_E{cfg_local['embedding_dim']}_H{cfg_local['hidden_size']}_enc{cfg_local['enc_layers']}_dec{cfg_local['dec_layers']}_drop{cfg_local['dropout']}\"\n",
        "            best_path = MODELS_DIR / f\"bilstm4lstm_{label}_best.pt\"\n",
        "            torch.save({\n",
        "                \"model_state\": model.state_dict(),\n",
        "                \"exp\": cfg_local,\n",
        "                \"bleu\": float(best_bleu),\n",
        "                \"tokenization\": tok[\"mode\"]\n",
        "            }, best_path)\n",
        "            print(\"  â†³ Saved best:\", best_path)\n",
        "\n",
        "        # A couple of samples\n",
        "        for i in range(min(2, len(samp_pred))):\n",
        "            print(f\"  pred: {samp_pred[i]}\")\n",
        "            print(f\"  ref : {samp_ref[i]}\")\n",
        "\n",
        "    print(\"\\nBest BLEU:\", best_bleu, \" | ckpt:\", best_path)\n",
        "    return best_path\n",
        "\n",
        "print(\"âœ… Cell #5 ready. Starting a quick run â€¦\")\n",
        "\n",
        "# --- Kick a quick baseline (optional: reduce epochs for smoke test) ---\n",
        "# exp[\"epochs\"] = 3  # uncomment for a super-fast check\n",
        "best_ckpt = train_model()   # uses exp_default.json settings (char, E=256,H=256,â€¦)\n",
        "print(\"Done.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nTGcSAPVbK51",
        "outputId": "e421fc84-53f5-4dd4-f24c-f5d3bb561452"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n",
            "âœ… Cell #5 ready. Starting a quick run â€¦\n",
            "[Ep 01] tf=1.00 | train ppl=14.27 | val ppl=4.59 | BLEU=0.56 | CER=0.665 | 49.7s\n",
            "  â†³ Saved best: /content/nmt_urdu_roman/models/bilstm4lstm_char_E256_H256_enc2_dec4_drop0.3_best.pt\n",
            "  pred: taaaah raaÄ ke ke mo to mai mhhÄ« bahÄ hoÃ± hoÃ± ho hai\n",
            "  ref : taaza rifÄqat ke mausam tak maiÃ± bhÄ« jiyÄ huuÃ± vo bhÄ« jiyÄ hai\n",
            "  pred: ham na sss ke shhrr ko hhhhÄ Äo no no nank lanÄ haiÄ haiÄ ha\n",
            "  ref : ham ne us ke shahr ko chhoá¸ŒÄ aur ÄÃ±khoÃ± ko muuÃ±d liyÄ hai\n",
            "[Ep 02] tf=0.97 | train ppl=3.49 | val ppl=2.02 | BLEU=10.89 | CER=0.426 | 49.0s\n",
            "  â†³ Saved best: /content/nmt_urdu_roman/models/bilstm4lstm_char_E256_H256_enc2_dec4_drop0.3_best.pt\n",
            "  pred: taÄza rafÄqt ke muusm tak meÃ± bhÄ« jhÄ« jayÃ± joÃ± voÄ joÄ joÄ j\n",
            "  ref : taaza rifÄqat ke mausam tak maiÃ± bhÄ« jiyÄ huuÃ± vo bhÄ« jiyÄ hai\n",
            "  pred: ham ne as ke shhhr ko chhvÄ Äo aankhoÃ± ko maundd layÄ laidd la\n",
            "  ref : ham ne us ke shahr ko chhoá¸ŒÄ aur ÄÃ±khoÃ± ko muuÃ±d liyÄ hai\n",
            "[Ep 03] tf=0.95 | train ppl=2.08 | val ppl=1.60 | BLEU=24.62 | CER=0.341 | 47.8s\n",
            "  â†³ Saved best: /content/nmt_urdu_roman/models/bilstm4lstm_char_E256_H256_enc2_dec4_drop0.3_best.pt\n",
            "  pred: tÄza rafÄqat ke mausm tak meÃ± bhÄ« jayÄ huuÃ± vo bhÄ« haiÄ haiÄ\n",
            "  ref : taaza rifÄqat ke mausam tak maiÃ± bhÄ« jiyÄ huuÃ± vo bhÄ« jiyÄ hai\n",
            "  pred: ham ne us ke shhhar ko chhÅ«á¸ŒÄ aur ÄÃ±khoÃ± ko muuÃ± lyÄ haiÄ hai\n",
            "  ref : ham ne us ke shahr ko chhoá¸ŒÄ aur ÄÃ±khoÃ± ko muuÃ±d liyÄ hai\n",
            "[Ep 04] tf=0.92 | train ppl=1.67 | val ppl=1.41 | BLEU=35.65 | CER=0.317 | 47.7s\n",
            "  â†³ Saved best: /content/nmt_urdu_roman/models/bilstm4lstm_char_E256_H256_enc2_dec4_drop0.3_best.pt\n",
            "  pred: tÄza rafÄqat ke mausm tak meÃ± bhÄ« jayÄ huuÃ± vo bhÄ« haiÄ baiÄ«\n",
            "  ref : taaza rifÄqat ke mausam tak maiÃ± bhÄ« jiyÄ huuÃ± vo bhÄ« jiyÄ hai\n",
            "  pred: ham ne us ke shahr ko chhoá¸ŒÄ aur ÄÃ±khoÃ± ko muiÃ±d liyÄ haihand\n",
            "  ref : ham ne us ke shahr ko chhoá¸ŒÄ aur ÄÃ±khoÃ± ko muuÃ±d liyÄ hai\n",
            "[Ep 05] tf=0.89 | train ppl=1.51 | val ppl=1.33 | BLEU=41.42 | CER=0.278 | 48.3s\n",
            "  â†³ Saved best: /content/nmt_urdu_roman/models/bilstm4lstm_char_E256_H256_enc2_dec4_drop0.3_best.pt\n",
            "  pred: tÄza rafÄqat ke mausam tak meÃ± bhÄ« jyÄ huuÃ± vo bhÄ« jayÄ jaiÄ« h\n",
            "  ref : taaza rifÄqat ke mausam tak maiÃ± bhÄ« jiyÄ huuÃ± vo bhÄ« jiyÄ hai\n",
            "  pred: ham ne us ke shahr ko chhoá¸ŒÄ aur ÄÃ±khoÃ± ko maund liyÄ haiand l\n",
            "  ref : ham ne us ke shahr ko chhoá¸ŒÄ aur ÄÃ±khoÃ± ko muuÃ±d liyÄ hai\n",
            "[Ep 06] tf=0.87 | train ppl=1.43 | val ppl=1.30 | BLEU=45.61 | CER=0.292 | 47.2s\n",
            "  â†³ Saved best: /content/nmt_urdu_roman/models/bilstm4lstm_char_E256_H256_enc2_dec4_drop0.3_best.pt\n",
            "  pred: tÄza rafÄqat ke mausam tak meÃ± bhÄ« juuÃ± voÃ± vahÄ« jii jiyÄ« hai\n",
            "  ref : taaza rifÄqat ke mausam tak maiÃ± bhÄ« jiyÄ huuÃ± vo bhÄ« jiyÄ hai\n",
            "  pred: ham ne us ke shahr ko chhoá¸ŒÄ aur ÄÃ±khoÃ± ko maiÃ±d liyÄ haiaia\n",
            "  ref : ham ne us ke shahr ko chhoá¸ŒÄ aur ÄÃ±khoÃ± ko muuÃ±d liyÄ hai\n",
            "[Ep 07] tf=0.84 | train ppl=1.39 | val ppl=1.25 | BLEU=49.73 | CER=0.300 | 49.4s\n",
            "  â†³ Saved best: /content/nmt_urdu_roman/models/bilstm4lstm_char_E256_H256_enc2_dec4_drop0.3_best.pt\n",
            "  pred: taaza rafÄqat ke mausam tak meÃ± bhÄ« juyÄ huuÃ± vo bhÄ« jaijÄ hai\n",
            "  ref : taaza rifÄqat ke mausam tak maiÃ± bhÄ« jiyÄ huuÃ± vo bhÄ« jiyÄ hai\n",
            "  pred: ham ne us ke shahr ko chhoá¸ŒÄ aur ÄÃ±khoÃ± ko mauÃ±d liyÄ haihai\n",
            "  ref : ham ne us ke shahr ko chhoá¸ŒÄ aur ÄÃ±khoÃ± ko muuÃ±d liyÄ hai\n",
            "[Ep 08] tf=0.82 | train ppl=1.36 | val ppl=1.23 | BLEU=50.53 | CER=0.302 | 48.0s\n",
            "  â†³ Saved best: /content/nmt_urdu_roman/models/bilstm4lstm_char_E256_H256_enc2_dec4_drop0.3_best.pt\n",
            "  pred: taaza rafÄqat ke mausam tak meÃ± bhÄ« jyÄ huuÃ± vo bhÄ« jiyÄ haibh\n",
            "  ref : taaza rifÄqat ke mausam tak maiÃ± bhÄ« jiyÄ huuÃ± vo bhÄ« jiyÄ hai\n",
            "  pred: ham ne us ke shahr ko chhoá¸ŒÄ aur ÄÃ±khoÃ± ko mauÃ±d liyÄ haiai\n",
            "  ref : ham ne us ke shahr ko chhoá¸ŒÄ aur ÄÃ±khoÃ± ko muuÃ±d liyÄ hai\n",
            "[Ep 09] tf=0.79 | train ppl=1.34 | val ppl=1.22 | BLEU=52.71 | CER=0.287 | 47.4s\n",
            "  â†³ Saved best: /content/nmt_urdu_roman/models/bilstm4lstm_char_E256_H256_enc2_dec4_drop0.3_best.pt\n",
            "  pred: taaza rafÄqat ke mausam tak maiÃ± bhÄ« jiyÄ huuÃ± vo bhÄ« jiyÄ hai\n",
            "  ref : taaza rifÄqat ke mausam tak maiÃ± bhÄ« jiyÄ huuÃ± vo bhÄ« jiyÄ hai\n",
            "  pred: ham ne us ke shahr ko chhoá¸ŒÄ aur ÄÃ±khoÃ± ko mauÃ±d liyÄ haimand\n",
            "  ref : ham ne us ke shahr ko chhoá¸ŒÄ aur ÄÃ±khoÃ± ko muuÃ±d liyÄ hai\n",
            "[Ep 10] tf=0.76 | train ppl=1.34 | val ppl=1.20 | BLEU=55.46 | CER=0.245 | 47.9s\n",
            "  â†³ Saved best: /content/nmt_urdu_roman/models/bilstm4lstm_char_E256_H256_enc2_dec4_drop0.3_best.pt\n",
            "  pred: taaza rafÄqat ke mausam tak meÃ± bhÄ« juyÄ huuÃ± vo bhÄ« jaiehÄ j\n",
            "  ref : taaza rifÄqat ke mausam tak maiÃ± bhÄ« jiyÄ huuÃ± vo bhÄ« jiyÄ hai\n",
            "  pred: ham ne us ke shahr ko chhoá¸ŒÄ aur ÄÃ±khoÃ± ko mauÃ±d liyÄ haiai\n",
            "  ref : ham ne us ke shahr ko chhoá¸ŒÄ aur ÄÃ±khoÃ± ko muuÃ±d liyÄ hai\n",
            "[Ep 11] tf=0.74 | train ppl=1.32 | val ppl=1.20 | BLEU=56.59 | CER=0.221 | 48.8s\n",
            "  â†³ Saved best: /content/nmt_urdu_roman/models/bilstm4lstm_char_E256_H256_enc2_dec4_drop0.3_best.pt\n",
            "  pred: taaza rafÄqat ke mausam tak meÃ± bhÄ« jyÄ huuÃ± voÃ± vo bhÄ« haiai\n",
            "  ref : taaza rifÄqat ke mausam tak maiÃ± bhÄ« jiyÄ huuÃ± vo bhÄ« jiyÄ hai\n",
            "  pred: ham ne us ke shahr ko chhoá¸ŒÄ aur ÄÃ±khoÃ± ko mauÃ±d liyÄ haiaÃ±d\n",
            "  ref : ham ne us ke shahr ko chhoá¸ŒÄ aur ÄÃ±khoÃ± ko muuÃ±d liyÄ hai\n",
            "[Ep 12] tf=0.71 | train ppl=1.30 | val ppl=1.18 | BLEU=62.21 | CER=0.134 | 46.9s\n",
            "  â†³ Saved best: /content/nmt_urdu_roman/models/bilstm4lstm_char_E256_H256_enc2_dec4_drop0.3_best.pt\n",
            "  pred: taaza rafÄqat ke mausam tak meÃ± bhÄ« jyÄ« jiyÄ huuÃ± vo bhÄ« jayÄ h\n",
            "  ref : taaza rifÄqat ke mausam tak maiÃ± bhÄ« jiyÄ huuÃ± vo bhÄ« jiyÄ hai\n",
            "  pred: ham ne us ke shahr ko chhoá¸ŒÄ aur ÄÃ±khoÃ± ko mauÃ±d liyÄ haiand\n",
            "  ref : ham ne us ke shahr ko chhoá¸ŒÄ aur ÄÃ±khoÃ± ko muuÃ±d liyÄ hai\n",
            "[Ep 13] tf=0.68 | train ppl=1.31 | val ppl=1.18 | BLEU=56.42 | CER=0.212 | 47.9s\n",
            "  pred: taaza rifÄqat ke mausam tak maiÃ± bhÄ« jiyÄ huuÃ± vo bhÄ« haihÄ« h\n",
            "  ref : taaza rifÄqat ke mausam tak maiÃ± bhÄ« jiyÄ huuÃ± vo bhÄ« jiyÄ hai\n",
            "  pred: ham ne us ke shahr ko chhoá¸ŒÄ aur ÄÃ±khoÃ± ko mauÃ±d liyÄ haiand\n",
            "  ref : ham ne us ke shahr ko chhoá¸ŒÄ aur ÄÃ±khoÃ± ko muuÃ±d liyÄ hai\n",
            "[Ep 14] tf=0.66 | train ppl=1.31 | val ppl=1.18 | BLEU=59.74 | CER=0.189 | 47.8s\n",
            "  pred: taaza rafÄqat ke mausam tak maiÃ± bhÄ« jiyÄ huuÃ± vo bhÄ« jayÄ hai\n",
            "  ref : taaza rifÄqat ke mausam tak maiÃ± bhÄ« jiyÄ huuÃ± vo bhÄ« jiyÄ hai\n",
            "  pred: ham ne us ke shahr ko chhoá¸ŒÄ aur ÄÃ±khoÃ± ko hainand liyÄ haia\n",
            "  ref : ham ne us ke shahr ko chhoá¸ŒÄ aur ÄÃ±khoÃ± ko muuÃ±d liyÄ hai\n",
            "[Ep 15] tf=0.63 | train ppl=1.30 | val ppl=1.17 | BLEU=59.76 | CER=0.195 | 46.8s\n",
            "  pred: taaza rifÄqat ke mausam tak maiÃ± bhÄ« jiyÄ huuÃ± vo bhÄ« jiyÄ hai\n",
            "  ref : taaza rifÄqat ke mausam tak maiÃ± bhÄ« jiyÄ huuÃ± vo bhÄ« jiyÄ hai\n",
            "  pred: ham ne us ke shahr ko chhoá¸ŒÄ aur ÄÃ±khoÃ± ko mauÃ±d liyÄ haiad\n",
            "  ref : ham ne us ke shahr ko chhoá¸ŒÄ aur ÄÃ±khoÃ± ko muuÃ±d liyÄ hai\n",
            "[Ep 16] tf=0.61 | train ppl=1.29 | val ppl=1.18 | BLEU=61.10 | CER=0.174 | 47.6s\n",
            "  pred: taaza rifÄqat ke mausam tak maiÃ± bhÄ« juyÄ huuÃ± vo bhÄ« jiyÄ hai\n",
            "  ref : taaza rifÄqat ke mausam tak maiÃ± bhÄ« jiyÄ huuÃ± vo bhÄ« jiyÄ hai\n",
            "  pred: ham ne us ke shahr ko chhoá¸ŒÄ aur ÄÃ±khoÃ± ko mauÃ±d liyÄ haiaÃ±d\n",
            "  ref : ham ne us ke shahr ko chhoá¸ŒÄ aur ÄÃ±khoÃ± ko muuÃ±d liyÄ hai\n",
            "[Ep 17] tf=0.58 | train ppl=1.31 | val ppl=1.16 | BLEU=61.81 | CER=0.163 | 48.4s\n",
            "  pred: taaza rifÄqat ke mausam tak meÃ± bhÄ« jiyÄ huuÃ± vo bhÄ« jiyÄ hai\n",
            "  ref : taaza rifÄqat ke mausam tak maiÃ± bhÄ« jiyÄ huuÃ± vo bhÄ« jiyÄ hai\n",
            "  pred: ham ne us ke shahr ko chhoá¸ŒÄ aur ÄÃ±khoÃ± ko mauÃ±d liyÄ haiaÃ±d\n",
            "  ref : ham ne us ke shahr ko chhoá¸ŒÄ aur ÄÃ±khoÃ± ko muuÃ±d liyÄ hai\n",
            "[Ep 18] tf=0.55 | train ppl=1.30 | val ppl=1.16 | BLEU=62.38 | CER=0.197 | 48.5s\n",
            "  â†³ Saved best: /content/nmt_urdu_roman/models/bilstm4lstm_char_E256_H256_enc2_dec4_drop0.3_best.pt\n",
            "  pred: taaza rifÄqat ke mausam tak meÃ± bhÄ« jiyÄ huuÃ± vo bhÄ« jiyÄ haiv\n",
            "  ref : taaza rifÄqat ke mausam tak maiÃ± bhÄ« jiyÄ huuÃ± vo bhÄ« jiyÄ hai\n",
            "  pred: ham ne us ke shahr ko chhoá¸ŒÄ aur ÄÃ±khoÃ± ko muuÃ±d liyÄ haiauÃ±d\n",
            "  ref : ham ne us ke shahr ko chhoá¸ŒÄ aur ÄÃ±khoÃ± ko muuÃ±d liyÄ hai\n",
            "[Ep 19] tf=0.53 | train ppl=1.30 | val ppl=1.16 | BLEU=62.04 | CER=0.221 | 47.9s\n",
            "  pred: taaza rifÄqat ke mausam tak meÃ± bhÄ« huyÄ huuÃ± vo bhÄ« jiyÄ haib\n",
            "  ref : taaza rifÄqat ke mausam tak maiÃ± bhÄ« jiyÄ huuÃ± vo bhÄ« jiyÄ hai\n",
            "  pred: ham ne us ke shahr ko chhoá¸ŒÄ aur ÄÃ±khoÃ± ko mauÃ±d liyÄ haiaid\n",
            "  ref : ham ne us ke shahr ko chhoá¸ŒÄ aur ÄÃ±khoÃ± ko muuÃ±d liyÄ hai\n",
            "[Ep 20] tf=0.50 | train ppl=1.30 | val ppl=1.16 | BLEU=63.04 | CER=0.199 | 47.1s\n",
            "  â†³ Saved best: /content/nmt_urdu_roman/models/bilstm4lstm_char_E256_H256_enc2_dec4_drop0.3_best.pt\n",
            "  pred: taaza rifÄqat ke mausam tak maiÃ± bhÄ« juyÄ huuÃ± vo bhÄ« jyÄ haib\n",
            "  ref : taaza rifÄqat ke mausam tak maiÃ± bhÄ« jiyÄ huuÃ± vo bhÄ« jiyÄ hai\n",
            "  pred: ham ne us ke shahr ko chhoá¸ŒÄ aur ÄÃ±khoÃ± ko hainand liyÄ haia\n",
            "  ref : ham ne us ke shahr ko chhoá¸ŒÄ aur ÄÃ±khoÃ± ko muuÃ±d liyÄ hai\n",
            "\n",
            "Best BLEU: 63.03771018975481  | ckpt: /content/nmt_urdu_roman/models/bilstm4lstm_char_E256_H256_enc2_dec4_drop0.3_best.pt\n",
            "Done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# Cell 5.1 â€” EOS-safe greedy decoding\n",
        "#   â€¢ Stops per sequence on EOS\n",
        "#   â€¢ Returns sequences trimmed to EOS\n",
        "#   â€¢ Optional simple no-repeat-3-gram guard\n",
        "# ==========================================\n",
        "import torch\n",
        "\n",
        "PAD_ID = 0; BOS_ID = 1; EOS_ID = 2\n",
        "\n",
        "def _has_repeat_ngram(ids, n=3):\n",
        "    if len(ids) < 2*n:\n",
        "        return False\n",
        "    last = tuple(ids[-n:])\n",
        "    for i in range(len(ids)-n*2+1):\n",
        "        if tuple(ids[i:i+n]) == last:\n",
        "            return True\n",
        "    return False\n",
        "\n",
        "def greedy_decode(model, batch, max_len=200, no_repeat_ngram=0):\n",
        "    \"\"\"\n",
        "    Returns a LongTensor [B, L] where each row is terminated by EOS (if produced)\n",
        "    and padded with EOS to equal length L.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        src = batch[\"src_ids\"].to(next(model.parameters()).device)\n",
        "        enc_outputs, (hn, cn) = model.encoder(src)\n",
        "        enc_mask = (src == PAD_ID)\n",
        "        dec_hc = model.bridge(hn, cn)\n",
        "        ctx = torch.zeros(src.size(0), enc_outputs.size(-1), device=src.device)\n",
        "        y_prev = torch.full((src.size(0),), BOS_ID, dtype=torch.long, device=src.device)\n",
        "\n",
        "        B = src.size(0)\n",
        "        finished = torch.zeros(B, dtype=torch.bool, device=src.device)\n",
        "        outputs = [[] for _ in range(B)]\n",
        "\n",
        "        for _ in range(max_len):\n",
        "            logits, dec_hc, ctx, _ = model.decoder(y_prev, dec_hc, enc_outputs, enc_mask, ctx)\n",
        "\n",
        "            # simple no-repeat-ngram guard (char/subword)\n",
        "            if no_repeat_ngram and no_repeat_ngram > 1:\n",
        "                for b in range(B):\n",
        "                    if not finished[b] and len(outputs[b]) >= no_repeat_ngram-1:\n",
        "                        # block last (n-1) history repeating as a new n-gram\n",
        "                        # (cheap heuristic: downweight the top-1 if it causes repeat)\n",
        "                        top1 = torch.argmax(logits[b])\n",
        "                        if _has_repeat_ngram(outputs[b] + [int(top1)], n=no_repeat_ngram):\n",
        "                            # pick 2nd best instead\n",
        "                            top2 = torch.topk(logits[b], k=2).indices[1]\n",
        "                            logits[b, top1] = -1e9\n",
        "                            logits[b, top2] += 1e-3  # tiny nudge\n",
        "\n",
        "            y_prev = torch.argmax(logits, dim=-1)\n",
        "\n",
        "            for b in range(B):\n",
        "                if finished[b]:\n",
        "                    continue\n",
        "                token = int(y_prev[b])\n",
        "                outputs[b].append(token)\n",
        "                if token == EOS_ID:\n",
        "                    finished[b] = True\n",
        "\n",
        "            if finished.all():\n",
        "                break\n",
        "\n",
        "        # pad with EOS to rectangular tensor\n",
        "        max_out = max(len(x) for x in outputs) if outputs else 1\n",
        "        out_tensor = torch.full((B, max_out), EOS_ID, dtype=torch.long, device=src.device)\n",
        "        for b, seq in enumerate(outputs):\n",
        "            if len(seq):\n",
        "                out_tensor[b, :len(seq)] = torch.tensor(seq, dtype=torch.long, device=src.device)\n",
        "        return out_tensor\n"
      ],
      "metadata": {
        "id": "hVMIX18dtHY4"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Colab Cell #6 â€” Load BEST ckpt â†’ Test metrics + translate()\n",
        "#   (standalone; assumes Cells 3â€“5 are already run)\n",
        "# ============================================================\n",
        "import os, json, math, random\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch, sentencepiece as spm\n",
        "import sacrebleu\n",
        "from jiwer import cer as jiwer_cer\n",
        "\n",
        "# ---- paths / device\n",
        "cfg = json.load(open(\"/content/nmt_urdu_roman/project_config.json\",\"r\",encoding=\"utf-8\"))\n",
        "PROJECT_DIR = Path(cfg[\"project_dir\"])\n",
        "ARTI        = PROJECT_DIR / \"artifacts\"\n",
        "MODELS_DIR  = PROJECT_DIR / \"models\"\n",
        "DATA_DIR    = PROJECT_DIR / \"data\"\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def find_best_ckpt():\n",
        "    cands = sorted(MODELS_DIR.glob(\"bilstm4lstm_*_best.pt\"), key=os.path.getmtime, reverse=True)\n",
        "    assert cands, \"No *_best.pt found. Train first.\"\n",
        "    return cands[0]\n",
        "\n",
        "best_path = find_best_ckpt()\n",
        "ckpt = torch.load(best_path, map_location=DEVICE)\n",
        "exp_used = ckpt[\"exp\"]\n",
        "print(\"Loaded:\", best_path.name, \"| tokenization:\", ckpt.get(\"tokenization\"), \"| BLEU(val):\", ckpt.get(\"bleu\"))\n",
        "\n",
        "# ---- Seq2Seq & greedy_decode must exist from Cell #5\n",
        "assert \"Seq2Seq\" in globals() and \"greedy_decode\" in globals(), \"Please re-run Cell #5.\"\n",
        "\n",
        "# ---- vocab sizes + decode\n",
        "PAD_ID = 0; BOS_ID = 1; EOS_ID = 2; UNK_ID = 3\n",
        "\n",
        "if exp_used[\"tokenization\"] == \"char\":\n",
        "    v_src = json.load(open(ARTI / \"vocab_src_char.json\",\"r\",encoding=\"utf-8\"))\n",
        "    v_tgt = json.load(open(ARTI / \"vocab_tgt_char.json\",\"r\",encoding=\"utf-8\"))\n",
        "    SRC_V = len(v_src[\"itos\"]); TGT_V = len(v_tgt[\"itos\"])\n",
        "    itos_tgt = v_tgt[\"itos\"]\n",
        "    def decode_fn(ids):\n",
        "        if isinstance(ids, torch.Tensor): ids = ids.tolist()\n",
        "        return \"\".join(itos_tgt[i] for i in ids if i not in (PAD_ID,BOS_ID,EOS_ID)).strip()\n",
        "else:\n",
        "    sp_src = spm.SentencePieceProcessor(); sp_src.load(str(ARTI / \"spm_src.model\"))\n",
        "    sp_tgt = spm.SentencePieceProcessor(); sp_tgt.load(str(ARTI / \"spm_tgt.model\"))\n",
        "    SRC_V = sp_src.get_piece_size(); TGT_V = sp_tgt.get_piece_size()\n",
        "    def decode_fn(ids):\n",
        "        if isinstance(ids, torch.Tensor): ids = ids.tolist()\n",
        "        ids = [i for i in ids if i not in (PAD_ID,BOS_ID,EOS_ID)]\n",
        "        try: return sp_tgt.decode(ids).strip()\n",
        "        except: return sp_tgt.decode_pieces([sp_tgt.id_to_piece(i) for i in ids]).strip()\n",
        "\n",
        "# ---- rebuild model AFTER we know SRC_V/TGT_V\n",
        "model = Seq2Seq(SRC_V, TGT_V,\n",
        "                exp_used[\"embedding_dim\"], exp_used[\"hidden_size\"],\n",
        "                exp_used[\"enc_layers\"],   exp_used[\"dec_layers\"],\n",
        "                exp_used[\"dropout\"]).to(DEVICE)\n",
        "model.load_state_dict(ckpt[\"model_state\"])\n",
        "model.eval()\n",
        "\n",
        "# ---- test loader via make_loaders from Cell #4\n",
        "assert \"make_loaders\" in globals(), \"Please re-run Cell #4.\"\n",
        "_, _, _, _, _, dl_test = make_loaders(mode=exp_used[\"tokenization\"], batch_size=exp_used[\"batch_size\"])\n",
        "\n",
        "def eval_on_loader(model, dl):\n",
        "    preds_text, refs_text = [], []\n",
        "    total_loss, total_tokens = 0.0, 0\n",
        "    with torch.no_grad():\n",
        "        for batch in dl:\n",
        "            src = batch[\"src_ids\"].to(DEVICE)\n",
        "            tgt = batch[\"tgt_ids\"].to(DEVICE)\n",
        "\n",
        "            # loss (teacher-forced)\n",
        "            logits = model(src, tgt, teacher_forcing=1.0)\n",
        "            B, Tm1, V = logits.shape\n",
        "            gold = tgt[:,1:1+Tm1]\n",
        "            loss = torch.nn.functional.cross_entropy(\n",
        "                logits.reshape(B*Tm1, V),\n",
        "                gold.reshape(B*Tm1),\n",
        "                ignore_index=PAD_ID\n",
        "            )\n",
        "            ntok = (gold != PAD_ID).sum().item()\n",
        "            total_loss += loss.item() * ntok\n",
        "            total_tokens += ntok\n",
        "\n",
        "            # greedy decode\n",
        "            gen = greedy_decode(model, batch, max_len=tgt.size(1)-1)\n",
        "            preds_text.extend([decode_fn(row) for row in gen])\n",
        "            refs_text.extend([decode_fn(row) for row in tgt])\n",
        "\n",
        "    avg_nll = total_loss / max(total_tokens,1)\n",
        "    ppl = math.exp(avg_nll)\n",
        "    bleu = sacrebleu.corpus_bleu(preds_text, [refs_text]).score\n",
        "    cer_scores = [jiwer_cer(r, p) for p, r in zip(preds_text, refs_text)]\n",
        "    cer_mean = float(np.mean(cer_scores)) if cer_scores else 1.0\n",
        "    return ppl, bleu, cer_mean\n",
        "\n",
        "test_ppl, test_bleu, test_cer = eval_on_loader(model, dl_test)\n",
        "print(f\"\\n=== TEST ===\\nPPL: {test_ppl:.3f} | BLEU: {test_bleu:.2f} | CER: {test_cer:.3f}\")\n",
        "\n",
        "# ---- simple translator\n",
        "def translate(texts, max_len=200):\n",
        "    batch = {\"src_ids\": [], \"tgt_ids\": []}\n",
        "    if exp_used[\"tokenization\"] == \"char\":\n",
        "        stoi = json.load(open(ARTI / \"vocab_src_char.json\",\"r\",encoding=\"utf-8\"))[\"stoi\"]\n",
        "        for t in texts:\n",
        "            ids = [BOS_ID] + [stoi.get(ch, UNK_ID) for ch in t] + [EOS_ID]\n",
        "            batch[\"src_ids\"].append(torch.tensor(ids, dtype=torch.long))\n",
        "            batch[\"tgt_ids\"].append(torch.tensor([BOS_ID, EOS_ID], dtype=torch.long))\n",
        "    else:\n",
        "        for t in texts:\n",
        "            ids = [BOS_ID] + sp_src.encode(t, out_type=int) + [EOS_ID]\n",
        "            batch[\"src_ids\"].append(torch.tensor(ids, dtype=torch.long))\n",
        "            batch[\"tgt_ids\"].append(torch.tensor([BOS_ID, EOS_ID], dtype=torch.long))\n",
        "\n",
        "    def pad(lst):\n",
        "        m = max(len(x) for x in lst)\n",
        "        out = torch.full((len(lst), m), PAD_ID, dtype=torch.long)\n",
        "        for i,x in enumerate(lst): out[i,:len(x)] = x\n",
        "        return out\n",
        "\n",
        "    batch = {k: pad(v).to(DEVICE) for k,v in batch.items()}\n",
        "    gen = greedy_decode(model, batch, max_len=max_len)\n",
        "    return [decode_fn(row) for row in gen]\n",
        "\n",
        "# demo on a few test lines\n",
        "test_df = pd.read_parquet(DATA_DIR / \"pairs_test.parquet\")\n",
        "samps = test_df.sample(5, random_state=7)[[\"src_ur\",\"tgt_rom\"]].to_dict(\"records\")\n",
        "preds = translate([s[\"src_ur\"] for s in samps], max_len=120)  # shorter max_len\n",
        "\n",
        "\n",
        "print(\"\\n=== Qualitative (5 random from TEST) ===\")\n",
        "for i, s in enumerate(samps):\n",
        "    print(f\"{i+1:02d}. UR : {s['src_ur']}\")\n",
        "    print(f\"    GT : {s['tgt_rom']}\")\n",
        "    print(f\"    PR : {preds[i]}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PM_tvh7YbT29",
        "outputId": "77aa34d5-7fb6-4e93-8ad6-8af7c943e210"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded: bilstm4lstm_char_E256_H256_enc2_dec4_drop0.3_best.pt | tokenization: char | BLEU(val): 63.03771018975481\n",
            "\n",
            "=== TEST ===\n",
            "PPL: 1.137 | BLEU: 75.15 | CER: 0.049\n",
            "\n",
            "=== Qualitative (5 random from TEST) ===\n",
            "01. UR : ÛÙˆ Ø±ÛÛ’ Ú¯Ø§ Ú©Ú†Û Ù†Û Ú©Ú†Û Ú¯ÛØ¨Ø±Ø§ÛŒÛŒÚº Ú©ÛŒØ§\n",
            "    GT : ho rahegÄ kuchh na kuchh ghabrÄ.eÃ± kyÄ\n",
            "    PR : ho rahegÄ kuchh na kuchh ghabrÄ.eÃ± kyÄ\n",
            "02. UR : Ù„Ú¯ Ú¯ÛŒÛŒ Ú†Ù¾ Ø­Ø§Ù„ÛŒØ” Ø±Ù†Ø¬ÙˆØ± Ú©Ùˆ\n",
            "    GT : lag ga.Ä« chup 'hÄlÄ«'-e-ranjÅ«r ko\n",
            "    PR : lag ga.Ä« chup 'hÄlÄ«' ranjÅ«r ko\n",
            "03. UR : Ù…Ø±ÛŒ Ø·Ø±Ø­ Ø¨ÛÛŒ Ú©ÙˆÛŒÛŒ Ù…ÛŒØ±Ø§ ØºÙ… Ú¯Ø³Ø§Ø± Ù†Û ÛÙˆ\n",
            "    GT : mirÄ« tarah bhÄ« koÄ« merÄ Ä¡ham-gusÄr na ho\n",
            "    PR : mirÄ« tarah bhÄ« koÄ« merÄ Ä¡ham-gusÄr na ho\n",
            "04. UR : Ø³ÙˆØ±Ø¬ Ø¯Ù…Ø§Øº Ù„ÙˆÚ¯ Ø¨ÛÛŒ Ø§Ø¨Ù„Ø§Øº ÙÚ©Ø± Ù…ÛŒÚº\n",
            "    GT : sÅ«raj-dimÄÄ¡h log bhÄ« ablÄÄ¡h-e-fikr meÃ±\n",
            "    PR : sÅ«raj-e-dimÄÄ¡h log bhÄ« iblÄÄ¡h-e-fikr meÃ±\n",
            "05. UR : Ù…ÙˆÛŒÛ’ Ø´ÛŒØ´Û Ø¯ÛŒØ¯Û‚ Ø³Ø§ØºØ± Ú©ÛŒ Ù…Ú˜Ú¯Ø§Ù†ÛŒ Ú©Ø±Û’\n",
            "    GT : mÅ«-e-shÄ«sha dÄ«da-e-sÄÄ¡har kÄ« mizhgÄnÄ« kare\n",
            "    PR : mÅ«-e-shÄ«sha dÄ«da-e-sÄÄ¡har kÄ« mizhgÄnÄ« kare\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Mini experiment sweep (robust to eval_on_loader signature)\n",
        "#   - Runs: char_H512, char_LR1e-3, spm_baseline (if SPM available)\n",
        "#   - Saves CSV + BLEU bar plot\n",
        "# ============================================================\n",
        "import os, json, time, math, numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "import torch, inspect\n",
        "\n",
        "# --- Expect Cell #4 and #5 already run\n",
        "assert 'train_model' in globals(), \"train_model not found. Run Cell #5.\"\n",
        "assert 'Seq2Seq' in globals(), \"Seq2Seq not found. Run Cell #5.\"\n",
        "assert 'DEVICE' in globals(), \"DEVICE not found. Run Cell #5.\"\n",
        "assert 'ARTI' in globals(), \"ARTI not found. Run Cell #5.\"\n",
        "assert 'get_loaders' in globals(), \"get_loaders not found. Run Cell #5.\"\n",
        "\n",
        "# ---- Shim eval_on_loader to support BOTH signatures\n",
        "assert 'eval_on_loader' in globals(), \"eval_on_loader not found. Run Cell #5.\"\n",
        "_orig_eval_on_loader = eval_on_loader  # keep original\n",
        "\n",
        "def eval_on_loader(*args, **kwargs):\n",
        "    \"\"\"\n",
        "    Wrapper that supports:\n",
        "      - old: eval_on_loader(model, dl)\n",
        "      - new: eval_on_loader(model, dl, decode_fn, mode)\n",
        "    \"\"\"\n",
        "    sig = inspect.signature(_orig_eval_on_loader)\n",
        "    if len(sig.parameters) == 2:\n",
        "        # old signature â€” just hand it model, dl\n",
        "        model, dl = args[:2]\n",
        "        return _orig_eval_on_loader(model, dl)\n",
        "    # new signature â€” pass through\n",
        "    return _orig_eval_on_loader(*args, **kwargs)\n",
        "\n",
        "# ---- Paths / base config\n",
        "cfg = json.load(open(\"/content/nmt_urdu_roman/project_config.json\",\"r\",encoding=\"utf-8\"))\n",
        "PROJECT_DIR = Path(cfg[\"project_dir\"])\n",
        "RUNS_DIR = PROJECT_DIR / \"runs\"; RUNS_DIR.mkdir(parents=True, exist_ok=True)\n",
        "MODELS_DIR = PROJECT_DIR / \"models\"\n",
        "\n",
        "base = json.load(open(ARTI / \"exp_default.json\",\"r\",encoding=\"utf-8\"))\n",
        "BASE_EPOCHS = 6\n",
        "BASE_BATCH  = base.get(\"batch_size\", 64)\n",
        "\n",
        "# SPM availability\n",
        "SPM_OK = (ARTI/'spm_src.model').exists() and (ARTI/'spm_tgt.model').exists()\n",
        "if not SPM_OK:\n",
        "    print(\"âš ï¸ SPM model files not found. The SPM config will be skipped.\")\n",
        "\n",
        "# ---- Sweep definitions\n",
        "sweep = [\n",
        "    {\"name\": \"char_H512\",   \"overrides\": {\"tokenization\": \"char\", \"hidden_size\": 512, \"epochs\": BASE_EPOCHS, \"batch_size\": BASE_BATCH}},\n",
        "    {\"name\": \"char_LR1e-3\", \"overrides\": {\"tokenization\": \"char\", \"learning_rate\": 1e-3, \"epochs\": BASE_EPOCHS, \"batch_size\": BASE_BATCH}},\n",
        "]\n",
        "if SPM_OK:\n",
        "    sweep.append({\"name\": \"spm_baseline\", \"overrides\": {\"tokenization\": \"spm\", \"epochs\": BASE_EPOCHS, \"batch_size\": BASE_BATCH}})\n",
        "\n",
        "# ---- Helper to get decode function only when needed\n",
        "def _decode_fn_for(tokenization):\n",
        "    if tokenization == \"char\":\n",
        "        v_tgt = json.load(open(ARTI / \"vocab_tgt_char.json\",\"r\",encoding=\"utf-8\"))\n",
        "        itos_tgt = v_tgt[\"itos\"]\n",
        "        PAD_ID, BOS_ID, EOS_ID = 0,1,2\n",
        "        def decode_char(ids):\n",
        "            return \"\".join(itos_tgt[i] for i in ids if i not in (PAD_ID,BOS_ID,EOS_ID) and 0<=i<len(itos_tgt)).strip()\n",
        "        return decode_char\n",
        "    else:\n",
        "        import sentencepiece as spm\n",
        "        PAD_ID, BOS_ID, EOS_ID = 0,1,2\n",
        "        sp_tgt = spm.SentencePieceProcessor(); sp_tgt.load(str(ARTI / \"spm_tgt.model\"))\n",
        "        def decode_spm(ids):\n",
        "            ids = [i for i in ids if i not in (PAD_ID,BOS_ID,EOS_ID)]\n",
        "            try: return sp_tgt.decode(ids).strip()\n",
        "            except: return sp_tgt.decode_pieces([sp_tgt.id_to_piece(i) for i in ids]).strip()\n",
        "        return decode_spm\n",
        "\n",
        "results = []\n",
        "\n",
        "for job in sweep:\n",
        "    name = job[\"name\"]\n",
        "    overrides = dict(base); overrides.update(job[\"overrides\"])\n",
        "    print(f\"\\n=== Running: {name} ===\")\n",
        "    t0 = time.time()\n",
        "    try:\n",
        "        ckpt_path = train_model(exp_overrides=overrides)\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Training failed for {name}: {e}\")\n",
        "        continue\n",
        "    dt = time.time() - t0\n",
        "\n",
        "    if not ckpt_path or not Path(ckpt_path).exists():\n",
        "        print(f\"âŒ No checkpoint produced for {name}\")\n",
        "        continue\n",
        "\n",
        "    meta = torch.load(ckpt_path, map_location=\"cpu\")\n",
        "    best_bleu = float(meta.get(\"bleu\", float('nan')))\n",
        "    exp_used  = meta.get(\"exp\", overrides)\n",
        "    tokenization = exp_used[\"tokenization\"]\n",
        "\n",
        "    # Rebuild model to recompute val ppl/CER\n",
        "    if tokenization == \"char\":\n",
        "        v_src = json.load(open(ARTI / \"vocab_src_char.json\",\"r\",encoding=\"utf-8\"))\n",
        "        v_tgt = json.load(open(ARTI / \"vocab_tgt_char.json\",\"r\",encoding=\"utf-8\"))\n",
        "        src_v = len(v_src[\"itos\"]); tgt_v = len(v_tgt[\"itos\"])\n",
        "    else:\n",
        "        import sentencepiece as spm\n",
        "        sp_src = spm.SentencePieceProcessor(); sp_src.load(str(ARTI / \"spm_src.model\"))\n",
        "        sp_tgt = spm.SentencePieceProcessor(); sp_tgt.load(str(ARTI / \"spm_tgt.model\"))\n",
        "        src_v = sp_src.get_piece_size(); tgt_v = sp_tgt.get_piece_size()\n",
        "\n",
        "    model = Seq2Seq(\n",
        "        src_vsize=src_v, tgt_vsize=tgt_v,\n",
        "        emb_dim=exp_used[\"embedding_dim\"], hid_size=exp_used[\"hidden_size\"],\n",
        "        enc_layers=exp_used[\"enc_layers\"], dec_layers=exp_used[\"dec_layers\"],\n",
        "        dropout=exp_used[\"dropout\"]\n",
        "    ).to(DEVICE)\n",
        "    state = torch.load(ckpt_path, map_location=DEVICE)[\"model_state\"]\n",
        "    model.load_state_dict(state)\n",
        "\n",
        "    # loaders + eval on VAL (signature-agnostic)\n",
        "    _, _, _, dl_train_tmp, dl_val_tmp, _ = get_loaders(tokenization, batch=exp_used[\"batch_size\"])\n",
        "    decode_fn = _decode_fn_for(tokenization)\n",
        "    try:\n",
        "        val_nll, val_ppl, val_bleu2, val_cer, _, _ = eval_on_loader(model, dl_val_tmp, decode_fn, tokenization)\n",
        "    except TypeError:\n",
        "        val_nll, val_ppl, val_bleu2, val_cer, _, _ = eval_on_loader(model, dl_val_tmp)\n",
        "\n",
        "    results.append({\n",
        "        \"name\": name,\n",
        "        \"tokenization\": tokenization,\n",
        "        \"embedding_dim\": exp_used[\"embedding_dim\"],\n",
        "        \"hidden_size\": exp_used[\"hidden_size\"],\n",
        "        \"learning_rate\": exp_used[\"learning_rate\"],\n",
        "        \"epochs\": exp_used[\"epochs\"],\n",
        "        \"batch_size\": exp_used[\"batch_size\"],\n",
        "        \"best_bleu_val_ckpt\": float(best_bleu),\n",
        "        \"val_ppl_reval\": float(val_ppl),\n",
        "        \"val_cer_reval\": float(val_cer),\n",
        "        \"train_time_s\": round(dt, 1),\n",
        "        \"ckpt_path\": str(ckpt_path),\n",
        "    })\n",
        "\n",
        "# ---- Save/show table safely\n",
        "if results:\n",
        "    df = pd.DataFrame(results).sort_values(\"best_bleu_val_ckpt\", ascending=False)\n",
        "    out_csv = RUNS_DIR / \"exp_results_sweep.csv\"\n",
        "    df.to_csv(out_csv, index=False)\n",
        "    print(\"\\nâœ… Saved sweep results:\", out_csv)\n",
        "    display(df)\n",
        "\n",
        "    # BLEU bar chart\n",
        "    plt.figure(figsize=(8,4))\n",
        "    x = np.arange(len(df))\n",
        "    plt.bar(x, df[\"best_bleu_val_ckpt\"])\n",
        "    plt.xticks(x, df[\"name\"], rotation=15, ha='right')\n",
        "    plt.ylabel(\"Best Val BLEU (ckpt meta)\")\n",
        "    plt.title(\"Mini Sweep â€” Best Validation BLEU\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"\\nâš ï¸ No successful runs to summarize (results list is empty).\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oo9Kdd3e0rtB",
        "outputId": "2108b107-09df-4e15-f361-5c1e322344ff"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Running: char_H512 ===\n",
            "âŒ Training failed for char_H512: not enough values to unpack (expected 6, got 3)\n",
            "\n",
            "=== Running: char_LR1e-3 ===\n",
            "âŒ Training failed for char_LR1e-3: not enough values to unpack (expected 6, got 3)\n",
            "\n",
            "=== Running: spm_baseline ===\n",
            "âŒ Training failed for spm_baseline: list index out of range\n",
            "\n",
            "âš ï¸ No successful runs to summarize (results list is empty).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save everything you need to /MyDrive\n",
        "from pathlib import Path\n",
        "import shutil, json, torch\n",
        "\n",
        "ROOT = Path(\"/content\")\n",
        "PROJ = ROOT / \"nmt_urdu_roman\"\n",
        "GDR  = Path(\"/content/drive/MyDrive/nmt_urdu_roman_export\")\n",
        "GDR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "for p in [\"models\",\"artifacts\",\"exp_results.csv\"]:\n",
        "    src = (PROJ / p) if p.endswith(\".csv\") else (PROJ / \"runs\" / p if (PROJ/\"runs\"/p).exists() else PROJ / p)\n",
        "    if src.exists():\n",
        "        dst = GDR / p\n",
        "        if src.is_dir():\n",
        "            shutil.copytree(src, dst, dirs_exist_ok=True)\n",
        "        else:\n",
        "            shutil.copy2(src, dst)\n",
        "\n",
        "# small manifest\n",
        "ckpt = \"/content/nmt_urdu_roman/models/bilstm4lstm_char_E256_H256_enc2_dec4_drop0.3_best.pt\"\n",
        "meta = torch.load(ckpt, map_location=\"cpu\")\n",
        "(Path(GDR/\"MANIFEST.json\")).write_text(json.dumps({\n",
        "    \"best_ckpt\": ckpt,\n",
        "    \"tokenization\": meta.get(\"tokenization\"),\n",
        "    \"bleu_val\": float(meta.get(\"bleu\", -1)),\n",
        "}, indent=2), encoding=\"utf-8\")\n",
        "\n",
        "print(\"âœ… Exported to:\", GDR)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZJi5HUZFrEt0",
        "outputId": "69afc2e3-c5bc-47be-b00a-8ef3ebad945a"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Exported to: /content/drive/MyDrive/nmt_urdu_roman_export\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Colab Cell #7 â€” Reusable inference (greedy or beam), batch API\n",
        "# ============================================================\n",
        "import json, torch\n",
        "from pathlib import Path\n",
        "import sentencepiece as spm\n",
        "\n",
        "# Paths\n",
        "cfg = json.load(open(\"/content/nmt_urdu_roman/project_config.json\",\"r\",encoding=\"utf-8\"))\n",
        "PROJECT_DIR = Path(cfg[\"project_dir\"])\n",
        "ARTI        = PROJECT_DIR / \"artifacts\"\n",
        "MODELS_DIR  = PROJECT_DIR / \"models\"\n",
        "\n",
        "# Load best checkpoint (adjust if you saved multiple)\n",
        "ckpts = sorted(MODELS_DIR.glob(\"bilstm4lstm_*_best.pt\"))\n",
        "assert ckpts, \"No best checkpoints found in models/.\"\n",
        "CKPT_PATH = ckpts[-1]\n",
        "ckpt = torch.load(CKPT_PATH, map_location=\"cpu\")\n",
        "exp_used = ckpt[\"exp\"]\n",
        "tokenization = ckpt.get(\"tokenization\", exp_used.get(\"tokenization\", \"char\"))\n",
        "print(f\"Using ckpt: {CKPT_PATH.name} | tokenization={tokenization}\")\n",
        "\n",
        "# Bring model defs from Cell #5\n",
        "assert \"Seq2Seq\" in globals(), \"Please re-run Cell #5 first (model classes).\"\n",
        "\n",
        "# Vocab / tokenizers\n",
        "PAD_ID=0; BOS_ID=1; EOS_ID=2; UNK_ID=3\n",
        "if tokenization == \"char\":\n",
        "    v_src = json.load(open(ARTI / \"vocab_src_char.json\",\"r\",encoding=\"utf-8\"))\n",
        "    v_tgt = json.load(open(ARTI / \"vocab_tgt_char.json\",\"r\",encoding=\"utf-8\"))\n",
        "    stoi_src, itos_tgt = v_src[\"stoi\"], v_tgt[\"itos\"]\n",
        "    SRC_V, TGT_V = len(v_src[\"itos\"]), len(v_tgt[\"itos\"])\n",
        "    def enc_src(s):\n",
        "        return [BOS_ID] + [stoi_src.get(ch, UNK_ID) for ch in s] + [EOS_ID]\n",
        "    def dec_tgt(ids):\n",
        "        return \"\".join(itos_tgt[i] for i in ids if i not in (PAD_ID,BOS_ID,EOS_ID)).strip()\n",
        "else:\n",
        "    sp_src = spm.SentencePieceProcessor(); sp_src.load(str(ARTI/\"spm_src.model\"))\n",
        "    sp_tgt = spm.SentencePieceProcessor(); sp_tgt.load(str(ARTI/\"spm_tgt.model\"))\n",
        "    SRC_V, TGT_V = sp_src.get_piece_size(), sp_tgt.get_piece_size()\n",
        "    def enc_src(s):\n",
        "        return [BOS_ID] + sp_src.encode(s, out_type=int) + [EOS_ID]\n",
        "    def dec_tgt(ids):\n",
        "        ids = [i for i in ids if i not in (PAD_ID,BOS_ID,EOS_ID)]\n",
        "        try: return sp_tgt.decode(ids).strip()\n",
        "        except: return sp_tgt.decode_pieces([sp_tgt.id_to_piece(i) for i in ids]).strip()\n",
        "\n",
        "# Rebuild model & load weights\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = Seq2Seq(SRC_V, TGT_V,\n",
        "                exp_used[\"embedding_dim\"], exp_used[\"hidden_size\"],\n",
        "                exp_used[\"enc_layers\"], exp_used[\"dec_layers\"],\n",
        "                exp_used[\"dropout\"]).to(DEVICE)\n",
        "model.load_state_dict(ckpt[\"model_state\"])\n",
        "model.eval()\n",
        "\n",
        "# --- Greedy (EOS-safe) from Cell 5.1 (assume it's already redefined) ---\n",
        "assert \"greedy_decode\" in globals(), \"Please run the EOS-safe greedy from earlier.\"\n",
        "\n",
        "# --- Lightweight Beam Search (beam=5) ---\n",
        "import heapq\n",
        "def beam_search(model, src_ids, beam_size=5, max_len=200, length_penalty=0.7):\n",
        "    \"\"\"\n",
        "    src_ids: 1D LongTensor on DEVICE including BOS/EOS around source.\n",
        "    returns: list[int] best hypothesis token ids (no BOS, ends at EOS if emitted).\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        src = src_ids.unsqueeze(0)  # [1, Tsrc]\n",
        "        enc_outputs, (hn, cn) = model.encoder(src)\n",
        "        enc_mask = (src == PAD_ID)\n",
        "        dec_hc = model.bridge(hn, cn)\n",
        "        ctx = torch.zeros(1, enc_outputs.size(-1), device=src.device)\n",
        "        # beams: list of tuples (-score, seq, dec_hc, ctx, last_token)\n",
        "        start = (0.0, [], dec_hc, ctx, torch.tensor([BOS_ID], device=src.device))\n",
        "        beams = [start]\n",
        "        finished = []\n",
        "\n",
        "        for _ in range(max_len):\n",
        "            new_beams = []\n",
        "            for score, seq, dec_hc, ctx, y_prev in beams:\n",
        "                if len(seq) and seq[-1] == EOS_ID:\n",
        "                    finished.append((score, seq))\n",
        "                    continue\n",
        "                logits, dec_hc_n, ctx_n, _ = model.decoder(y_prev, dec_hc, enc_outputs, enc_mask, ctx)\n",
        "                logp = torch.log_softmax(logits, dim=-1).squeeze(0)  # [V]\n",
        "                topk = torch.topk(logp, k=beam_size)\n",
        "                for k in range(beam_size):\n",
        "                    tok = int(topk.indices[k])\n",
        "                    sc = score - float(topk.values[k])  # negative log-prob\n",
        "                    new_beams.append((sc, seq+[tok], dec_hc_n, ctx_n, torch.tensor([tok], device=src.device)))\n",
        "            # keep top-K\n",
        "            beams = heapq.nsmallest(beam_size, new_beams, key=lambda x: x[0])\n",
        "            # early stop if we already have K finished with EOS\n",
        "            if len(finished) >= beam_size and all(seq and seq[-1]==EOS_ID for _,seq in finished[:beam_size]):\n",
        "                break\n",
        "\n",
        "        cand = finished if finished else beams\n",
        "        # length penalty: favor slightly longer but reasonable sequences\n",
        "        cand = [(sc / (len(seq) ** length_penalty if len(seq)>0 else 1.0), seq) for sc, seq in cand]\n",
        "        best = min(cand, key=lambda x: x[0])[1]\n",
        "        return best\n",
        "\n",
        "# --- Public functions ---\n",
        "def translate_one(text, decoder=\"greedy\", max_len=None, beam_size=5):\n",
        "    ids = enc_src(text)\n",
        "    if max_len is None:\n",
        "        max_len = max(30, len(ids) + 10)  # safe cap\n",
        "    src_tensor = torch.tensor(ids, dtype=torch.long, device=DEVICE)\n",
        "\n",
        "    if decoder == \"beam\":\n",
        "        out = beam_search(model, src_tensor, beam_size=beam_size, max_len=max_len)\n",
        "        return dec_tgt(out)\n",
        "    else:\n",
        "        batch = {\n",
        "            \"src_ids\": src_tensor.unsqueeze(0),\n",
        "            \"tgt_ids\": torch.tensor([[BOS_ID, EOS_ID]], dtype=torch.long, device=DEVICE)\n",
        "        }\n",
        "        gen = greedy_decode(model, batch, max_len=max_len)\n",
        "        return dec_tgt(gen[0].tolist())\n",
        "\n",
        "def translate_batch(texts, decoder=\"greedy\", max_len=None, beam_size=5):\n",
        "    outs = []\n",
        "    for t in texts:\n",
        "        outs.append(translate_one(t, decoder=decoder, max_len=max_len, beam_size=beam_size))\n",
        "    return outs\n",
        "\n",
        "print(\"âœ… Inference ready. Example:\")\n",
        "print(\"UR  :\", \"Ù…Ø²Û’ Ø¬ÛØ§Ù† Ú©Û’ Ø§Ù¾Ù†ÛŒ Ù†Ø¸Ø± Ù…ÛŒÚº Ø®Ø§Ú© Ù†ÛÛŒÚº\")\n",
        "print(\"ROM :\", translate_one(\"Ù…Ø²Û’ Ø¬ÛØ§Ù† Ú©Û’ Ø§Ù¾Ù†ÛŒ Ù†Ø¸Ø± Ù…ÛŒÚº Ø®Ø§Ú© Ù†ÛÛŒÚº\", decoder=\"beam\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vmULxZDltkOk",
        "outputId": "9349cf3f-5c44-4f24-dd64-4c8f3b02b01f"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using ckpt: bilstm4lstm_char_E256_H256_enc2_dec4_drop0.3_best.pt | tokenization=char\n",
            "âœ… Inference ready. Example:\n",
            "UR  : Ù…Ø²Û’ Ø¬ÛØ§Ù† Ú©Û’ Ø§Ù¾Ù†ÛŒ Ù†Ø¸Ø± Ù…ÛŒÚº Ø®Ø§Ú© Ù†ÛÛŒÚº\n",
            "ROM : maze jahÄn ke apnÄ« nazar meÃ± á¸³haak nahÄ«Ã±\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Cell #8 (fixed) â€” Batch translate & export CSV\n",
        "#   * Robust beam search tuples (score, seq, h_c, ctx)\n",
        "#   * Works with tokenization=\"char\" or \"spm\"\n",
        "#   * Saves /content/nmt_urdu_roman/runs/preds_test.csv\n",
        "# ============================================================\n",
        "import os, json, math\n",
        "from pathlib import Path\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import pandas as pd\n",
        "\n",
        "# --------- Common constants / device ---------\n",
        "PAD_ID = 0; BOS_ID = 1; EOS_ID = 2; UNK_ID = 3\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# --------- Paths and config ---------\n",
        "cfg = json.load(open(\"/content/nmt_urdu_roman/project_config.json\",\"r\",encoding=\"utf-8\"))\n",
        "PROJECT_DIR = Path(cfg[\"project_dir\"])\n",
        "ARTI        = PROJECT_DIR / \"artifacts\"\n",
        "DATA_DIR    = PROJECT_DIR / \"data\"\n",
        "RUNS_DIR    = PROJECT_DIR / \"runs\"\n",
        "RUNS_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# --------- Helper: load checkpoint / model / tokenizers ---------\n",
        "def load_model_and_tok(ckpt_path=None):\n",
        "    # Try to reuse globals if they exist\n",
        "    global best_model, exp_used\n",
        "    if ckpt_path is None:\n",
        "        # Pick the best *.pt in models dir if not given\n",
        "        models = sorted((PROJECT_DIR / \"models\").glob(\"*.pt\"), key=os.path.getmtime, reverse=True)\n",
        "        assert models, \"No checkpoints found in /models. Train first.\"\n",
        "        ckpt_path = models[0]\n",
        "\n",
        "    ckpt = torch.load(ckpt_path, map_location=DEVICE)\n",
        "    exp_local = ckpt[\"exp\"]\n",
        "    tokenization = ckpt.get(\"tokenization\", exp_local.get(\"tokenization\", \"char\"))\n",
        "\n",
        "    # Build model skeleton (reuse class from Cell #5)\n",
        "    assert \"Seq2Seq\" in globals(), \"Seq2Seq not found. Re-run Cell #5.\"\n",
        "    # Get vocab sizes\n",
        "    if tokenization == \"char\":\n",
        "        v_src = json.load(open(ARTI / \"vocab_src_char.json\",\"r\",encoding=\"utf-8\"))\n",
        "        v_tgt = json.load(open(ARTI / \"vocab_tgt_char.json\",\"r\",encoding=\"utf-8\"))\n",
        "        src_v = len(v_src[\"itos\"]); tgt_v = len(v_tgt[\"itos\"])\n",
        "    else:\n",
        "        import sentencepiece as spm\n",
        "        sp_src = spm.SentencePieceProcessor(); sp_src.load(str(ARTI / \"spm_src.model\"))\n",
        "        sp_tgt = spm.SentencePieceProcessor(); sp_tgt.load(str(ARTI / \"spm_tgt.model\"))\n",
        "        src_v = sp_src.get_piece_size(); tgt_v = sp_tgt.get_piece_size()\n",
        "\n",
        "    model = Seq2Seq(src_v, tgt_v,\n",
        "                    exp_local[\"embedding_dim\"], exp_local[\"hidden_size\"],\n",
        "                    exp_local[\"enc_layers\"], exp_local[\"dec_layers\"],\n",
        "                    exp_local[\"dropout\"]).to(DEVICE)\n",
        "    model.load_state_dict(ckpt[\"model_state\"])\n",
        "    model.eval()\n",
        "\n",
        "    # Tokenizers & decode from Cell #5 utility\n",
        "    assert \"_load_tokenizers\" in globals(), \"_load_tokenizers not found. Re-run Cell #5.\"\n",
        "    tok = _load_tokenizers(tokenization)\n",
        "\n",
        "    # For char-mode encoding we need src stoi\n",
        "    if tokenization == \"char\":\n",
        "        v_src = json.load(open(ARTI / \"vocab_src_char.json\",\"r\",encoding=\"utf-8\"))\n",
        "        stoi_src = v_src[\"stoi\"]\n",
        "    else:\n",
        "        stoi_src = None  # weâ€™ll use tok[\"sp_src\"]\n",
        "\n",
        "    return model, exp_local, tok, stoi_src, ckpt_path\n",
        "\n",
        "best_model, exp_used, tok, stoi_src_char, used_ckpt = load_model_and_tok()\n",
        "print(f\"Using ckpt: {Path(used_ckpt).name} | tokenization={exp_used['tokenization']}\")\n",
        "\n",
        "# --------- Encoding / decoding helpers ---------\n",
        "def encode_src_text(s: str, tokenization: str):\n",
        "    if tokenization == \"char\":\n",
        "        ids = [BOS_ID] + [stoi_src_char.get(ch, UNK_ID) for ch in s] + [EOS_ID]\n",
        "    else:\n",
        "        ids = [BOS_ID] + tok[\"sp_src\"].encode(s, out_type=int) + [EOS_ID]\n",
        "    t = torch.tensor(ids, dtype=torch.long, device=DEVICE).unsqueeze(0)  # [1,T]\n",
        "    return t\n",
        "\n",
        "def decode_ids(ids: torch.Tensor):\n",
        "    # ids: [L] on cpu or gpu\n",
        "    arr = ids.tolist() if isinstance(ids, torch.Tensor) else ids\n",
        "    return tok[\"decode\"](arr)\n",
        "\n",
        "# --------- Greedy (quick) ---------\n",
        "def greedy_one(model, src_ids, max_len=200):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        enc_out, (hn, cn) = model.encoder(src_ids)\n",
        "        enc_mask = (src_ids == PAD_ID)\n",
        "        dec_hc = model.bridge(hn, cn)\n",
        "        ctx = torch.zeros(src_ids.size(0), enc_out.size(-1), device=src_ids.device)\n",
        "        y_prev = torch.full((src_ids.size(0),), BOS_ID, dtype=torch.long, device=src_ids.device)\n",
        "        outs = []\n",
        "        for _ in range(max_len):\n",
        "            logits, dec_hc, ctx, _ = model.decoder(y_prev, dec_hc, enc_out, enc_mask, ctx)\n",
        "            y_prev = torch.argmax(logits, dim=-1)\n",
        "            outs.append(y_prev)\n",
        "            if (y_prev == EOS_ID).all(): break\n",
        "        return torch.stack(outs, dim=1).squeeze(0)  # [L]\n",
        "\n",
        "# --------- Beam search (fixed tuples) ---------\n",
        "def beam_one(model, src_ids, beam_size=5, max_len=200, length_penalty=0.8):\n",
        "    \"\"\"\n",
        "    Returns best seq ids (Tensor [L]).\n",
        "    Tuples in beams/finished are (neg_logprob, seq_list, (h,c), ctx)\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        # Encode once\n",
        "        enc_out, (hn, cn) = model.encoder(src_ids)\n",
        "        enc_mask = (src_ids == PAD_ID)\n",
        "        dec_hc0 = model.bridge(hn, cn)\n",
        "        ctx0 = torch.zeros(src_ids.size(0), enc_out.size(-1), device=src_ids.device)\n",
        "\n",
        "        # Init beams\n",
        "        beams = [(0.0, [BOS_ID], dec_hc0, ctx0)]  # negative log-prob score\n",
        "        finished = []\n",
        "\n",
        "        for _ in range(max_len):\n",
        "            new_beams = []\n",
        "            for score, seq, dec_hc, ctx in beams:\n",
        "                last = seq[-1]\n",
        "                if last == EOS_ID:\n",
        "                    finished.append((score, seq, dec_hc, ctx))\n",
        "                    continue\n",
        "                y_prev = torch.tensor([last], dtype=torch.long, device=src_ids.device)\n",
        "                logits, dec_hc_new, ctx_new, _ = model.decoder(y_prev, dec_hc, enc_out, enc_mask, ctx)\n",
        "                logp = F.log_softmax(logits, dim=-1).squeeze(0)  # [V]\n",
        "                topk = torch.topk(logp, beam_size)\n",
        "                for k in range(beam_size):\n",
        "                    tok_id = int(topk.indices[k].item())\n",
        "                    sc = float(-topk.values[k].item())  # negative log prob\n",
        "                    new_seq = seq + [tok_id]\n",
        "                    new_beams.append((score + sc, new_seq, dec_hc_new, ctx_new))\n",
        "\n",
        "            # prune\n",
        "            new_beams.sort(key=lambda x: x[0])\n",
        "            beams = new_beams[:beam_size]\n",
        "\n",
        "            # early stop if all finished\n",
        "            if len(finished) >= beam_size and all(b[-1] == EOS_ID for _, b, _, _ in finished[-beam_size:]):\n",
        "                break\n",
        "\n",
        "        # Candidate pool: prefer finished, else current beams\n",
        "        cand = finished if finished else beams\n",
        "\n",
        "        # Apply length penalty on a view with only (score, seq)\n",
        "        normed = []\n",
        "        for sc, seq, _, _ in cand:\n",
        "            L = max(1, len(seq))\n",
        "            normed.append((sc / (L ** length_penalty), seq))\n",
        "        best_seq = min(normed, key=lambda x: x[0])[1]\n",
        "\n",
        "        # strip leading BOS, truncate at EOS\n",
        "        if best_seq and best_seq[0] == BOS_ID:\n",
        "            best_seq = best_seq[1:]\n",
        "        if EOS_ID in best_seq:\n",
        "            best_seq = best_seq[:best_seq.index(EOS_ID)]\n",
        "        return torch.tensor(best_seq, dtype=torch.long)\n",
        "\n",
        "# --------- Public helpers ---------\n",
        "def translate_one(urdu_text: str, decoder=\"beam\", beam_size=5, max_len=200):\n",
        "    src_ids = encode_src_text(urdu_text, exp_used[\"tokenization\"])\n",
        "    if decoder == \"greedy\":\n",
        "        out = greedy_one(best_model, src_ids, max_len=max_len)\n",
        "    else:\n",
        "        out = beam_one(best_model, src_ids, beam_size=beam_size, max_len=max_len)\n",
        "    return decode_ids(out)\n",
        "\n",
        "def translate_batch(texts, decoder=\"beam\", beam_size=5, max_len=200):\n",
        "    outs = []\n",
        "    for s in texts:\n",
        "        outs.append(translate_one(str(s), decoder=decoder, beam_size=beam_size, max_len=max_len))\n",
        "    return outs\n",
        "\n",
        "# --------- Run on test split (or your own list) and export ---------\n",
        "test_df = pd.read_parquet(DATA_DIR / \"pairs_test.parquet\")[[\"src_ur\",\"tgt_rom\"]].copy()\n",
        "\n",
        "# Example for custom list instead:\n",
        "# custom = [\"Ù…Ø²Û’ Ø¬ÛØ§Ù† Ú©Û’ Ø§Ù¾Ù†ÛŒ Ù†Ø¸Ø± Ù…ÛŒÚº Ø®Ø§Ú© Ù†ÛÛŒÚº\", \"Ø§Ø¨ Ú©Û’ ÛÙ… Ø¨Ú†Ú¾Ú‘Û’ ØªÙˆ Ø´Ø§ÛŒØ¯ Ú©Ø¨Ú¾ÛŒ Ø®ÙˆØ§Ø¨ÙˆÚº Ù…ÛŒÚº Ù…Ù„ÛŒÚº\"]\n",
        "# test_df = pd.DataFrame({\"src_ur\": custom, \"tgt_rom\": [\"\"]*len(custom)})\n",
        "\n",
        "preds = translate_batch(test_df[\"src_ur\"].tolist(), decoder=\"beam\", beam_size=5)\n",
        "out = test_df.assign(pred=preds)\n",
        "\n",
        "csv_path = RUNS_DIR / \"preds_test.csv\"\n",
        "out.to_csv(csv_path, index=False)\n",
        "print(\"âœ… Saved:\", csv_path)\n",
        "out.head(10)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 591
        },
        "id": "fEtkl5s-uHP2",
        "outputId": "693313f1-3acf-4482-ffb0-c26c048142b8"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using ckpt: bilstm4lstm_char_E256_H256_enc2_dec4_drop0.3_best.pt | tokenization=char\n",
            "âœ… Saved: /content/nmt_urdu_roman/runs/preds_test.csv\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                    src_ur  \\\n",
              "0    Ø¹Ø§Ø´Ù‚ÛŒ Ù…ÛŒÚº Ù…ÛŒØ±Ø” Ø¬ÛŒØ³Û’ Ø®ÙˆØ§Ø¨ Ù…Øª Ø¯ÛŒÚ©ÛØ§ Ú©Ø±Ùˆ   \n",
              "1       Ø¨Ø§ÙˆÙ„Û’ ÛÙˆ Ø¬Ø§Ùˆ Ú¯Û’ Ù…ÛØªØ§Ø¨ Ù…Øª Ø¯ÛŒÚ©ÛØ§ Ú©Ø±Ùˆ   \n",
              "2        Ø¬Ø³ØªÛ Ø¬Ø³ØªÛ Ù¾Ú‘Û Ù„ÛŒØ§ Ú©Ø±Ù†Ø§ Ù…Ø¶Ø§Ù…ÛŒÙ† ÙˆÙØ§   \n",
              "3       Ù¾Ø± Ú©ØªØ§Ø¨ Ø¹Ø´Ù‚ Ú©Ø§ ÛØ± Ø¨Ø§Ø¨ Ù…Øª Ø¯ÛŒÚ©ÛØ§ Ú©Ø±Ùˆ   \n",
              "4    Ø§Ø³ ØªÙ…Ø§Ø´Û’ Ù…ÛŒÚº Ø§Ù„Ù¹ Ø¬Ø§ØªÛŒ ÛÛŒÚº Ø§Ú©Ø«Ø± Ú©Ø´ØªÛŒØ§Úº   \n",
              "5       ÚˆÙˆØ¨Ù†Û’ ÙˆØ§Ù„ÙˆÚº Ú©Ùˆ Ø²ÛŒØ± Ø¢Ø¨ Ù…Øª Ø¯ÛŒÚ©ÛØ§ Ú©Ø±Ùˆ   \n",
              "6  Ù…Û’ Ú©Ø¯Û’ Ù…ÛŒÚº Ú©ÛŒØ§ ØªÚ©Ù„Ù Ù…Û’ Ú©Ø´ÛŒ Ù…ÛŒÚº Ú©ÛŒØ§ Ø­Ø¬Ø§Ø¨   \n",
              "7       Ø¨Ø²Ù… Ø³Ø§Ù‚ÛŒ Ù…ÛŒÚº Ø§Ø¯Ø¨ Ø¢Ø¯Ø§Ø¨ Ù…Øª Ø¯ÛŒÚ©ÛØ§ Ú©Ø±Ùˆ   \n",
              "8  ÛÙ… Ø³Û’ Ø¯Ø±ÙˆÛŒØ´ÙˆÚº Ú©Û’ Ú¯ÛØ± Ø¢Ùˆ ØªÙˆ ÛŒØ§Ø±ÙˆÚº Ú©ÛŒ Ø·Ø±Ø­   \n",
              "9      ÛØ± Ø¬Ú¯Û Ø®Ø³ Ø®Ø§Ù†Û Ùˆ Ø¨Ø±ÙØ§Ø¨ Ù…Øª Ø¯ÛŒÚ©ÛØ§ Ú©Ø±Ùˆ   \n",
              "\n",
              "                                             tgt_rom  \\\n",
              "0        ÄshiqÄ« meÃ± 'mÄ«r' jaise á¸³hvÄb mat dekhÄ karo   \n",
              "1               bÄvle ho jÄoge mahtÄb mat dekhÄ karo   \n",
              "2         jasta jasta paá¸Œh liyÄ karnÄ mazÄmÄ«n-e-vafÄ   \n",
              "3        par kitÄb-e-ishq kÄ har baab mat dekhÄ karo   \n",
              "4     is tamÄshe meÃ± ulaT jaatÄ« haiÃ± aksar kashtiyÄÃ±   \n",
              "5             DÅ«bne vÄloÃ± ko zer-e-Äb mat dekhÄ karo   \n",
              "6  mai-kade meÃ± kyÄ takalluf mai-kashÄ« meÃ± kyÄ hijÄb   \n",
              "7           bazm-e-sÄqÄ« meÃ± adab ÄdÄb mat dekhÄ karo   \n",
              "8     ham se durveshoÃ± ke ghar aao to yÄroÃ± kÄ« tarah   \n",
              "9       har jagah á¸³has-á¸³hÄna o barfÄb mat dekhÄ karo   \n",
              "\n",
              "                                                pred  \n",
              "0        ÄshiqÄ« meÃ± 'mÄ«r' jaise á¸³hvÄb mat dekhÄ karo  \n",
              "1              bÄvale ho jÄoge mahtÄb mat dekhÄ karo  \n",
              "2         jasta jasta paá¸Œh liyÄ karnÄ mazÄmÄ«n-e-vafÄ  \n",
              "3        par kitÄb-e-ishq kÄ har baab mat dekhÄ karo  \n",
              "4     is tamÄshe meÃ± ulaT jaatÄ« haiÃ± aksar kushtiyÄÃ±  \n",
              "5             DÅ«bne vÄloÃ± ko zer-e-Äb mat dekhÄ karo  \n",
              "6  mai-kade meÃ± kyÄ takalluf-e-mai-kashÄ« meÃ± kyÄ ...  \n",
              "7           bazm-e-sÄqÄ« meÃ± adab ÄdÄb mat dekhÄ karo  \n",
              "8     ham se darveshoÃ± ke ghar aao to yÄroÃ± kÄ« tarah  \n",
              "9       har jagah á¸³has-á¸³hÄna-o-barfÄb mat dekhÄ karo  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e903265f-8d83-4795-bfb3-66eeb4d3e56a\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>src_ur</th>\n",
              "      <th>tgt_rom</th>\n",
              "      <th>pred</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Ø¹Ø§Ø´Ù‚ÛŒ Ù…ÛŒÚº Ù…ÛŒØ±Ø” Ø¬ÛŒØ³Û’ Ø®ÙˆØ§Ø¨ Ù…Øª Ø¯ÛŒÚ©ÛØ§ Ú©Ø±Ùˆ</td>\n",
              "      <td>ÄshiqÄ« meÃ± 'mÄ«r' jaise á¸³hvÄb mat dekhÄ karo</td>\n",
              "      <td>ÄshiqÄ« meÃ± 'mÄ«r' jaise á¸³hvÄb mat dekhÄ karo</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Ø¨Ø§ÙˆÙ„Û’ ÛÙˆ Ø¬Ø§Ùˆ Ú¯Û’ Ù…ÛØªØ§Ø¨ Ù…Øª Ø¯ÛŒÚ©ÛØ§ Ú©Ø±Ùˆ</td>\n",
              "      <td>bÄvle ho jÄoge mahtÄb mat dekhÄ karo</td>\n",
              "      <td>bÄvale ho jÄoge mahtÄb mat dekhÄ karo</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Ø¬Ø³ØªÛ Ø¬Ø³ØªÛ Ù¾Ú‘Û Ù„ÛŒØ§ Ú©Ø±Ù†Ø§ Ù…Ø¶Ø§Ù…ÛŒÙ† ÙˆÙØ§</td>\n",
              "      <td>jasta jasta paá¸Œh liyÄ karnÄ mazÄmÄ«n-e-vafÄ</td>\n",
              "      <td>jasta jasta paá¸Œh liyÄ karnÄ mazÄmÄ«n-e-vafÄ</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Ù¾Ø± Ú©ØªØ§Ø¨ Ø¹Ø´Ù‚ Ú©Ø§ ÛØ± Ø¨Ø§Ø¨ Ù…Øª Ø¯ÛŒÚ©ÛØ§ Ú©Ø±Ùˆ</td>\n",
              "      <td>par kitÄb-e-ishq kÄ har baab mat dekhÄ karo</td>\n",
              "      <td>par kitÄb-e-ishq kÄ har baab mat dekhÄ karo</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Ø§Ø³ ØªÙ…Ø§Ø´Û’ Ù…ÛŒÚº Ø§Ù„Ù¹ Ø¬Ø§ØªÛŒ ÛÛŒÚº Ø§Ú©Ø«Ø± Ú©Ø´ØªÛŒØ§Úº</td>\n",
              "      <td>is tamÄshe meÃ± ulaT jaatÄ« haiÃ± aksar kashtiyÄÃ±</td>\n",
              "      <td>is tamÄshe meÃ± ulaT jaatÄ« haiÃ± aksar kushtiyÄÃ±</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>ÚˆÙˆØ¨Ù†Û’ ÙˆØ§Ù„ÙˆÚº Ú©Ùˆ Ø²ÛŒØ± Ø¢Ø¨ Ù…Øª Ø¯ÛŒÚ©ÛØ§ Ú©Ø±Ùˆ</td>\n",
              "      <td>DÅ«bne vÄloÃ± ko zer-e-Äb mat dekhÄ karo</td>\n",
              "      <td>DÅ«bne vÄloÃ± ko zer-e-Äb mat dekhÄ karo</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Ù…Û’ Ú©Ø¯Û’ Ù…ÛŒÚº Ú©ÛŒØ§ ØªÚ©Ù„Ù Ù…Û’ Ú©Ø´ÛŒ Ù…ÛŒÚº Ú©ÛŒØ§ Ø­Ø¬Ø§Ø¨</td>\n",
              "      <td>mai-kade meÃ± kyÄ takalluf mai-kashÄ« meÃ± kyÄ hijÄb</td>\n",
              "      <td>mai-kade meÃ± kyÄ takalluf-e-mai-kashÄ« meÃ± kyÄ ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Ø¨Ø²Ù… Ø³Ø§Ù‚ÛŒ Ù…ÛŒÚº Ø§Ø¯Ø¨ Ø¢Ø¯Ø§Ø¨ Ù…Øª Ø¯ÛŒÚ©ÛØ§ Ú©Ø±Ùˆ</td>\n",
              "      <td>bazm-e-sÄqÄ« meÃ± adab ÄdÄb mat dekhÄ karo</td>\n",
              "      <td>bazm-e-sÄqÄ« meÃ± adab ÄdÄb mat dekhÄ karo</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>ÛÙ… Ø³Û’ Ø¯Ø±ÙˆÛŒØ´ÙˆÚº Ú©Û’ Ú¯ÛØ± Ø¢Ùˆ ØªÙˆ ÛŒØ§Ø±ÙˆÚº Ú©ÛŒ Ø·Ø±Ø­</td>\n",
              "      <td>ham se durveshoÃ± ke ghar aao to yÄroÃ± kÄ« tarah</td>\n",
              "      <td>ham se darveshoÃ± ke ghar aao to yÄroÃ± kÄ« tarah</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>ÛØ± Ø¬Ú¯Û Ø®Ø³ Ø®Ø§Ù†Û Ùˆ Ø¨Ø±ÙØ§Ø¨ Ù…Øª Ø¯ÛŒÚ©ÛØ§ Ú©Ø±Ùˆ</td>\n",
              "      <td>har jagah á¸³has-á¸³hÄna o barfÄb mat dekhÄ karo</td>\n",
              "      <td>har jagah á¸³has-á¸³hÄna-o-barfÄb mat dekhÄ karo</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e903265f-8d83-4795-bfb3-66eeb4d3e56a')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-e903265f-8d83-4795-bfb3-66eeb4d3e56a button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-e903265f-8d83-4795-bfb3-66eeb4d3e56a');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-e7c4cf32-13a5-4d21-b04c-ef14a2cad53a\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-e7c4cf32-13a5-4d21-b04c-ef14a2cad53a')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-e7c4cf32-13a5-4d21-b04c-ef14a2cad53a button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "out",
              "summary": "{\n  \"name\": \"out\",\n  \"rows\": 5255,\n  \"fields\": [\n    {\n      \"column\": \"src_ur\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5255,\n        \"samples\": [\n          \"\\u0632\\u06c1\\u0631 \\u0627\\u06af\\u0644\\u062a\\u06d2 \\u06c1\\u06cc\\u06ba \\u062c\\u0628 \\u0645\\u0644 \\u06a9\\u0631 \\u062f\\u0646\\u06cc\\u0627 \\u0648\\u0627\\u0644\\u06d2\",\n          \"\\u0632\\u0648\\u0631 \\u0646\\u0633\\u0628\\u062a \\u0645\\u06d2 \\u0633\\u06d2 \\u0631\\u06a9\\u06c1\\u062a\\u0627 \\u06c1\\u06d2 \\u0627\\u0636\\u0627\\u0631\\u0627 \\u06a9\\u0627 \\u0646\\u0645\\u06a9\",\n          \"\\u06a9\\u0628\\u06c1\\u06cc \\u067e\\u0631\\u06cc \\u0645\\u0631\\u06cc \\u062e\\u0644\\u0648\\u062a \\u0645\\u06cc\\u06ba \\u0622 \\u0646\\u06a9\\u0644\\u062a\\u06cc \\u06c1\\u06d2\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"tgt_rom\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5255,\n        \"samples\": [\n          \"zahr ugalte hai\\u00f1 jab mil kar duniy\\u0101 vaale\",\n          \"zor nisbat mai se rakht\\u0101 hai az\\u0101r\\u0101 k\\u0101 namak\",\n          \"kabh\\u012b par\\u012b mir\\u012b \\u1e33halvat me\\u00f1 aa nikalt\\u012b hai\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"pred\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5255,\n        \"samples\": [\n          \"zahr ugalte hai\\u00f1 jab mil kar duniy\\u0101 vaale\",\n          \"zor-e-nisbat mai se rakht\\u0101 hai az\\u0101r\\u0101 k\\u0101 namak\",\n          \"kabh\\u012b par\\u012b mir\\u012b \\u1e33halvat me\\u00f1 aa nikalt\\u012b hai\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Cell #9 â€” Quick error analysis + light post-processing\n",
        "#   - Shows top confusions\n",
        "#   - Normalizes hyphens/ezÄfe/wa and a couple frequent typos\n",
        "#   - Re-scores BLEU/CER after cleanup and saves new CSV\n",
        "# ============================================================\n",
        "import re, pandas as pd, numpy as np, sacrebleu\n",
        "from jiwer import cer as jiwer_cer\n",
        "from pathlib import Path\n",
        "\n",
        "PROJECT_DIR = Path(\"/content/nmt_urdu_roman\")\n",
        "RUNS_DIR    = PROJECT_DIR / \"runs\"\n",
        "DATA_DIR    = PROJECT_DIR / \"data\"\n",
        "\n",
        "df = pd.read_csv(RUNS_DIR / \"preds_test.csv\", sep=\",\")\n",
        "print(\"Rows:\", len(df))\n",
        "\n",
        "# ---- quick error peek\n",
        "def token_diff_rows(df, n=10):\n",
        "    rows = []\n",
        "    for i, r in df.iterrows():\n",
        "        gt = str(r[\"tgt_rom\"]).split()\n",
        "        pr = str(r[\"pred\"]).split()\n",
        "        if gt != pr:\n",
        "            rows.append((i, r[\"src_ur\"], r[\"tgt_rom\"], r[\"pred\"]))\n",
        "        if len(rows) >= n: break\n",
        "    return rows\n",
        "\n",
        "print(\"\\nExamples with diffs (first 10):\")\n",
        "for i, ur, gt, pr in token_diff_rows(df, n=10):\n",
        "    print(f\"- {i:04d} UR: {ur}\\n   GT: {gt}\\n   PR: {pr}\\n\")\n",
        "\n",
        "# ---- light post-processing\n",
        "EZAFE = r\"(?:\\s*-\\s*e\\s*-\\s*|\\s+e\\s+|\\s*e\\s*-\\s*|\\s*-\\s*e\\s*)\"\n",
        "WA    = r\"(?:\\s*-\\s*o\\s*-\\s*|\\s+o\\s+|\\s*o\\s*-\\s*|\\s*-\\s*o\\s*)\"\n",
        "\n",
        "def tidy_roman(s: str) -> str:\n",
        "    if not isinstance(s, str): return \"\"\n",
        "    t = s\n",
        "\n",
        "    # unify ezÄfe and wa to \"-e-\" / \"-o-\"\n",
        "    t = re.sub(EZAFE, \"-e-\", t)\n",
        "    t = re.sub(WA, \"-o-\", t)\n",
        "\n",
        "    # collapse multiple hyphens/spaces around hyphens\n",
        "    t = re.sub(r\"\\s*-\\s*\", \"-\", t)\n",
        "    t = re.sub(r\"\\s{2,}\", \" \", t).strip()\n",
        "\n",
        "    # common char fixes\n",
        "    # kashti vs kushti (if your ground truth prefers 'kashtiyÄÃ±')\n",
        "    t = re.sub(r\"\\bkushti(yÄÃ±|yÄn|yÄ|yÄn?)\\b\", r\"kashti\\1\", t, flags=re.I)\n",
        "\n",
        "    # normalize dotted 'á¸³haak' variants -> 'á¸³haak' / 'khÄk' style harmonization is repo-specific; skip heavy changes\n",
        "    # small punctuation spacing\n",
        "    t = re.sub(r\"\\s+([,Ø›Û”?!])\", r\"\\1\", t)\n",
        "\n",
        "    return t\n",
        "\n",
        "df[\"pred_clean\"] = df[\"pred\"].map(tidy_roman)\n",
        "\n",
        "# ---- score before/after\n",
        "preds_raw   = df[\"pred\"].tolist()\n",
        "preds_clean = df[\"pred_clean\"].tolist()\n",
        "refs        = df[\"tgt_rom\"].tolist()\n",
        "\n",
        "bleu_raw   = sacrebleu.corpus_bleu(preds_raw,   [refs]).score\n",
        "bleu_clean = sacrebleu.corpus_bleu(preds_clean, [refs]).score\n",
        "cer_raw    = float(np.mean([jiwer_cer(r, p) for p, r in zip(preds_raw, refs)]))\n",
        "cer_clean  = float(np.mean([jiwer_cer(r, p) for p, r in zip(preds_clean, refs)]))\n",
        "\n",
        "print(f\"\\nBLEU raw   : {bleu_raw:.2f} | CER raw   : {cer_raw:.3f}\")\n",
        "print(f\"BLEU clean : {bleu_clean:.2f} | CER clean : {cer_clean:.3f}\")\n",
        "\n",
        "# ---- save cleaned file\n",
        "out_path = RUNS_DIR / \"preds_test_clean.csv\"\n",
        "df[[\"src_ur\",\"tgt_rom\",\"pred_clean\"]].to_csv(out_path, index=False)\n",
        "print(\"âœ… Saved cleaned preds:\", out_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a1itu53BuY98",
        "outputId": "dc92a261-b16e-44bd-e9df-ff3f47c58056"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rows: 5255\n",
            "\n",
            "Examples with diffs (first 10):\n",
            "- 0001 UR: Ø¨Ø§ÙˆÙ„Û’ ÛÙˆ Ø¬Ø§Ùˆ Ú¯Û’ Ù…ÛØªØ§Ø¨ Ù…Øª Ø¯ÛŒÚ©ÛØ§ Ú©Ø±Ùˆ\n",
            "   GT: bÄvle ho jÄoge mahtÄb mat dekhÄ karo\n",
            "   PR: bÄvale ho jÄoge mahtÄb mat dekhÄ karo\n",
            "\n",
            "- 0004 UR: Ø§Ø³ ØªÙ…Ø§Ø´Û’ Ù…ÛŒÚº Ø§Ù„Ù¹ Ø¬Ø§ØªÛŒ ÛÛŒÚº Ø§Ú©Ø«Ø± Ú©Ø´ØªÛŒØ§Úº\n",
            "   GT: is tamÄshe meÃ± ulaT jaatÄ« haiÃ± aksar kashtiyÄÃ±\n",
            "   PR: is tamÄshe meÃ± ulaT jaatÄ« haiÃ± aksar kushtiyÄÃ±\n",
            "\n",
            "- 0006 UR: Ù…Û’ Ú©Ø¯Û’ Ù…ÛŒÚº Ú©ÛŒØ§ ØªÚ©Ù„Ù Ù…Û’ Ú©Ø´ÛŒ Ù…ÛŒÚº Ú©ÛŒØ§ Ø­Ø¬Ø§Ø¨\n",
            "   GT: mai-kade meÃ± kyÄ takalluf mai-kashÄ« meÃ± kyÄ hijÄb\n",
            "   PR: mai-kade meÃ± kyÄ takalluf-e-mai-kashÄ« meÃ± kyÄ hijÄb\n",
            "\n",
            "- 0008 UR: ÛÙ… Ø³Û’ Ø¯Ø±ÙˆÛŒØ´ÙˆÚº Ú©Û’ Ú¯ÛØ± Ø¢Ùˆ ØªÙˆ ÛŒØ§Ø±ÙˆÚº Ú©ÛŒ Ø·Ø±Ø­\n",
            "   GT: ham se durveshoÃ± ke ghar aao to yÄroÃ± kÄ« tarah\n",
            "   PR: ham se darveshoÃ± ke ghar aao to yÄroÃ± kÄ« tarah\n",
            "\n",
            "- 0009 UR: ÛØ± Ø¬Ú¯Û Ø®Ø³ Ø®Ø§Ù†Û Ùˆ Ø¨Ø±ÙØ§Ø¨ Ù…Øª Ø¯ÛŒÚ©ÛØ§ Ú©Ø±Ùˆ\n",
            "   GT: har jagah á¸³has-á¸³hÄna o barfÄb mat dekhÄ karo\n",
            "   PR: har jagah á¸³has-á¸³hÄna-o-barfÄb mat dekhÄ karo\n",
            "\n",
            "- 0010 UR: Ù…Ø§Ù†Ú¯Û’ ØªØ§Ù†Ú¯Û’ Ú©ÛŒ Ù‚Ø¨Ø§ÛŒÛŒÚº Ø¯ÛŒØ± ØªÚ© Ø±ÛØªÛŒ Ù†ÛÛŒÚº\n",
            "   GT: mÄÃ±ge-tÄÃ±ge kÄ« qabÄ.eÃ± der tak rahtÄ« nahÄ«Ã±\n",
            "   PR: mÄÃ±ge tÄÃ±ge kÄ« qabÄ.eÃ± der tah rahtÄ« nahÄ«Ã±\n",
            "\n",
            "- 0011 UR: ÛŒØ§Ø± Ù„ÙˆÚ¯ÙˆÚº Ú©Û’ Ù„Ù‚Ø¨ Ø§Ù„Ù‚Ø§Ø¨ Ù…Øª Ø¯ÛŒÚ©ÛØ§ Ú©Ø±Ùˆ\n",
            "   GT: yaar logoÃ± ke laqab-alqÄb mat dekhÄ karo\n",
            "   PR: yaar logoÃ± ke laqb-e-ulqÄb mat dekhÄ karo\n",
            "\n",
            "- 0012 UR: ØªØ´Ù†Ú¯ÛŒ Ù…ÛŒÚº Ù„Ø¨ Ø¨ÛÚ¯Ùˆ Ù„ÛŒÙ†Ø§ Ø¨ÛÛŒ Ú©Ø§ÙÛŒ ÛÛ’ ÙØ±Ø§Ø²Ø”\n",
            "   GT: tishnagÄ« meÃ± lab bhigo lenÄ bhÄ« kaafÄ« hai 'farÄz'\n",
            "   PR: tishnagÄ« meÃ± lab bhugÅ« lenÄ bhÄ« kaafÄ« hai 'farÄz'\n",
            "\n",
            "- 0016 UR: ÛŒÙˆÚº ÛÛŒ Ù…ÙˆØ³Ù… Ú©ÛŒ Ø§Ø¯Ø§ Ø¯ÛŒÚ©Û Ú©Û’ ÛŒØ§Ø¯ Ø¢ÛŒØ§ ÛÛ’\n",
            "   GT: yÅ«Ã±hÄ« mausam kÄ« adÄ dekh ke yaad aayÄ hai\n",
            "   PR: yuuÃ± hÄ« mausam kÄ« adÄ dekh ke yaad aayÄ hai\n",
            "\n",
            "- 0020 UR: Ø¯Ù„ ÛŒÛ Ú©ÛØªØ§ ÛÛ’ Ú©Û Ø´Ø§ÛŒØ¯ ÛÛ’ ÙØ³Ø±Ø¯Û ØªÙˆ Ø¨ÛÛŒ\n",
            "   GT: dil ye kahtÄ hai ki shÄyad hai fasurda tÅ« bhÄ«\n",
            "   PR: dil ye kahtÄ hai ki shÄyad hai fasrda to bhÄ«\n",
            "\n",
            "\n",
            "BLEU raw   : 74.89 | CER raw   : 0.048\n",
            "BLEU clean : 74.64 | CER clean : 0.049\n",
            "âœ… Saved cleaned preds: /content/nmt_urdu_roman/runs/preds_test_clean.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# If not already mounted:\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# Copy folders into Drive\n",
        "!cp -r /content/nmt_urdu_roman /content/drive/MyDrive/\n",
        "!cp -r /content/urdu_ghazals_rekhta /content/drive/MyDrive/\n",
        "\n",
        "print(\"Saved to MyDrive/nmt_urdu_roman and MyDrive/urdu_ghazals_rekhta\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j7-8yu1qzL5n",
        "outputId": "dccfab4f-e6ff-4308-fb7f-b457b6bb308a"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Saved to MyDrive/nmt_urdu_roman and MyDrive/urdu_ghazals_rekhta\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xDsKSMUu3mYT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}