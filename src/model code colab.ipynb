{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 907
        },
        "id": "BOpmp1ADUkak",
        "outputId": "988fd2e3-7b54-4445-93b3-e165e55f5f1c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python: 3.12.11 (main, Jun  4 2025, 08:56:18) [GCC 11.4.0]\n",
            "PyTorch: 2.8.0+cu126\n",
            "CUDA available: True\n",
            "CUDA device: Tesla T4\n",
            "Mounted at /content/drive\n",
            "Project root: /content/nmt_urdu_roman\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m60.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[33m  DEPRECATION: Building 'pandarallel' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'pandarallel'. Discussion can be found at https://github.com/pypa/pip/issues/6334\u001b[0m\u001b[33m\n",
            "\u001b[0m  Building wheel for pandarallel (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Seed fixed to 42\n",
            "⚠️ Could not find your local poet folders in Drive.\n",
            "Attempting to clone GitHub dataset into project data ...\n",
            "Saved config: /content/nmt_urdu_roman/project_config.json\n",
            "Found 3 poet folders (showing up to 30):\n",
            " - .git\n",
            " - dataset\n",
            " - sample_dataset\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2607418669.py:112: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  \"created_at\": datetime.utcnow().isoformat() + \"Z\",\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'caas_jupyter_tools'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2607418669.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    156\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msamples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATASET_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpoets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n✅ Setup complete. If the scan table opened, review the paths & sizes.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2607418669.py\u001b[0m in \u001b[0;36msample_files\u001b[0;34m(base, poets_list, n_per_poet)\u001b[0m\n\u001b[1;32m    150\u001b[0m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m         \u001b[0;32mfrom\u001b[0m \u001b[0mcaas_jupyter_tools\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdisplay_dataframe_to_user\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m         \u001b[0mdisplay_dataframe_to_user\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Dataset quick scan\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'caas_jupyter_tools'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "# ============================================================\n",
        "# Colab Cell #1 — Setup, Mount, Paths, Installs, Quick Scan\n",
        "# ============================================================\n",
        "import sys, os, re, random, json, math, unicodedata, shutil, glob\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "\n",
        "# --------------------------\n",
        "# 0) Basic environment info\n",
        "# --------------------------\n",
        "print(\"Python:\", sys.version)\n",
        "try:\n",
        "    import torch\n",
        "    print(\"PyTorch:\", torch.__version__)\n",
        "    print(\"CUDA available:\", torch.cuda.is_available())\n",
        "    if torch.cuda.is_available():\n",
        "        print(\"CUDA device:\", torch.cuda.get_device_name(0))\n",
        "except Exception as e:\n",
        "    print(\"Torch not yet available:\", e)\n",
        "\n",
        "# --------------------------\n",
        "# 1) Mount Google Drive\n",
        "# --------------------------\n",
        "IN_COLAB = \"google.colab\" in sys.modules\n",
        "if IN_COLAB:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "    ROOT = Path(\"/content\")\n",
        "    GDRIVE = Path(\"/content/drive/MyDrive\")\n",
        "else:\n",
        "    # Fallback for local dev (optional)\n",
        "    ROOT = Path.cwd()\n",
        "    GDRIVE = ROOT\n",
        "\n",
        "PROJECT_DIR = ROOT / \"nmt_urdu_roman\"\n",
        "for p in [\"data\",\"artifacts\",\"models\",\"logs\",\"src\",\"runs\"]:\n",
        "    (PROJECT_DIR / p).mkdir(parents=True, exist_ok=True)\n",
        "print(\"Project root:\", PROJECT_DIR)\n",
        "\n",
        "# --------------------------\n",
        "# 2) Pip installs\n",
        "# --------------------------\n",
        "# Keep installs minimal in first cell; add others later when needed\n",
        "!pip -q install --upgrade pip\n",
        "!pip -q install sacrebleu==2.4.2 jiwer==3.0.4 python-Levenshtein==0.25.1 sentencepiece==0.2.0 pyarrow==17.0.0 pandas==2.2.2 pandarallel==1.6.5\n",
        "\n",
        "import pandas as pd\n",
        "import sacrebleu, sentencepiece as spm\n",
        "from jiwer import cer\n",
        "import Levenshtein\n",
        "\n",
        "# --------------------------\n",
        "# 3) Reproducibility\n",
        "# --------------------------\n",
        "import numpy as np\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "try:\n",
        "    import torch\n",
        "    torch.manual_seed(SEED)\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "except:\n",
        "    pass\n",
        "print(\"Seed fixed to\", SEED)\n",
        "\n",
        "# --------------------------\n",
        "# 4) Dataset wiring\n",
        "# --------------------------\n",
        "# >>>> IMPORTANT: Set this to your Drive path where poet folders live <<<<\n",
        "# Example: /content/drive/MyDrive/marwah/dataset\n",
        "CANDIDATE_PATHS = [\n",
        "    GDRIVE / \"marwah\" / \"dataset\",\n",
        "    GDRIVE / \"dataset\" / \"urdu_ghazals_rekhta\",\n",
        "    PROJECT_DIR / \"data\" / \"urdu_ghazals_rekhta\"\n",
        "]\n",
        "\n",
        "DATASET_DIR = None\n",
        "for p in CANDIDATE_PATHS:\n",
        "    if p.exists() and any((p / d).exists() for d in [\n",
        "        \"ahmad-faraz\",\"akbar-allahabadi\",\"allama-iqbal\",\"ameer-khusrau\",\n",
        "        \"mirza-ghalib\",\"parveen-shakir\",\"faiz-ahmad-faiz\"\n",
        "    ]):\n",
        "        DATASET_DIR = p\n",
        "        break\n",
        "\n",
        "if DATASET_DIR is None:\n",
        "    print(\"⚠️ Could not find your local poet folders in Drive.\")\n",
        "    print(\"Attempting to clone GitHub dataset into project data ...\")\n",
        "    !rm -rf /content/urdu_ghazals_rekhta\n",
        "    !git clone -q https://github.com/amir9ume/urdu_ghazals_rekhta.git /content/urdu_ghazals_rekhta\n",
        "    src = Path(\"/content/urdu_ghazals_rekhta\")\n",
        "    if src.exists():\n",
        "        # try to locate poet folders\n",
        "        poet_dirs = [d for d in src.iterdir() if d.is_dir()]\n",
        "        if poet_dirs:\n",
        "            target = PROJECT_DIR / \"data\" / \"urdu_ghazals_rekhta\"\n",
        "            target.mkdir(parents=True, exist_ok=True)\n",
        "            # Copy structure lightly to our data dir\n",
        "            for d in poet_dirs:\n",
        "                shutil.copytree(d, target / d.name, dirs_exist_ok=True)\n",
        "            DATASET_DIR = target\n",
        "        else:\n",
        "            print(\"❌ Clone succeeded but poet directories not found. Please set DATASET_DIR manually.\")\n",
        "else:\n",
        "    print(\"✅ Found dataset at:\", DATASET_DIR)\n",
        "\n",
        "# Persist a config file\n",
        "cfg = {\n",
        "    \"created_at\": datetime.utcnow().isoformat() + \"Z\",\n",
        "    \"seed\": SEED,\n",
        "    \"dataset_dir\": str(DATASET_DIR) if DATASET_DIR else None,\n",
        "    \"device\": \"cuda\" if (\"torch\" in sys.modules and torch.cuda.is_available()) else \"cpu\",\n",
        "    \"project_dir\": str(PROJECT_DIR)\n",
        "}\n",
        "with open(PROJECT_DIR / \"project_config.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(cfg, f, indent=2, ensure_ascii=False)\n",
        "print(\"Saved config:\", PROJECT_DIR / \"project_config.json\")\n",
        "\n",
        "# --------------------------\n",
        "# 5) Quick scan of poets\n",
        "# --------------------------\n",
        "def list_poets(base: Path, limit=30):\n",
        "    if base is None or not base.exists():\n",
        "        print(\"Dataset path not set. Please update CANDIDATE_PATHS.\")\n",
        "        return []\n",
        "    poets = sorted([d.name for d in base.iterdir() if d.is_dir()])\n",
        "    print(f\"Found {len(poets)} poet folders (showing up to {limit}):\")\n",
        "    for name in poets[:limit]:\n",
        "        print(\" -\", name)\n",
        "    return poets\n",
        "\n",
        "poets = list_poets(DATASET_DIR)\n",
        "\n",
        "# Probe a few files inside first 2 poets to understand file patterns\n",
        "def sample_files(base: Path, poets_list, n_per_poet=3):\n",
        "    samples = []\n",
        "    for poet in poets_list[:2]:  # keep light\n",
        "        folder = base / poet\n",
        "        if not folder.exists():\n",
        "            continue\n",
        "        files = sorted([*folder.rglob(\"*\")])\n",
        "        text_like = [p for p in files if p.suffix.lower() in (\".txt\", \".csv\", \".json\", \".md\")]\n",
        "        chosen = text_like[:n_per_poet] if text_like else files[:n_per_poet]\n",
        "        for f in chosen:\n",
        "            size_kb = os.path.getsize(f) / 1024.0\n",
        "            samples.append({\"poet\": poet, \"path\": str(f), \"size_kb\": round(size_kb, 2)})\n",
        "    df = pd.DataFrame(samples)\n",
        "    if not df.empty:\n",
        "        from caas_jupyter_tools import display_dataframe_to_user\n",
        "        display_dataframe_to_user(\"Dataset quick scan\", df)\n",
        "    else:\n",
        "        print(\"No text-like files found in first two poets; structure may be nested differently.\")\n",
        "    return samples\n",
        "\n",
        "_ = sample_files(DATASET_DIR, poets)\n",
        "\n",
        "print(\"\\n✅ Setup complete. If the scan table opened, review the paths & sizes.\")\n",
        "print(\"If your dataset lives elsewhere in Drive, update CANDIDATE_PATHS above and re-run this cell.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install --force-reinstall \"sentencepiece==0.1.99\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xjZfKo2ej0JY",
        "outputId": "6e378b7c-acab-480f-ac87-32de883c3222"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[33m  DEPRECATION: Building 'sentencepiece' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'sentencepiece'. Discussion can be found at https://github.com/pypa/pip/issues/6334\u001b[0m\u001b[33m\n",
            "\u001b[0m  Building wheel for sentencepiece (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Colab Cell #1B — Point to /MyDrive/dataset + safe quick scan\n",
        "# ============================================================\n",
        "import json, os\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "\n",
        "# If you changed your folder name, update this path:\n",
        "DATASET_DIR = Path(\"/content/drive/MyDrive/dataset\")\n",
        "\n",
        "assert DATASET_DIR.exists(), f\"Dataset path not found: {DATASET_DIR}\"\n",
        "\n",
        "# Basic sanity: look for a few poet folders\n",
        "EXPECTED = {\n",
        "    \"ahmad-faraz\",\"akbar-allahabadi\",\"allama-iqbal\",\"altaf-hussain-hali\",\n",
        "    \"ameer-khusrau\",\"bahadur-shah-zafar\",\"dagh-dehlvi\",\"fahmida-riaz\",\n",
        "    \"faiz-ahmad-faiz\",\"firaq-gorakhpuri\",\"gulzar\",\"habib-jalib\",\n",
        "    \"jaan-nisar-akhtar\",\"jaun-eliya\",\"javed-akhtar\",\"meer-taqi-meer\",\n",
        "    \"mirza-ghalib\",\"parveen-shakir\"\n",
        "}\n",
        "present = {d.name for d in DATASET_DIR.iterdir() if d.is_dir()}\n",
        "print(f\"✅ DATASET_DIR set to: {DATASET_DIR}\")\n",
        "print(f\"Found {len(present)} top-level folders.\")\n",
        "\n",
        "missing = sorted(list(EXPECTED - present))\n",
        "if missing:\n",
        "    print(\"Note: some expected poet folders not found (ok if your dump differs). Example missing:\", missing[:8])\n",
        "\n",
        "# Show a tiny table-like listing (no special display tools needed)\n",
        "def sample_files(base: Path, n_per_poet=3, poets_limit=5):\n",
        "    rows = []\n",
        "    poets = sorted([d for d in base.iterdir() if d.is_dir()])[:poets_limit]\n",
        "    for poet_dir in poets:\n",
        "        files = sorted([*poet_dir.rglob(\"*\")])\n",
        "        text_like = [p for p in files if p.suffix.lower() in (\".txt\", \".csv\", \".json\", \".md\")]\n",
        "        chosen = text_like[:n_per_poet] if text_like else files[:n_per_poet]\n",
        "        for f in chosen:\n",
        "            try:\n",
        "                size_kb = os.path.getsize(f) / 1024.0\n",
        "            except Exception:\n",
        "                size_kb = -1\n",
        "            rows.append((poet_dir.name, str(f.relative_to(base)), round(size_kb, 2)))\n",
        "    print(\"\\nSample of discovered files:\")\n",
        "    print(\"poet\\t\\trelative_path\\t\\tsize_kb\")\n",
        "    for poet, rel, sz in rows[:30]:\n",
        "        print(f\"{poet}\\t{rel}\\t{sz}\")\n",
        "\n",
        "sample_files(DATASET_DIR)\n",
        "\n",
        "# Persist the config with the corrected path\n",
        "PROJECT_DIR = Path(\"/content/nmt_urdu_roman\")\n",
        "cfg_path = PROJECT_DIR / \"project_config.json\"\n",
        "try:\n",
        "    with open(cfg_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        cfg = json.load(f)\n",
        "except Exception:\n",
        "    cfg = {}\n",
        "\n",
        "cfg.update({\n",
        "    \"dataset_dir\": str(DATASET_DIR),\n",
        "    \"updated_at\": datetime.utcnow().isoformat() + \"Z\",\n",
        "})\n",
        "with open(cfg_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(cfg, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "print(\"\\n🔧 Config updated at:\", cfg_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u4BukGwZVCYZ",
        "outputId": "98deb798-e0ac-4acd-ed6d-0c6d48e8c229"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ DATASET_DIR set to: /content/drive/MyDrive/dataset\n",
            "Found 30 top-level folders.\n",
            "\n",
            "Sample of discovered files:\n",
            "poet\t\trelative_path\t\tsize_kb\n",
            "ahmad-faraz\tahmad-faraz/.DS_Store\t6.0\n",
            "ahmad-faraz\tahmad-faraz/en\t4.0\n",
            "ahmad-faraz\tahmad-faraz/en/aankh-se-duur-na-ho-dil-se-utar-jaaegaa-ahmad-faraz-ghazals\t0.46\n",
            "akbar-allahabadi\takbar-allahabadi/en\t4.0\n",
            "akbar-allahabadi\takbar-allahabadi/en/aah-jo-dil-se-nikaalii-jaaegii-akbar-allahabadi-ghazals\t0.38\n",
            "akbar-allahabadi\takbar-allahabadi/en/aaj-aaraaish-e-gesuu-e-dotaa-hotii-hai-akbar-allahabadi-ghazals\t1.22\n",
            "allama-iqbal\tallama-iqbal/.DS_Store\t6.0\n",
            "allama-iqbal\tallama-iqbal/en\t4.0\n",
            "allama-iqbal\tallama-iqbal/en/aalam-e-aab-o-khaak-o-baad-sirr-e-ayaan-hai-tuu-ki-main-allama-iqbal-ghazals\t0.46\n",
            "altaf-hussain-hali\taltaf-hussain-hali/en\t4.0\n",
            "altaf-hussain-hali\taltaf-hussain-hali/en/aage-badhe-na-qissa-e-ishq-e-butaan-se-ham-altaf-hussain-hali-ghazals-3\t0.48\n",
            "altaf-hussain-hali\taltaf-hussain-hali/en/ab-vo-aglaa-saa-iltifaat-nahiin-altaf-hussain-hali-ghazals\t0.49\n",
            "ameer-khusrau\tameer-khusrau/en\t4.0\n",
            "ameer-khusrau\tameer-khusrau/en/ze-haal-e-miskiin-makun-tagaaful-duraae-nainaan-banaae-batiyaan-ameer-khusrau-ghazals\t0.7\n",
            "ameer-khusrau\tameer-khusrau/hi\t4.0\n",
            "\n",
            "🔧 Config updated at: /content/nmt_urdu_roman/project_config.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2404578599.py:61: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  \"updated_at\": datetime.utcnow().isoformat() + \"Z\",\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Colab Cell #2 — Parse Rekhta dump → Urdu/Roman pairs (raw)\n",
        "# ============================================================\n",
        "from pathlib import Path\n",
        "import os, io, re, json, unicodedata, itertools\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Load config written earlier\n",
        "import json\n",
        "cfg_path = Path(\"/content/nmt_urdu_roman/project_config.json\")\n",
        "with open(cfg_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    CFG = json.load(f)\n",
        "PROJECT_DIR = Path(CFG[\"project_dir\"])\n",
        "DATASET_DIR = Path(CFG[\"dataset_dir\"])\n",
        "OUT_DIR = PROJECT_DIR / \"data\"\n",
        "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(\"Using DATASET_DIR =\", DATASET_DIR)\n",
        "\n",
        "# --------------------------\n",
        "# 1) Normalization helpers\n",
        "# --------------------------\n",
        "ZW_CHARS = \"\".join([\n",
        "    \"\\u200b\", \"\\u200c\", \"\\u200d\", \"\\ufeff\"  # ZW space, joiners, BOM\n",
        "])\n",
        "\n",
        "URDU_MAP = {\n",
        "    # Common confusables → canonical\n",
        "    \"ي\": \"ی\",   # Arabic yeh → Farsi yeh\n",
        "    \"ك\": \"ک\",   # Arabic kaf → Farsi kaf\n",
        "    \"ہ\": \"ہ\",   # already Urdu heh-doachashmee\n",
        "    \"ۀ\": \"ہ\",   # heh with hamza → heh\n",
        "    \"ھ\": \"ہ\",   # heh goal → heh (approx)\n",
        "    \"ۃ\": \"ہ\",\n",
        "    \"أ\": \"ا\", \"إ\": \"ا\", \"آ\": \"آ\", \"ٱ\": \"ا\",\n",
        "    \"ؤ\": \"و\", \"ئ\": \"ی\",\n",
        "    \"ٔ\": \"\", \"ٰ\": \"\", \"ٌ\": \"\", \"ً\": \"\", \"ٍ\": \"\", \"ْ\": \"\", \"ّ\": \"\",  # remove tashkeel\n",
        "    \"ـ\": \"\",  # tatweel\n",
        "}\n",
        "\n",
        "PUNCT_KEEP = set(list(\"،؛؟!.,:;!?…—–-()[]{}\\\"'`’“”«»\"))\n",
        "\n",
        "def normalize_urdu(s: str) -> str:\n",
        "    if not isinstance(s, str): return \"\"\n",
        "    s = unicodedata.normalize(\"NFC\", s)\n",
        "    # Remove zero-width & BOM\n",
        "    s = re.sub(f\"[{re.escape(ZW_CHARS)}]\", \"\", s)\n",
        "    # Map confusables\n",
        "    s = \"\".join(URDU_MAP.get(ch, ch) for ch in s)\n",
        "    # Collapse spaces\n",
        "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
        "    return s\n",
        "\n",
        "def normalize_roman(s: str) -> str:\n",
        "    if not isinstance(s, str): return \"\"\n",
        "    s = unicodedata.normalize(\"NFC\", s)\n",
        "    s = s.replace(\"’\", \"'\").replace(\"“\", '\"').replace(\"”\", '\"')\n",
        "    s = re.sub(f\"[{re.escape(ZW_CHARS)}]\", \"\", s)\n",
        "    # unify long vowels a bit (gentle)\n",
        "    s = re.sub(r\"\\baa\\b\", \"aa\", s, flags=re.I)\n",
        "    s = re.sub(r\"\\bee\\b\", \"ee\", s, flags=re.I)\n",
        "    s = re.sub(r\"\\boo\\b\", \"oo\", s, flags=re.I)\n",
        "    # collapse spaces\n",
        "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
        "    return s\n",
        "\n",
        "def looks_like_title(line: str) -> bool:\n",
        "    # Heuristic: titles are often long slugs or very short with no Urdu letters\n",
        "    urdu_chars = re.findall(r\"[\\u0600-\\u06FF]\", line)\n",
        "    if not line: return True\n",
        "    if len(urdu_chars) == 0 and len(line.split()) <= 3:\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "# --------------------------\n",
        "# 2) File readers\n",
        "# --------------------------\n",
        "def read_text_file(p: Path) -> list[str]:\n",
        "    \"\"\"Read a text file with unknown extension; try utf-8 then latin-1 as fallback.\"\"\"\n",
        "    try_enc = [\"utf-8\", \"utf-8-sig\", \"cp1256\", \"latin-1\"]\n",
        "    for enc in try_enc:\n",
        "        try:\n",
        "            with open(p, \"r\", encoding=enc, errors=\"ignore\") as f:\n",
        "                raw = f.read()\n",
        "            break\n",
        "        except Exception:\n",
        "            continue\n",
        "    else:\n",
        "        return []\n",
        "\n",
        "    lines = [ln.strip() for ln in raw.splitlines()]\n",
        "    # remove empty & very short decoration lines\n",
        "    lines = [ln for ln in lines if ln.strip() != \"\"]\n",
        "    return lines\n",
        "\n",
        "def collect_language_folder(poet_dir: Path, lang_code: str) -> dict[str, list[str]]:\n",
        "    \"\"\"\n",
        "    Returns a dict: { ghazal_key -> list_of_lines } for given language subfolder.\n",
        "    lang_code in {\"ur\",\"en\",\"hi\"} (en ~ English transliteration ~ Roman Urdu).\n",
        "    Handles files WITH or WITHOUT extensions.\n",
        "    \"\"\"\n",
        "    base = poet_dir / lang_code\n",
        "    if not base.exists():\n",
        "        return {}\n",
        "    files = sorted([p for p in base.rglob(\"*\") if p.is_file()])\n",
        "    out = {}\n",
        "    for f in files:\n",
        "        key = f.name  # keep full filename (with/without ext) to maximize pairing\n",
        "        # Also allow pairing by stem if we need later\n",
        "        lines = read_text_file(f)\n",
        "        # remove leading title-ish junk line if it looks like a header\n",
        "        if lines and looks_like_title(lines[0]) and len(lines) > 1:\n",
        "            lines = lines[1:]\n",
        "        out[key] = lines\n",
        "    return out\n",
        "\n",
        "# --------------------------\n",
        "# 3) Pairing Urdu ↔ Roman per ghazal file\n",
        "# --------------------------\n",
        "def align_lines(ur_lines: list[str], rom_lines: list[str]) -> list[tuple[str, str]]:\n",
        "    \"\"\"\n",
        "    Align lines by index. If lengths differ, cut to min length.\n",
        "    Apply normalization per line.\n",
        "    Drop pairs that become empty after normalization.\n",
        "    \"\"\"\n",
        "    n = min(len(ur_lines), len(rom_lines))\n",
        "    pairs = []\n",
        "    for i in range(n):\n",
        "        ur = normalize_urdu(ur_lines[i])\n",
        "        rom = normalize_roman(rom_lines[i])\n",
        "        # filter trivial separators\n",
        "        if ur and rom and not re.fullmatch(r\"[-–—…\\.\\*]+\", ur) and not re.fullmatch(r\"[-–—…\\.\\*]+\", rom):\n",
        "            pairs.append((ur, rom))\n",
        "    return pairs\n",
        "\n",
        "def best_key_match(key: str, candidates: set[str]) -> str | None:\n",
        "    \"\"\"\n",
        "    Try exact key, then stem match, then relaxed slug match.\n",
        "    \"\"\"\n",
        "    if key in candidates:\n",
        "        return key\n",
        "    stem = Path(key).stem\n",
        "    by_stem = {Path(c).stem: c for c in candidates}\n",
        "    if stem in by_stem:\n",
        "        return by_stem[stem]\n",
        "    # relaxed: remove trailing numerals like -1/-2, etc.\n",
        "    stem_relaxed = re.sub(r\"[-_]{0,1}\\d+$\", \"\", stem)\n",
        "    for s, full in by_stem.items():\n",
        "        if re.sub(r\"[-_]{0,1}\\d+$\", \"\", s) == stem_relaxed:\n",
        "            return full\n",
        "    return None\n",
        "\n",
        "# --------------------------\n",
        "# 4) Walk poets and build a dataframe of pairs\n",
        "# --------------------------\n",
        "rows = []\n",
        "poets = sorted([d for d in DATASET_DIR.iterdir() if d.is_dir()])\n",
        "\n",
        "for poet_dir in tqdm(poets, desc=\"Poets\"):\n",
        "    poet = poet_dir.name\n",
        "    ur_map = collect_language_folder(poet_dir, \"ur\")\n",
        "    en_map = collect_language_folder(poet_dir, \"en\")  # Roman Urdu (English transliteration)\n",
        "    if not en_map:\n",
        "        # Some dumps may use 'roman' instead of 'en'; try that too\n",
        "        en_map = collect_language_folder(poet_dir, \"roman\")\n",
        "    if not ur_map or not en_map:\n",
        "        # Keep note but continue\n",
        "        print(f\"Note: skipping '{poet}' (ur={len(ur_map)}, en/roman={len(en_map)})\")\n",
        "        continue\n",
        "\n",
        "    en_keys = set(en_map.keys())\n",
        "    for key_ur, ur_lines in ur_map.items():\n",
        "        key_en = best_key_match(key_ur, en_keys)\n",
        "        if key_en is None:\n",
        "            # attempt match by stem across all en files quickly\n",
        "            continue\n",
        "        rom_lines = en_map.get(key_en, [])\n",
        "        pairs = align_lines(ur_lines, rom_lines)\n",
        "        ghazal_id = Path(key_ur).stem\n",
        "        for idx, (src_ur, tgt_rom) in enumerate(pairs):\n",
        "            rows.append({\n",
        "                \"poet\": poet,\n",
        "                \"ghazal_id\": ghazal_id,\n",
        "                \"line_idx\": idx,\n",
        "                \"src_ur\": src_ur,\n",
        "                \"tgt_rom_gold\": tgt_rom,  # gold transliteration when present\n",
        "                \"tgt_rom_rule\": None,     # placeholder (we’ll fill later if needed)\n",
        "            })\n",
        "\n",
        "df = pd.DataFrame(rows)\n",
        "print(\"\\nCollected pairs:\", len(df))\n",
        "print(\"Unique poets parsed:\", df[\"poet\"].nunique() if not df.empty else 0)\n",
        "\n",
        "# Basic sanity stats\n",
        "if not df.empty:\n",
        "    lens_src = df[\"src_ur\"].str.len()\n",
        "    lens_tgt = df[\"tgt_rom_gold\"].str.len()\n",
        "    print(\"Avg src len:\", round(lens_src.mean(), 1), \"| 95p:\", int(lens_src.quantile(0.95)))\n",
        "    print(\"Avg tgt len:\", round(lens_tgt.mean(), 1), \"| 95p:\", int(lens_tgt.quantile(0.95)))\n",
        "\n",
        "# Save raw (gold-present) pairs\n",
        "RAW_PATH = OUT_DIR / \"train_raw_gold.parquet\"\n",
        "if not df.empty:\n",
        "    df.to_parquet(RAW_PATH, index=False)\n",
        "    print(\"✅ Saved:\", RAW_PATH)\n",
        "else:\n",
        "    print(\"⚠️ No aligned pairs found yet. We may need to handle alternative layouts.\")\n",
        "\n",
        "# Peek a few rows\n",
        "print(\"\\nSample rows:\")\n",
        "print(df.sample(min(10, len(df)), random_state=42) if not df.empty else \"No data\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "78dmXRQMYQoV",
        "outputId": "691eaba7-970c-42ad-aae4-f8db28b3de33"
      },
      "execution_count": 4,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using DATASET_DIR = /content/drive/MyDrive/dataset\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Poets: 100%|██████████| 30/30 [27:45<00:00, 55.53s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Collected pairs: 20983\n",
            "Unique poets parsed: 30\n",
            "Avg src len: 33.2 | 95p: 43\n",
            "Avg tgt len: 40.3 | 95p: 52\n",
            "✅ Saved: /content/nmt_urdu_roman/data/train_raw_gold.parquet\n",
            "\n",
            "Sample rows:\n",
            "                     poet                                          ghazal_id  \\\n",
            "10172        javed-akhtar  mujh-ko-yaqiin-hai-sach-kahtii-thiin-jo-bhii-a...   \n",
            "14968        mirza-ghalib  maze-jahaan-ke-apnii-nazar-men-khaak-nahiin-mi...   \n",
            "13406        mirza-ghalib  dost-gam-khvaarii-men-merii-saii-farmaavenge-k...   \n",
            "3211   bahadur-shah-zafar  khvaah-kar-insaaf-zaalim-khvaah-kar-bedaad-tuu...   \n",
            "6914     firaq-gorakhpuri  samajhtaa-huun-ki-tuu-mujh-se-judaa-hai-firaq-...   \n",
            "18471          nida-fazli  safar-men-dhuup-to-hogii-jo-chal-sako-to-chalo...   \n",
            "11114    jigar-moradabadi  kuchh-is-adaa-se-aaj-vo-pahluu-nashiin-rahe-ji...   \n",
            "3725   bahadur-shah-zafar  zulf-jo-rukh-par-tire-ai-mehr-e-talat-khul-gai...   \n",
            "19854  wali-mohammad-wali  aaj-distaa-hai-haal-kuchh-kaa-kuchh-wali-moham...   \n",
            "10594    jigar-moradabadi  allaah-agar-taufiiq-na-de-insaan-ke-bas-kaa-ka...   \n",
            "\n",
            "       line_idx                                          src_ur  \\\n",
            "10172        12  ایک یہ گہر جس گہر میں میرا ساز و ساماں رہتا ہے   \n",
            "14968         0               مزے جہان کے اپنی نظر میں خاک نہیں   \n",
            "13406         4             حضرت ناصح گر آویں دیدہ و دل فرش راہ   \n",
            "3211          1        پر جو فریادی ہیں ان کی سن تو لے فریاد تو   \n",
            "6914         21                      کویی اس رنگ سے شرما رہا ہے   \n",
            "18471         1         سبہی ہیں بہیڑ میں تم بہی نکل سکو تو چلو   \n",
            "11114        32                    اس عشق کی تلافی مافات دیکہنا   \n",
            "3725         13           اب گرہ دل کی ہمارے فی الحقیقت کہل گیی   \n",
            "19854         8                       اے ولیؔ دل کوں آج کرتی ہے   \n",
            "10594         2   یہ تو نے کہا کیا اے ناداں فیاضی قدرت عام نہیں   \n",
            "\n",
            "                                            tgt_rom_gold tgt_rom_rule  \n",
            "10172  ek ye ghar jis ghar meñ merā sāz-o-sāmāñ rahtā...         None  \n",
            "14968           maze jahān ke apnī nazar meñ ḳhaak nahīñ         None  \n",
            "13406   hazrat-e-nāseh gar aaveñ diida o dil farsh-e-rāh         None  \n",
            "3211     par jo fariyādī haiñ un kī sun to le fariyād tū         None  \n",
            "6914                      koī is rañg se sharmā rahā hai         None  \n",
            "18471   sabhī haiñ bhiiḌ meñ tum bhī nikal sako to chalo         None  \n",
            "11114                   is ishq kī talāfi-e-māfāt dekhnā         None  \n",
            "3725        ab girah dil kī hamāre fil-haqīqat khul ga.ī         None  \n",
            "19854                   ai 'valī' dil kuuñ aaj kartī hai         None  \n",
            "10594  ye tū ne kahā kyā ai nādāñ fayyāzī-e-qudrat aa...         None  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Colab Cell #3 — Rule translit, unified corpus, 50/25/25 split,\n",
        "#                  char vocabs saved to artifacts/\n",
        "# ============================================================\n",
        "from pathlib import Path\n",
        "import json, re, math, random\n",
        "import pandas as pd\n",
        "from collections import Counter, defaultdict\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load config and paths\n",
        "cfg = json.load(open(\"/content/nmt_urdu_roman/project_config.json\", \"r\", encoding=\"utf-8\"))\n",
        "PROJECT_DIR = Path(cfg[\"project_dir\"])\n",
        "DATA_DIR = PROJECT_DIR / \"data\"\n",
        "ARTI = PROJECT_DIR / \"artifacts\"\n",
        "ARTI.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "RAW_GOLD = DATA_DIR / \"train_raw_gold.parquet\"\n",
        "assert RAW_GOLD.exists(), f\"Missing: {RAW_GOLD}\"\n",
        "df = pd.read_parquet(RAW_GOLD)\n",
        "print(\"Loaded gold pairs:\", len(df))\n",
        "\n",
        "# --------------------------\n",
        "# 1) Rule-based Urdu→Roman\n",
        "# --------------------------\n",
        "# This is a light transliteration (not phonetic-perfect), good as a fallback.\n",
        "UR2ROM = {\n",
        "    \"ا\":\"a\", \"آ\":\"aa\", \"أ\":\"a\", \"إ\":\"i\", \"ٱ\":\"a\", \"ء\":\"'\", \"ؤ\":\"o\", \"ئ\":\"i\",\n",
        "    \"ب\":\"b\", \"پ\":\"p\", \"ت\":\"t\", \"ٹ\":\"t\", \"ث\":\"s\", \"ج\":\"j\", \"چ\":\"ch\", \"ح\":\"h\",\n",
        "    \"خ\":\"kh\", \"د\":\"d\", \"ڈ\":\"d\", \"ذ\":\"z\", \"ر\":\"r\", \"ڑ\":\"r\", \"ز\":\"z\", \"ژ\":\"zh\",\n",
        "    \"س\":\"s\", \"ش\":\"sh\", \"ص\":\"s\", \"ض\":\"z\", \"ط\":\"t\", \"ظ\":\"z\", \"ع\":\"'\", \"غ\":\"gh\",\n",
        "    \"ف\":\"f\", \"ق\":\"q\", \"ک\":\"k\", \"گ\":\"g\", \"ل\":\"l\", \"م\":\"m\", \"ن\":\"n\", \"ں\":\"n\",\n",
        "    \"و\":\"v\",  # will handle vowels via context rules below\n",
        "    \"ہ\":\"h\", \"ھ\":\"h\", \"ء\":\"'\", \"ٔ\":\"\", \"ٰ\":\"\", \"ى\":\"a\", \"ی\":\"y\", \"ے\":\"e\",\n",
        "    \"،\":\",\", \"؛\":\";\", \"؟\":\"?\", \"۔\":\".\", \"—\":\"-\", \"–\":\"-\", \"ْ\":\"\", \"ّ\":\"\"\n",
        "}\n",
        "# Digits and spaces\n",
        "for d_ar, d_en in zip(\"٠١٢٣٤٥٦٧٨٩\", \"0123456789\"):\n",
        "    UR2ROM[d_ar] = d_en\n",
        "\n",
        "# Simple vowel context rules\n",
        "def urdu_to_roman_rule(text: str) -> str:\n",
        "    if not isinstance(text, str): return \"\"\n",
        "    out = []\n",
        "    for i, ch in enumerate(text):\n",
        "        rom = UR2ROM.get(ch)\n",
        "        if rom is None:\n",
        "            # Urdu whitespace / Latin punctuation passthrough\n",
        "            if re.match(r\"[\\u0600-\\u06FF]\", ch):\n",
        "                rom = \"\"  # unknown Urdu char → empty\n",
        "            else:\n",
        "                rom = ch\n",
        "        out.append(rom)\n",
        "    s = \"\".join(out)\n",
        "\n",
        "    # Heuristics:\n",
        "    #   - handle long vowels around y/w (ی/و became y/v)\n",
        "    #   - basic combos\n",
        "    s = re.sub(r\"\\bkh\", \"kh\", s)  # keep\n",
        "    s = re.sub(r\"gh\", \"gh\", s)\n",
        "\n",
        "    # Normalize multiple apostrophes/spaces\n",
        "    s = re.sub(r\"'+\", \"'\", s)\n",
        "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
        "\n",
        "    # Light tidy for v/w and y/i/u heuristics (very rough):\n",
        "    s = re.sub(r\"\\bv\", \"w\", s)  # often و is 'w' in roman urdu\n",
        "    s = re.sub(r\"([aeiou])y\\b\", r\"\\1i\", s)\n",
        "    return s\n",
        "\n",
        "# Fill rule transliteration only where gold missing (future-proof)\n",
        "needs_rule = df[\"tgt_rom_gold\"].isna() | df[\"tgt_rom_gold\"].eq(\"\")\n",
        "if needs_rule.any():\n",
        "    df.loc[needs_rule, \"tgt_rom_rule\"] = df.loc[needs_rule, \"src_ur\"].apply(urdu_to_roman_rule)\n",
        "\n",
        "# Choose final target: prefer gold, else rule\n",
        "df[\"tgt_rom\"] = df[\"tgt_rom_gold\"].fillna(\"\").replace(\"\", pd.NA)\n",
        "df[\"tgt_rom\"] = df[\"tgt_rom\"].fillna(df[\"tgt_rom_rule\"].fillna(\"\"))\n",
        "\n",
        "# Drop empty/degenerate rows\n",
        "before = len(df)\n",
        "df = df[(df[\"src_ur\"].str.len() >= 2) & (df[\"tgt_rom\"].str.len() >= 2)]\n",
        "df = df[~df[\"src_ur\"].str.fullmatch(r\"[-–—…\\.\\*]+\")]\n",
        "df = df[~df[\"tgt_rom\"].str.fullmatch(r\"[-–—…\\.\\*]+\")]\n",
        "after = len(df)\n",
        "print(f\"Filtered: {before} → {after}\")\n",
        "\n",
        "# --------------------------\n",
        "# 2) Ghazal-level split (50/25/25)\n",
        "#    to avoid same ghazal leaking across sets\n",
        "# --------------------------\n",
        "key = df[\"poet\"].astype(str) + \"§\" + df[\"ghazal_id\"].astype(str)\n",
        "unique_gh = key.drop_duplicates().tolist()\n",
        "random.Random(42).shuffle(unique_gh)\n",
        "\n",
        "n = len(unique_gh)\n",
        "n_train = int(0.50 * n)\n",
        "n_val   = int(0.25 * n)\n",
        "train_gh = set(unique_gh[:n_train])\n",
        "val_gh   = set(unique_gh[n_train:n_train+n_val])\n",
        "test_gh  = set(unique_gh[n_train+n_val:])\n",
        "\n",
        "def assign_split(row):\n",
        "    k = f\"{row.poet}§{row.ghazal_id}\"\n",
        "    if k in train_gh: return \"train\"\n",
        "    if k in val_gh:   return \"val\"\n",
        "    return \"test\"\n",
        "\n",
        "df[\"split\"] = df.apply(assign_split, axis=1)\n",
        "\n",
        "print(df[\"split\"].value_counts())\n",
        "\n",
        "# Save split files\n",
        "for sp in [\"train\", \"val\", \"test\"]:\n",
        "    outp = DATA_DIR / f\"pairs_{sp}.parquet\"\n",
        "    df[df[\"split\"] == sp][[\"poet\",\"ghazal_id\",\"line_idx\",\"src_ur\",\"tgt_rom\"]].to_parquet(outp, index=False)\n",
        "    print(\"✅ Saved:\", outp)\n",
        "\n",
        "# --------------------------\n",
        "# 3) Build character-level vocabs\n",
        "# --------------------------\n",
        "SPECIAL = [\"<pad>\", \"<s>\", \"</s>\", \"<unk>\"]\n",
        "\n",
        "def build_char_vocab(series: pd.Series, extra_chars=None):\n",
        "    counter = Counter()\n",
        "    for s in series.astype(str).tolist():\n",
        "        for ch in s:\n",
        "            counter[ch] += 1\n",
        "    chars = sorted(counter.keys())\n",
        "    if extra_chars:\n",
        "        for ch in extra_chars:\n",
        "            if ch not in chars: chars.append(ch)\n",
        "    # index mapping\n",
        "    itos = SPECIAL + chars\n",
        "    stoi = {ch:i for i,ch in enumerate(itos)}\n",
        "    meta = {\"size\": len(itos), \"num_special\": len(SPECIAL)}\n",
        "    return {\"itos\": itos, \"stoi\": stoi, \"meta\": meta}\n",
        "\n",
        "train_df = df[df[\"split\"]==\"train\"]\n",
        "\n",
        "src_vocab = build_char_vocab(train_df[\"src_ur\"])\n",
        "tgt_vocab = build_char_vocab(train_df[\"tgt_rom\"])\n",
        "\n",
        "with open(ARTI / \"vocab_src_char.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(src_vocab, f, ensure_ascii=False, indent=2)\n",
        "with open(ARTI / \"vocab_tgt_char.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(tgt_vocab, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "print(\"\\nVocab sizes — src:\", src_vocab[\"meta\"][\"size\"], \"tgt:\", tgt_vocab[\"meta\"][\"size\"])\n",
        "print(\"Special tokens:\", SPECIAL)\n",
        "\n",
        "# Quick peek\n",
        "print(\"\\nSamples:\")\n",
        "print(df[df[\"split\"]==\"train\"][[\"src_ur\",\"tgt_rom\"]].head(5))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FdkWFjv3YfPk",
        "outputId": "cc3fdf66-3916-45f6-ef04-a335a050c42d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded gold pairs: 20983\n",
            "Filtered: 20983 → 20983\n",
            "split\n",
            "train    10493\n",
            "test      5255\n",
            "val       5235\n",
            "Name: count, dtype: int64\n",
            "✅ Saved: /content/nmt_urdu_roman/data/pairs_train.parquet\n",
            "✅ Saved: /content/nmt_urdu_roman/data/pairs_val.parquet\n",
            "✅ Saved: /content/nmt_urdu_roman/data/pairs_test.parquet\n",
            "\n",
            "Vocab sizes — src: 50 tgt: 46\n",
            "Special tokens: ['<pad>', '<s>', '</s>', '<unk>']\n",
            "\n",
            "Samples:\n",
            "                                    src_ur  \\\n",
            "24      اب اور کیا کسی سے مراسم بڑہاییں ہم   \n",
            "25  یہ بہی بہت ہے تجہ کو اگر بہول جاییں ہم   \n",
            "26      صحرایے زندگی میں کویی دوسرا نہ تہا   \n",
            "27       سنتے رہے ہیں آپ ہی اپنی صداییں ہم   \n",
            "28        اس زندگی میں اتنی فراغت کسے نصیب   \n",
            "\n",
            "                                           tgt_rom  \n",
            "24         ab aur kyā kisī se marāsim baḌhā.eñ ham  \n",
            "25  ye bhī bahut hai tujh ko agar bhuul jaa.eñ ham  \n",
            "26            sahrā-e-zindagī meñ koī dūsrā na thā  \n",
            "27         sunte rahe haiñ aap hī apnī sadā.eñ ham  \n",
            "28         is zindagī meñ itnī farāġhat kise nasīb  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Colab Cell #4 — Train SentencePiece (patched) + Dataset/Loaders\n",
        "#   - Trains separate SPM models for src (Urdu) & tgt (Roman)\n",
        "#   - Uses normalization_rule_name=\"identity\" to avoid nmt_nfkc error\n",
        "#   - Builds PyTorch Dataset (char/spm), DataLoaders, and saves exp config\n",
        "# ============================================================\n",
        "from pathlib import Path\n",
        "import json, os, io, random\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import sentencepiece as spm\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# ----- Paths & config\n",
        "cfg = json.load(open(\"/content/nmt_urdu_roman/project_config.json\",\"r\",encoding=\"utf-8\"))\n",
        "PROJECT_DIR = Path(cfg[\"project_dir\"])\n",
        "DATA_DIR    = PROJECT_DIR / \"data\"\n",
        "ARTI        = PROJECT_DIR / \"artifacts\"\n",
        "ARTI.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "TRAIN_PQ = DATA_DIR / \"pairs_train.parquet\"\n",
        "VAL_PQ   = DATA_DIR / \"pairs_val.parquet\"\n",
        "TEST_PQ  = DATA_DIR / \"pairs_test.parquet\"\n",
        "\n",
        "df_train = pd.read_parquet(TRAIN_PQ)\n",
        "df_val   = pd.read_parquet(VAL_PQ)\n",
        "df_test  = pd.read_parquet(TEST_PQ)\n",
        "\n",
        "# ============================================================\n",
        "# (A) Train SentencePiece tokenizers (patched)\n",
        "#     Separate models for src/tgt. Reserve: pad=0, bos=1, eos=2, unk=3\n",
        "# ============================================================\n",
        "SRC_TXT = ARTI / \"spm_src_train.txt\"\n",
        "TGT_TXT = ARTI / \"spm_tgt_train.txt\"\n",
        "SRC_MODEL = ARTI / \"spm_src.model\"\n",
        "TGT_MODEL = ARTI / \"spm_tgt.model\"\n",
        "\n",
        "if not SRC_TXT.exists():\n",
        "    SRC_TXT.write_text(\"\\n\".join(df_train[\"src_ur\"].astype(str).tolist()), encoding=\"utf-8\")\n",
        "if not TGT_TXT.exists():\n",
        "    TGT_TXT.write_text(\"\\n\".join(df_train[\"tgt_rom\"].astype(str).tolist()), encoding=\"utf-8\")\n",
        "\n",
        "SPM_SRC_VOCAB = 2000\n",
        "SPM_TGT_VOCAB = 2000\n",
        "\n",
        "def _clean_partial_models(model_prefix_str: str):\n",
        "    # delete tiny/corrupt artifacts to allow clean retrain\n",
        "    for ext in (\".model\", \".vocab\"):\n",
        "        f = Path(model_prefix_str + ext)\n",
        "        if f.exists() and f.stat().st_size < 1024:\n",
        "            f.unlink(missing_ok=True)\n",
        "\n",
        "def train_spm(input_path, model_path, vocab_size, character_coverage=0.9995, model_type=\"bpe\"):\n",
        "    model_prefix = str(Path(model_path).with_suffix(\"\"))\n",
        "    _clean_partial_models(model_prefix)\n",
        "    if Path(model_prefix + \".model\").exists():\n",
        "        return  # already trained\n",
        "\n",
        "    # Key fix: normalization_rule_name=\"identity\" (prevents 'nmt_nfkc' missing error)\n",
        "    spm.SentencePieceTrainer.Train(\n",
        "        input=str(input_path),\n",
        "        model_prefix=model_prefix,\n",
        "        vocab_size=int(vocab_size),\n",
        "        model_type=model_type,\n",
        "        character_coverage=float(character_coverage),\n",
        "        pad_id=0, bos_id=1, eos_id=2, unk_id=3,\n",
        "        input_sentence_size=1_000_000,\n",
        "        shuffle_input_sentence=True,\n",
        "        normalization_rule_name=\"identity\",\n",
        "    )\n",
        "\n",
        "# Train patched SPM models\n",
        "train_spm(SRC_TXT, SRC_MODEL, SPM_SRC_VOCAB, character_coverage=0.9995, model_type=\"bpe\")\n",
        "train_spm(TGT_TXT, TGT_MODEL, SPM_TGT_VOCAB, character_coverage=0.9995, model_type=\"bpe\")\n",
        "\n",
        "# Load processors\n",
        "sp_src = spm.SentencePieceProcessor()\n",
        "sp_tgt = spm.SentencePieceProcessor()\n",
        "sp_src.load(str(SRC_MODEL))\n",
        "sp_tgt.load(str(TGT_MODEL))\n",
        "\n",
        "print(\"SPM src size:\", sp_src.get_piece_size(), \"| tgt size:\", sp_tgt.get_piece_size())\n",
        "\n",
        "# ============================================================\n",
        "# (B) Tokenization utilities\n",
        "#     - CHAR mode uses vocab jsons from Cell #3\n",
        "#     - SPM mode uses the SentencePiece models above\n",
        "# ============================================================\n",
        "SPECIAL = [\"<pad>\", \"<s>\", \"</s>\", \"<unk>\"]\n",
        "v_src_char = json.load(open(ARTI / \"vocab_src_char.json\", \"r\", encoding=\"utf-8\"))\n",
        "v_tgt_char = json.load(open(ARTI / \"vocab_tgt_char.json\", \"r\", encoding=\"utf-8\"))\n",
        "stoi_src_char = v_src_char[\"stoi\"]; itos_src_char = v_src_char[\"itos\"]\n",
        "stoi_tgt_char = v_tgt_char[\"stoi\"]; itos_tgt_char = v_tgt_char[\"itos\"]\n",
        "\n",
        "PAD_ID = 0; BOS_ID = 1; EOS_ID = 2; UNK_ID = 3\n",
        "\n",
        "def encode_char_src(s: str):\n",
        "    ids = [BOS_ID]\n",
        "    for ch in s:\n",
        "        ids.append(stoi_src_char.get(ch, UNK_ID))\n",
        "    ids.append(EOS_ID)\n",
        "    return ids\n",
        "\n",
        "def encode_char_tgt(s: str):\n",
        "    ids = [BOS_ID]\n",
        "    for ch in s:\n",
        "        ids.append(stoi_tgt_char.get(ch, UNK_ID))\n",
        "    ids.append(EOS_ID)\n",
        "    return ids\n",
        "\n",
        "def encode_spm_src(s: str):\n",
        "    return [BOS_ID] + sp_src.encode(s, out_type=int, enable_sampling=False) + [EOS_ID]\n",
        "\n",
        "def encode_spm_tgt(s: str):\n",
        "    return [BOS_ID] + sp_tgt.encode(s, out_type=int, enable_sampling=False) + [EOS_ID]\n",
        "\n",
        "# ============================================================\n",
        "# (C) PyTorch Dataset + Collate (supports 'char' or 'spm')\n",
        "# ============================================================\n",
        "class NMTDataset(Dataset):\n",
        "    def __init__(self, df: pd.DataFrame, mode=\"char\"):\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.mode = mode\n",
        "        assert mode in (\"char\", \"spm\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        src = str(row[\"src_ur\"])\n",
        "        tgt = str(row[\"tgt_rom\"])\n",
        "        if self.mode == \"char\":\n",
        "            src_ids = encode_char_src(src)\n",
        "            tgt_ids = encode_char_tgt(tgt)\n",
        "        else:\n",
        "            src_ids = encode_spm_src(src)\n",
        "            tgt_ids = encode_spm_tgt(tgt)\n",
        "        return {\n",
        "            \"src_ids\": torch.tensor(src_ids, dtype=torch.long),\n",
        "            \"tgt_ids\": torch.tensor(tgt_ids, dtype=torch.long),\n",
        "            \"len_src\": len(src_ids),\n",
        "            \"len_tgt\": len(tgt_ids),\n",
        "        }\n",
        "\n",
        "def collate_fn(batch):\n",
        "    # Sort by src length (desc) for minor efficiency\n",
        "    batch = sorted(batch, key=lambda x: x[\"len_src\"], reverse=True)\n",
        "    src_lens = [b[\"len_src\"] for b in batch]\n",
        "    tgt_lens = [b[\"len_tgt\"] for b in batch]\n",
        "    max_src = max(src_lens)\n",
        "    max_tgt = max(tgt_lens)\n",
        "\n",
        "    def pad_seq(seq, max_len):\n",
        "        out = torch.full((max_len,), PAD_ID, dtype=torch.long)\n",
        "        out[:len(seq)] = seq\n",
        "        return out\n",
        "\n",
        "    src_pad = torch.stack([pad_seq(b[\"src_ids\"], max_src) for b in batch], dim=0)\n",
        "    tgt_pad = torch.stack([pad_seq(b[\"tgt_ids\"], max_tgt) for b in batch], dim=0)\n",
        "\n",
        "    return {\n",
        "        \"src_ids\": src_pad,    # [B, Tsrc]\n",
        "        \"tgt_ids\": tgt_pad,    # [B, Ttgt]\n",
        "        \"src_lens\": torch.tensor(src_lens, dtype=torch.long),\n",
        "        \"tgt_lens\": torch.tensor(tgt_lens, dtype=torch.long),\n",
        "    }\n",
        "\n",
        "# ============================================================\n",
        "# (D) Build DataLoaders (both modes so you can A/B later)\n",
        "# ============================================================\n",
        "def make_loaders(mode=\"char\", batch_size=64, num_workers=2, shuffle_train=True):\n",
        "    ds_train = NMTDataset(df_train, mode=mode)\n",
        "    ds_val   = NMTDataset(df_val,   mode=mode)\n",
        "    ds_test  = NMTDataset(df_test,  mode=mode)\n",
        "\n",
        "    # pin_memory only helps on CUDA; harmless on CPU but can warn\n",
        "    dl_train = DataLoader(ds_train, batch_size=batch_size, shuffle=shuffle_train,\n",
        "                          num_workers=num_workers, collate_fn=collate_fn, pin_memory=torch.cuda.is_available())\n",
        "    dl_val   = DataLoader(ds_val,   batch_size=batch_size, shuffle=False,\n",
        "                          num_workers=num_workers, collate_fn=collate_fn, pin_memory=torch.cuda.is_available())\n",
        "    dl_test  = DataLoader(ds_test,  batch_size=batch_size, shuffle=False,\n",
        "                          num_workers=num_workers, collate_fn=collate_fn, pin_memory=torch.cuda.is_available())\n",
        "    return ds_train, ds_val, ds_test, dl_train, dl_val, dl_test\n",
        "\n",
        "# Quick smoke test (char mode)\n",
        "_, _, _, dl_tr_char, dl_v_char, _ = make_loaders(mode=\"char\", batch_size=32)\n",
        "batch_char = next(iter(dl_tr_char))\n",
        "print(\"CHAR batch shapes:\",\n",
        "      batch_char[\"src_ids\"].shape, batch_char[\"tgt_ids\"].shape)\n",
        "\n",
        "# Quick smoke test (spm mode)\n",
        "_, _, _, dl_tr_spm, dl_v_spm, _ = make_loaders(mode=\"spm\", batch_size=32)\n",
        "batch_spm = next(iter(dl_tr_spm))\n",
        "print(\"SPM  batch shapes:\",\n",
        "      batch_spm[\"src_ids\"].shape, batch_spm[\"tgt_ids\"].shape)\n",
        "\n",
        "# ============================================================\n",
        "# (E) Save a default experiment config\n",
        "# ============================================================\n",
        "exp_cfg = {\n",
        "    \"tokenization\": \"char\",   # or \"spm\"\n",
        "    \"embedding_dim\": 256,\n",
        "    \"hidden_size\": 256,\n",
        "    \"enc_layers\": 2,\n",
        "    \"dec_layers\": 4,\n",
        "    \"dropout\": 0.3,\n",
        "    \"learning_rate\": 5e-4,\n",
        "    \"batch_size\": 64,\n",
        "    \"teacher_forcing_start\": 1.0,\n",
        "    \"teacher_forcing_end\": 0.5,\n",
        "    \"epochs\": 20,\n",
        "    \"grad_clip\": 1.0,\n",
        "    \"beam_size\": 5\n",
        "}\n",
        "with open(ARTI / \"exp_default.json\",\"w\",encoding=\"utf-8\") as f:\n",
        "    json.dump(exp_cfg, f, indent=2)\n",
        "print(\"Saved default exp config:\", ARTI / \"exp_default.json\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B29Cie8lbHVu",
        "outputId": "d336506b-ea0d-4332-f455-f12914690ef1"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SPM src size: 2000 | tgt size: 2000\n",
            "CHAR batch shapes: torch.Size([32, 41]) torch.Size([32, 49])\n",
            "SPM  batch shapes: torch.Size([32, 17]) torch.Size([32, 25])\n",
            "Saved default exp config: /content/nmt_urdu_roman/artifacts/exp_default.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Colab Cell #5 — BiLSTM Encoder + 4-Layer LSTM Decoder (Luong)\n",
        "#   • Restart-safe: redefines everything needed for training\n",
        "#   • Works with tokenization = \"char\" or \"spm\"\n",
        "#   • Trains, evaluates on val (loss, ppl, BLEU, CER), saves best\n",
        "# ============================================================\n",
        "import os, json, math, time, numpy as np\n",
        "from pathlib import Path\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.utils import clip_grad_norm_\n",
        "import sentencepiece as spm\n",
        "import sacrebleu\n",
        "from jiwer import cer as jiwer_cer\n",
        "\n",
        "# ---------- Paths / config ----------\n",
        "cfg = json.load(open(\"/content/nmt_urdu_roman/project_config.json\",\"r\",encoding=\"utf-8\"))\n",
        "PROJECT_DIR = Path(cfg[\"project_dir\"])\n",
        "ARTI        = PROJECT_DIR / \"artifacts\"\n",
        "MODELS_DIR  = PROJECT_DIR / \"models\"\n",
        "MODELS_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Load experiment base (you can override when calling train_model)\n",
        "exp = json.load(open(ARTI / \"exp_default.json\",\"r\",encoding=\"utf-8\"))\n",
        "\n",
        "# Special tokens\n",
        "PAD_ID = 0; BOS_ID = 1; EOS_ID = 2; UNK_ID = 3\n",
        "\n",
        "# Device\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", DEVICE)\n",
        "\n",
        "# ---------- Bring in loaders from Cell #4 (and validate) ----------\n",
        "try:\n",
        "    make_loaders\n",
        "except NameError:\n",
        "    raise RuntimeError(\"❌ make_loaders is undefined. Please re-run Cell #4 first.\")\n",
        "\n",
        "def get_loaders(mode, batch):\n",
        "    return make_loaders(mode=mode, batch_size=batch, num_workers=2, shuffle_train=True)\n",
        "\n",
        "# ---------- Tokenizers / decoders ----------\n",
        "def _load_tokenizers(tokenization):\n",
        "    if tokenization == \"char\":\n",
        "        v_src = json.load(open(ARTI / \"vocab_src_char.json\",\"r\",encoding=\"utf-8\"))\n",
        "        v_tgt = json.load(open(ARTI / \"vocab_tgt_char.json\",\"r\",encoding=\"utf-8\"))\n",
        "        SRC_VSIZE = len(v_src[\"itos\"]); TGT_VSIZE = len(v_tgt[\"itos\"])\n",
        "        itos_tgt  = v_tgt[\"itos\"]\n",
        "        def decode_char(ids):\n",
        "            out=[]\n",
        "            for i in ids:\n",
        "                if i in (PAD_ID, BOS_ID, EOS_ID): continue\n",
        "                out.append(itos_tgt[i] if 0 <= i < len(itos_tgt) else \"\")\n",
        "            return \"\".join(out).strip()\n",
        "        return {\"mode\":\"char\",\"SRC_V\":SRC_VSIZE,\"TGT_V\":TGT_VSIZE,\n",
        "                \"decode\":decode_char, \"sp_src\":None, \"sp_tgt\":None, \"v_tgt\":v_tgt}\n",
        "    else:\n",
        "        sp_src = spm.SentencePieceProcessor(); sp_src.load(str(ARTI / \"spm_src.model\"))\n",
        "        sp_tgt = spm.SentencePieceProcessor(); sp_tgt.load(str(ARTI / \"spm_tgt.model\"))\n",
        "        SRC_VSIZE = sp_src.get_piece_size(); TGT_VSIZE = sp_tgt.get_piece_size()\n",
        "        def decode_spm(ids):\n",
        "            ids = [i for i in ids if i not in (PAD_ID, BOS_ID, EOS_ID)]\n",
        "            try: return sp_tgt.decode(ids).strip()\n",
        "            except: return sp_tgt.decode_pieces([sp_tgt.id_to_piece(i) for i in ids]).strip()\n",
        "        return {\"mode\":\"spm\",\"SRC_V\":SRC_VSIZE,\"TGT_V\":TGT_VSIZE,\n",
        "                \"decode\":decode_spm, \"sp_src\":sp_src, \"sp_tgt\":sp_tgt, \"v_tgt\":None}\n",
        "\n",
        "# ---------- Model ----------\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, vocab_size, emb_dim, hid_size, num_layers=2, dropout=0.3, bidirectional=True):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, emb_dim, padding_idx=PAD_ID)\n",
        "        self.bilstm = nn.LSTM(\n",
        "            emb_dim, hid_size, num_layers=num_layers, dropout=dropout,\n",
        "            bidirectional=bidirectional, batch_first=True\n",
        "        )\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.bidirectional = bidirectional\n",
        "        self.hid_size = hid_size\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "    def forward(self, src_ids):\n",
        "        emb = self.dropout(self.embedding(src_ids))          # [B, T, E]\n",
        "        outputs, (hn, cn) = self.bilstm(emb)                 # outputs: [B, T, 2H] if bi\n",
        "        return outputs, (hn, cn)\n",
        "\n",
        "class LuongAttention(nn.Module):\n",
        "    # general: s_t^T * W * h_i\n",
        "    def __init__(self, dec_hid, enc_hid_bi):\n",
        "        super().__init__()\n",
        "        self.W = nn.Linear(dec_hid, enc_hid_bi, bias=False)\n",
        "    def forward(self, dec_h_t, enc_outputs, mask=None):\n",
        "        # dec_h_t: [B,H], enc_outputs: [B,Tsrc,Henc]\n",
        "        score = torch.bmm(self.W(dec_h_t).unsqueeze(1), enc_outputs.transpose(1,2))  # [B,1,Tsrc]\n",
        "        if mask is not None:\n",
        "            score = score.masked_fill(mask.unsqueeze(1), -1e9)\n",
        "        attn = torch.softmax(score, dim=-1)       # [B,1,Tsrc]\n",
        "        ctx  = torch.bmm(attn, enc_outputs).squeeze(1)  # [B,Henc]\n",
        "        return ctx, attn.squeeze(1)\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, vocab_size, emb_dim, hid_size, enc_hid_bi, num_layers=4, dropout=0.3):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, emb_dim, padding_idx=PAD_ID)\n",
        "        self.input_size = emb_dim + enc_hid_bi\n",
        "        self.lstm = nn.LSTM(self.input_size, hid_size, num_layers=num_layers,\n",
        "                            dropout=dropout, batch_first=True)\n",
        "        self.attn = LuongAttention(hid_size, enc_hid_bi)\n",
        "        self.fc_out = nn.Linear(hid_size + enc_hid_bi, vocab_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "    def forward_step(self, y_prev, h_c, enc_outputs, enc_pad_mask, ctx_prev):\n",
        "        # y_prev: [B]\n",
        "        emb = self.dropout(self.embedding(y_prev))           # [B,E]\n",
        "        lstm_in = torch.cat([emb, ctx_prev], dim=-1).unsqueeze(1)  # [B,1,E+Henc]\n",
        "        out, h_c = self.lstm(lstm_in, h_c)                   # out: [B,1,Hdec]\n",
        "        h_t = out.squeeze(1)                                 # [B,Hdec]\n",
        "        ctx_t, attn = self.attn(h_t, enc_outputs, enc_pad_mask)    # [B,Henc]\n",
        "        logits = self.fc_out(torch.cat([h_t, ctx_t], dim=-1))      # [B,V]\n",
        "        return logits, h_c, ctx_t, attn\n",
        "\n",
        "    # explicit forward so Module is happy\n",
        "    def forward(self, y_prev, h_c, enc_outputs, enc_pad_mask, ctx_prev):\n",
        "        return self.forward_step(y_prev, h_c, enc_outputs, enc_pad_mask, ctx_prev)\n",
        "\n",
        "class Bridge(nn.Module):\n",
        "    def __init__(self, enc_hid, dec_hid, dec_layers, bidirectional=True):\n",
        "        super().__init__()\n",
        "        mul = 2 if bidirectional else 1\n",
        "        self.h_proj = nn.Linear(enc_hid*mul, dec_hid)\n",
        "        self.c_proj = nn.Linear(enc_hid*mul, dec_hid)\n",
        "        self.dec_layers = dec_layers\n",
        "    def forward(self, enc_hn, enc_cn):\n",
        "        # enc_hn: [L*mul, B, H]\n",
        "        top_h = torch.cat([enc_hn[-2], enc_hn[-1]], dim=-1)  # [B, 2H]\n",
        "        top_c = torch.cat([enc_cn[-2], enc_cn[-1]], dim=-1)  # [B, 2H]\n",
        "        h0 = torch.tanh(self.h_proj(top_h))                  # [B,Hd]\n",
        "        c0 = torch.tanh(self.c_proj(top_c))                  # [B,Hd]\n",
        "        h0 = h0.unsqueeze(0).repeat(self.dec_layers, 1, 1)   # [Ldec,B,Hd]\n",
        "        c0 = c0.unsqueeze(0).repeat(self.dec_layers, 1, 1)\n",
        "        return (h0, c0)\n",
        "\n",
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, src_vsize, tgt_vsize, emb_dim, hid_size, enc_layers, dec_layers, dropout=0.3):\n",
        "        super().__init__()\n",
        "        self.encoder = Encoder(src_vsize, emb_dim, hid_size, enc_layers, dropout, bidirectional=True)\n",
        "        enc_hid_bi = hid_size * 2\n",
        "        self.decoder = Decoder(tgt_vsize, emb_dim, hid_size, enc_hid_bi, dec_layers, dropout)\n",
        "        self.bridge  = Bridge(hid_size, hid_size, dec_layers, bidirectional=True)\n",
        "\n",
        "    def forward(self, src_ids, tgt_ids, teacher_forcing=1.0):\n",
        "        # src_ids: [B,Tsrc], tgt_ids: [B,Ttgt]\n",
        "        B, Tt = tgt_ids.size()\n",
        "        enc_outputs, (hn, cn) = self.encoder(src_ids)            # enc_outputs: [B,Tsrc,2H]\n",
        "        enc_pad_mask = (src_ids == PAD_ID)\n",
        "\n",
        "        dec_hc = self.bridge(hn, cn)\n",
        "        ctx = torch.zeros(src_ids.size(0), enc_outputs.size(-1), device=src_ids.device)\n",
        "        logits_all = []\n",
        "        y_prev = tgt_ids[:,0]  # BOS\n",
        "\n",
        "        for t in range(1, Tt):\n",
        "            use_tf = (np.random.rand() < teacher_forcing)\n",
        "            logits, dec_hc, ctx, _ = self.decoder(y_prev, dec_hc, enc_outputs, enc_pad_mask, ctx)\n",
        "            logits_all.append(logits.unsqueeze(1))\n",
        "            y_prev = tgt_ids[:,t] if use_tf else torch.argmax(logits, dim=-1)\n",
        "\n",
        "        return torch.cat(logits_all, dim=1)  # [B, Tt-1, V]\n",
        "\n",
        "# ---------- Training / Evaluation ----------\n",
        "def sequence_nll(logits, tgt_ids):\n",
        "    # logits: [B,T-1,V], tgt_ids: [B,T]\n",
        "    B, Tm1, V = logits.shape\n",
        "    gold = tgt_ids[:,1:1+Tm1]                      # [B,T-1]\n",
        "    loss = F.cross_entropy(\n",
        "        logits.reshape(B*Tm1, V),\n",
        "        gold.reshape(B*Tm1),\n",
        "        ignore_index=PAD_ID\n",
        "    )\n",
        "    return loss\n",
        "\n",
        "def greedy_decode(model, batch, max_len=150):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        src = batch[\"src_ids\"].to(DEVICE)\n",
        "        enc_outputs, (hn, cn) = model.encoder(src)\n",
        "        enc_mask = (src == PAD_ID)\n",
        "        dec_hc = model.bridge(hn, cn)\n",
        "        ctx = torch.zeros(src.size(0), enc_outputs.size(-1), device=src.device)\n",
        "        y_prev = torch.full((src.size(0),), BOS_ID, dtype=torch.long, device=src.device)\n",
        "        outs = []\n",
        "        for _ in range(max_len):\n",
        "            logits, dec_hc, ctx, _ = model.decoder(y_prev, dec_hc, enc_outputs, enc_mask, ctx)\n",
        "            y_prev = torch.argmax(logits, dim=-1)\n",
        "            outs.append(y_prev.unsqueeze(1))\n",
        "        return torch.cat(outs, dim=1)  # [B, L]\n",
        "\n",
        "def _ids_to_strs_tgt(batch_ids, decode_fn):\n",
        "    # batch_ids: Tensor [B,L] or list[list[int]]\n",
        "    arr = batch_ids.cpu().tolist() if isinstance(batch_ids, torch.Tensor) else batch_ids\n",
        "    return [decode_fn(ids) for ids in arr]\n",
        "\n",
        "def eval_on_loader(model, dl, decode_fn, mode):\n",
        "    model.eval()\n",
        "    total_loss, total_tokens = 0.0, 0\n",
        "    preds_text, refs_text = [], []\n",
        "    with torch.no_grad():\n",
        "        for batch in dl:\n",
        "            src = batch[\"src_ids\"].to(DEVICE)\n",
        "            tgt = batch[\"tgt_ids\"].to(DEVICE)\n",
        "\n",
        "            # teacher-forced loss\n",
        "            logits = model(src, tgt, teacher_forcing=1.0)\n",
        "            loss = sequence_nll(logits, tgt)\n",
        "\n",
        "            gold = tgt[:, 1:1+logits.size(1)]\n",
        "            ntok = (gold != PAD_ID).sum().item()\n",
        "            total_loss += loss.item() * ntok\n",
        "            total_tokens += ntok\n",
        "\n",
        "            # greedy decode for metrics\n",
        "            gen = greedy_decode(model, batch, max_len=tgt.size(1)-1)\n",
        "            preds_text.extend(_ids_to_strs_tgt(gen, decode_fn))\n",
        "            refs_text.extend(_ids_to_strs_tgt(tgt, decode_fn))\n",
        "\n",
        "    avg_nll = total_loss / max(total_tokens, 1)\n",
        "    ppl = math.exp(avg_nll)\n",
        "    bleu = sacrebleu.corpus_bleu(preds_text, [refs_text]).score\n",
        "    cer_scores = [jiwer_cer(r, p) for p, r in zip(preds_text, refs_text)]\n",
        "    cer_mean = float(np.mean(cer_scores)) if cer_scores else 1.0\n",
        "    return avg_nll, ppl, bleu, cer_mean, preds_text[:5], refs_text[:5]\n",
        "\n",
        "def train_model(exp_overrides=None):\n",
        "    # Merge overrides with default exp\n",
        "    cfg_local = dict(exp)\n",
        "    if exp_overrides: cfg_local.update(exp_overrides)\n",
        "\n",
        "    # Tokenizers & decode function\n",
        "    tok = _load_tokenizers(cfg_local[\"tokenization\"])\n",
        "    SRC_VSIZE, TGT_VSIZE = tok[\"SRC_V\"], tok[\"TGT_V\"]\n",
        "    decode_fn = tok[\"decode\"]\n",
        "\n",
        "    # Data\n",
        "    _, _, _, dl_train, dl_val, _ = get_loaders(cfg_local[\"tokenization\"], batch=cfg_local[\"batch_size\"])\n",
        "\n",
        "    # Model + opt\n",
        "    model = Seq2Seq(\n",
        "        src_vsize=SRC_VSIZE, tgt_vsize=TGT_VSIZE,\n",
        "        emb_dim=cfg_local[\"embedding_dim\"], hid_size=cfg_local[\"hidden_size\"],\n",
        "        enc_layers=cfg_local[\"enc_layers\"], dec_layers=cfg_local[\"dec_layers\"],\n",
        "        dropout=cfg_local[\"dropout\"]\n",
        "    ).to(DEVICE)\n",
        "    opt = torch.optim.Adam(model.parameters(), lr=cfg_local[\"learning_rate\"])\n",
        "\n",
        "    # Schedules\n",
        "    EPOCHS   = cfg_local[\"epochs\"]\n",
        "    TF_START = cfg_local[\"teacher_forcing_start\"]\n",
        "    TF_END   = cfg_local[\"teacher_forcing_end\"]\n",
        "    def tf_ratio(epoch_idx):\n",
        "        if EPOCHS <= 1: return TF_END\n",
        "        return TF_START + (TF_END - TF_START) * (epoch_idx / (EPOCHS-1))\n",
        "\n",
        "    best_bleu, best_path = -1.0, None\n",
        "\n",
        "    for ep in range(1, EPOCHS+1):\n",
        "        model.train()\n",
        "        t0 = time.time()\n",
        "        tf = tf_ratio(ep-1)\n",
        "        total_loss, total_tok = 0.0, 0\n",
        "\n",
        "        for batch in dl_train:\n",
        "            src = batch[\"src_ids\"].to(DEVICE)\n",
        "            tgt = batch[\"tgt_ids\"].to(DEVICE)\n",
        "            logits = model(src, tgt, teacher_forcing=tf)\n",
        "            loss = sequence_nll(logits, tgt)\n",
        "\n",
        "            opt.zero_grad(set_to_none=True)\n",
        "            loss.backward()\n",
        "            clip_grad_norm_(model.parameters(), cfg_local[\"grad_clip\"])\n",
        "            opt.step()\n",
        "\n",
        "            gold = tgt[:,1:1+logits.size(1)]\n",
        "            ntok = (gold != PAD_ID).sum().item()\n",
        "            total_loss += loss.item() * ntok\n",
        "            total_tok += ntok\n",
        "\n",
        "        train_nll = total_loss / max(total_tok,1)\n",
        "        train_ppl = math.exp(train_nll)\n",
        "\n",
        "        # Validate\n",
        "        val_nll, val_ppl, val_bleu, val_cer, samp_pred, samp_ref = eval_on_loader(model, dl_val, decode_fn, tok[\"mode\"])\n",
        "\n",
        "        dt = time.time()-t0\n",
        "        print(f\"[Ep {ep:02d}] tf={tf:.2f} | train ppl={train_ppl:.2f} | \"\n",
        "              f\"val ppl={val_ppl:.2f} | BLEU={val_bleu:.2f} | CER={val_cer:.3f} | {dt:.1f}s\")\n",
        "\n",
        "        # Save best by BLEU\n",
        "        if val_bleu > best_bleu:\n",
        "            best_bleu = val_bleu\n",
        "            label = f\"{tok['mode']}_E{cfg_local['embedding_dim']}_H{cfg_local['hidden_size']}_enc{cfg_local['enc_layers']}_dec{cfg_local['dec_layers']}_drop{cfg_local['dropout']}\"\n",
        "            best_path = MODELS_DIR / f\"bilstm4lstm_{label}_best.pt\"\n",
        "            torch.save({\n",
        "                \"model_state\": model.state_dict(),\n",
        "                \"exp\": cfg_local,\n",
        "                \"bleu\": float(best_bleu),\n",
        "                \"tokenization\": tok[\"mode\"]\n",
        "            }, best_path)\n",
        "            print(\"  ↳ Saved best:\", best_path)\n",
        "\n",
        "        # A couple of samples\n",
        "        for i in range(min(2, len(samp_pred))):\n",
        "            print(f\"  pred: {samp_pred[i]}\")\n",
        "            print(f\"  ref : {samp_ref[i]}\")\n",
        "\n",
        "    print(\"\\nBest BLEU:\", best_bleu, \" | ckpt:\", best_path)\n",
        "    return best_path\n",
        "\n",
        "print(\"✅ Cell #5 ready. Starting a quick run …\")\n",
        "\n",
        "# --- Kick a quick baseline (optional: reduce epochs for smoke test) ---\n",
        "# exp[\"epochs\"] = 3  # uncomment for a super-fast check\n",
        "best_ckpt = train_model()   # uses exp_default.json settings (char, E=256,H=256,…)\n",
        "print(\"Done.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nTGcSAPVbK51",
        "outputId": "e421fc84-53f5-4dd4-f24c-f5d3bb561452"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n",
            "✅ Cell #5 ready. Starting a quick run …\n",
            "[Ep 01] tf=1.00 | train ppl=14.27 | val ppl=4.59 | BLEU=0.56 | CER=0.665 | 49.7s\n",
            "  ↳ Saved best: /content/nmt_urdu_roman/models/bilstm4lstm_char_E256_H256_enc2_dec4_drop0.3_best.pt\n",
            "  pred: taaaah raaā ke ke mo to mai mhhī bahā hoñ hoñ ho hai\n",
            "  ref : taaza rifāqat ke mausam tak maiñ bhī jiyā huuñ vo bhī jiyā hai\n",
            "  pred: ham na sss ke shhrr ko hhhhā āo no no nank lanā haiā haiā ha\n",
            "  ref : ham ne us ke shahr ko chhoḌā aur āñkhoñ ko muuñd liyā hai\n",
            "[Ep 02] tf=0.97 | train ppl=3.49 | val ppl=2.02 | BLEU=10.89 | CER=0.426 | 49.0s\n",
            "  ↳ Saved best: /content/nmt_urdu_roman/models/bilstm4lstm_char_E256_H256_enc2_dec4_drop0.3_best.pt\n",
            "  pred: taāza rafāqt ke muusm tak meñ bhī jhī jayñ joñ voā joā joā j\n",
            "  ref : taaza rifāqat ke mausam tak maiñ bhī jiyā huuñ vo bhī jiyā hai\n",
            "  pred: ham ne as ke shhhr ko chhvā āo aankhoñ ko maundd layā laidd la\n",
            "  ref : ham ne us ke shahr ko chhoḌā aur āñkhoñ ko muuñd liyā hai\n",
            "[Ep 03] tf=0.95 | train ppl=2.08 | val ppl=1.60 | BLEU=24.62 | CER=0.341 | 47.8s\n",
            "  ↳ Saved best: /content/nmt_urdu_roman/models/bilstm4lstm_char_E256_H256_enc2_dec4_drop0.3_best.pt\n",
            "  pred: tāza rafāqat ke mausm tak meñ bhī jayā huuñ vo bhī haiā haiā\n",
            "  ref : taaza rifāqat ke mausam tak maiñ bhī jiyā huuñ vo bhī jiyā hai\n",
            "  pred: ham ne us ke shhhar ko chhūḌā aur āñkhoñ ko muuñ lyā haiā hai\n",
            "  ref : ham ne us ke shahr ko chhoḌā aur āñkhoñ ko muuñd liyā hai\n",
            "[Ep 04] tf=0.92 | train ppl=1.67 | val ppl=1.41 | BLEU=35.65 | CER=0.317 | 47.7s\n",
            "  ↳ Saved best: /content/nmt_urdu_roman/models/bilstm4lstm_char_E256_H256_enc2_dec4_drop0.3_best.pt\n",
            "  pred: tāza rafāqat ke mausm tak meñ bhī jayā huuñ vo bhī haiā baiī\n",
            "  ref : taaza rifāqat ke mausam tak maiñ bhī jiyā huuñ vo bhī jiyā hai\n",
            "  pred: ham ne us ke shahr ko chhoḌā aur āñkhoñ ko muiñd liyā haihand\n",
            "  ref : ham ne us ke shahr ko chhoḌā aur āñkhoñ ko muuñd liyā hai\n",
            "[Ep 05] tf=0.89 | train ppl=1.51 | val ppl=1.33 | BLEU=41.42 | CER=0.278 | 48.3s\n",
            "  ↳ Saved best: /content/nmt_urdu_roman/models/bilstm4lstm_char_E256_H256_enc2_dec4_drop0.3_best.pt\n",
            "  pred: tāza rafāqat ke mausam tak meñ bhī jyā huuñ vo bhī jayā jaiī h\n",
            "  ref : taaza rifāqat ke mausam tak maiñ bhī jiyā huuñ vo bhī jiyā hai\n",
            "  pred: ham ne us ke shahr ko chhoḌā aur āñkhoñ ko maund liyā haiand l\n",
            "  ref : ham ne us ke shahr ko chhoḌā aur āñkhoñ ko muuñd liyā hai\n",
            "[Ep 06] tf=0.87 | train ppl=1.43 | val ppl=1.30 | BLEU=45.61 | CER=0.292 | 47.2s\n",
            "  ↳ Saved best: /content/nmt_urdu_roman/models/bilstm4lstm_char_E256_H256_enc2_dec4_drop0.3_best.pt\n",
            "  pred: tāza rafāqat ke mausam tak meñ bhī juuñ voñ vahī jii jiyī hai\n",
            "  ref : taaza rifāqat ke mausam tak maiñ bhī jiyā huuñ vo bhī jiyā hai\n",
            "  pred: ham ne us ke shahr ko chhoḌā aur āñkhoñ ko maiñd liyā haiaia\n",
            "  ref : ham ne us ke shahr ko chhoḌā aur āñkhoñ ko muuñd liyā hai\n",
            "[Ep 07] tf=0.84 | train ppl=1.39 | val ppl=1.25 | BLEU=49.73 | CER=0.300 | 49.4s\n",
            "  ↳ Saved best: /content/nmt_urdu_roman/models/bilstm4lstm_char_E256_H256_enc2_dec4_drop0.3_best.pt\n",
            "  pred: taaza rafāqat ke mausam tak meñ bhī juyā huuñ vo bhī jaijā hai\n",
            "  ref : taaza rifāqat ke mausam tak maiñ bhī jiyā huuñ vo bhī jiyā hai\n",
            "  pred: ham ne us ke shahr ko chhoḌā aur āñkhoñ ko mauñd liyā haihai\n",
            "  ref : ham ne us ke shahr ko chhoḌā aur āñkhoñ ko muuñd liyā hai\n",
            "[Ep 08] tf=0.82 | train ppl=1.36 | val ppl=1.23 | BLEU=50.53 | CER=0.302 | 48.0s\n",
            "  ↳ Saved best: /content/nmt_urdu_roman/models/bilstm4lstm_char_E256_H256_enc2_dec4_drop0.3_best.pt\n",
            "  pred: taaza rafāqat ke mausam tak meñ bhī jyā huuñ vo bhī jiyā haibh\n",
            "  ref : taaza rifāqat ke mausam tak maiñ bhī jiyā huuñ vo bhī jiyā hai\n",
            "  pred: ham ne us ke shahr ko chhoḌā aur āñkhoñ ko mauñd liyā haiai\n",
            "  ref : ham ne us ke shahr ko chhoḌā aur āñkhoñ ko muuñd liyā hai\n",
            "[Ep 09] tf=0.79 | train ppl=1.34 | val ppl=1.22 | BLEU=52.71 | CER=0.287 | 47.4s\n",
            "  ↳ Saved best: /content/nmt_urdu_roman/models/bilstm4lstm_char_E256_H256_enc2_dec4_drop0.3_best.pt\n",
            "  pred: taaza rafāqat ke mausam tak maiñ bhī jiyā huuñ vo bhī jiyā hai\n",
            "  ref : taaza rifāqat ke mausam tak maiñ bhī jiyā huuñ vo bhī jiyā hai\n",
            "  pred: ham ne us ke shahr ko chhoḌā aur āñkhoñ ko mauñd liyā haimand\n",
            "  ref : ham ne us ke shahr ko chhoḌā aur āñkhoñ ko muuñd liyā hai\n",
            "[Ep 10] tf=0.76 | train ppl=1.34 | val ppl=1.20 | BLEU=55.46 | CER=0.245 | 47.9s\n",
            "  ↳ Saved best: /content/nmt_urdu_roman/models/bilstm4lstm_char_E256_H256_enc2_dec4_drop0.3_best.pt\n",
            "  pred: taaza rafāqat ke mausam tak meñ bhī juyā huuñ vo bhī jaiehā j\n",
            "  ref : taaza rifāqat ke mausam tak maiñ bhī jiyā huuñ vo bhī jiyā hai\n",
            "  pred: ham ne us ke shahr ko chhoḌā aur āñkhoñ ko mauñd liyā haiai\n",
            "  ref : ham ne us ke shahr ko chhoḌā aur āñkhoñ ko muuñd liyā hai\n",
            "[Ep 11] tf=0.74 | train ppl=1.32 | val ppl=1.20 | BLEU=56.59 | CER=0.221 | 48.8s\n",
            "  ↳ Saved best: /content/nmt_urdu_roman/models/bilstm4lstm_char_E256_H256_enc2_dec4_drop0.3_best.pt\n",
            "  pred: taaza rafāqat ke mausam tak meñ bhī jyā huuñ voñ vo bhī haiai\n",
            "  ref : taaza rifāqat ke mausam tak maiñ bhī jiyā huuñ vo bhī jiyā hai\n",
            "  pred: ham ne us ke shahr ko chhoḌā aur āñkhoñ ko mauñd liyā haiañd\n",
            "  ref : ham ne us ke shahr ko chhoḌā aur āñkhoñ ko muuñd liyā hai\n",
            "[Ep 12] tf=0.71 | train ppl=1.30 | val ppl=1.18 | BLEU=62.21 | CER=0.134 | 46.9s\n",
            "  ↳ Saved best: /content/nmt_urdu_roman/models/bilstm4lstm_char_E256_H256_enc2_dec4_drop0.3_best.pt\n",
            "  pred: taaza rafāqat ke mausam tak meñ bhī jyī jiyā huuñ vo bhī jayā h\n",
            "  ref : taaza rifāqat ke mausam tak maiñ bhī jiyā huuñ vo bhī jiyā hai\n",
            "  pred: ham ne us ke shahr ko chhoḌā aur āñkhoñ ko mauñd liyā haiand\n",
            "  ref : ham ne us ke shahr ko chhoḌā aur āñkhoñ ko muuñd liyā hai\n",
            "[Ep 13] tf=0.68 | train ppl=1.31 | val ppl=1.18 | BLEU=56.42 | CER=0.212 | 47.9s\n",
            "  pred: taaza rifāqat ke mausam tak maiñ bhī jiyā huuñ vo bhī haihī h\n",
            "  ref : taaza rifāqat ke mausam tak maiñ bhī jiyā huuñ vo bhī jiyā hai\n",
            "  pred: ham ne us ke shahr ko chhoḌā aur āñkhoñ ko mauñd liyā haiand\n",
            "  ref : ham ne us ke shahr ko chhoḌā aur āñkhoñ ko muuñd liyā hai\n",
            "[Ep 14] tf=0.66 | train ppl=1.31 | val ppl=1.18 | BLEU=59.74 | CER=0.189 | 47.8s\n",
            "  pred: taaza rafāqat ke mausam tak maiñ bhī jiyā huuñ vo bhī jayā hai\n",
            "  ref : taaza rifāqat ke mausam tak maiñ bhī jiyā huuñ vo bhī jiyā hai\n",
            "  pred: ham ne us ke shahr ko chhoḌā aur āñkhoñ ko hainand liyā haia\n",
            "  ref : ham ne us ke shahr ko chhoḌā aur āñkhoñ ko muuñd liyā hai\n",
            "[Ep 15] tf=0.63 | train ppl=1.30 | val ppl=1.17 | BLEU=59.76 | CER=0.195 | 46.8s\n",
            "  pred: taaza rifāqat ke mausam tak maiñ bhī jiyā huuñ vo bhī jiyā hai\n",
            "  ref : taaza rifāqat ke mausam tak maiñ bhī jiyā huuñ vo bhī jiyā hai\n",
            "  pred: ham ne us ke shahr ko chhoḌā aur āñkhoñ ko mauñd liyā haiad\n",
            "  ref : ham ne us ke shahr ko chhoḌā aur āñkhoñ ko muuñd liyā hai\n",
            "[Ep 16] tf=0.61 | train ppl=1.29 | val ppl=1.18 | BLEU=61.10 | CER=0.174 | 47.6s\n",
            "  pred: taaza rifāqat ke mausam tak maiñ bhī juyā huuñ vo bhī jiyā hai\n",
            "  ref : taaza rifāqat ke mausam tak maiñ bhī jiyā huuñ vo bhī jiyā hai\n",
            "  pred: ham ne us ke shahr ko chhoḌā aur āñkhoñ ko mauñd liyā haiañd\n",
            "  ref : ham ne us ke shahr ko chhoḌā aur āñkhoñ ko muuñd liyā hai\n",
            "[Ep 17] tf=0.58 | train ppl=1.31 | val ppl=1.16 | BLEU=61.81 | CER=0.163 | 48.4s\n",
            "  pred: taaza rifāqat ke mausam tak meñ bhī jiyā huuñ vo bhī jiyā hai\n",
            "  ref : taaza rifāqat ke mausam tak maiñ bhī jiyā huuñ vo bhī jiyā hai\n",
            "  pred: ham ne us ke shahr ko chhoḌā aur āñkhoñ ko mauñd liyā haiañd\n",
            "  ref : ham ne us ke shahr ko chhoḌā aur āñkhoñ ko muuñd liyā hai\n",
            "[Ep 18] tf=0.55 | train ppl=1.30 | val ppl=1.16 | BLEU=62.38 | CER=0.197 | 48.5s\n",
            "  ↳ Saved best: /content/nmt_urdu_roman/models/bilstm4lstm_char_E256_H256_enc2_dec4_drop0.3_best.pt\n",
            "  pred: taaza rifāqat ke mausam tak meñ bhī jiyā huuñ vo bhī jiyā haiv\n",
            "  ref : taaza rifāqat ke mausam tak maiñ bhī jiyā huuñ vo bhī jiyā hai\n",
            "  pred: ham ne us ke shahr ko chhoḌā aur āñkhoñ ko muuñd liyā haiauñd\n",
            "  ref : ham ne us ke shahr ko chhoḌā aur āñkhoñ ko muuñd liyā hai\n",
            "[Ep 19] tf=0.53 | train ppl=1.30 | val ppl=1.16 | BLEU=62.04 | CER=0.221 | 47.9s\n",
            "  pred: taaza rifāqat ke mausam tak meñ bhī huyā huuñ vo bhī jiyā haib\n",
            "  ref : taaza rifāqat ke mausam tak maiñ bhī jiyā huuñ vo bhī jiyā hai\n",
            "  pred: ham ne us ke shahr ko chhoḌā aur āñkhoñ ko mauñd liyā haiaid\n",
            "  ref : ham ne us ke shahr ko chhoḌā aur āñkhoñ ko muuñd liyā hai\n",
            "[Ep 20] tf=0.50 | train ppl=1.30 | val ppl=1.16 | BLEU=63.04 | CER=0.199 | 47.1s\n",
            "  ↳ Saved best: /content/nmt_urdu_roman/models/bilstm4lstm_char_E256_H256_enc2_dec4_drop0.3_best.pt\n",
            "  pred: taaza rifāqat ke mausam tak maiñ bhī juyā huuñ vo bhī jyā haib\n",
            "  ref : taaza rifāqat ke mausam tak maiñ bhī jiyā huuñ vo bhī jiyā hai\n",
            "  pred: ham ne us ke shahr ko chhoḌā aur āñkhoñ ko hainand liyā haia\n",
            "  ref : ham ne us ke shahr ko chhoḌā aur āñkhoñ ko muuñd liyā hai\n",
            "\n",
            "Best BLEU: 63.03771018975481  | ckpt: /content/nmt_urdu_roman/models/bilstm4lstm_char_E256_H256_enc2_dec4_drop0.3_best.pt\n",
            "Done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# Cell 5.1 — EOS-safe greedy decoding\n",
        "#   • Stops per sequence on EOS\n",
        "#   • Returns sequences trimmed to EOS\n",
        "#   • Optional simple no-repeat-3-gram guard\n",
        "# ==========================================\n",
        "import torch\n",
        "\n",
        "PAD_ID = 0; BOS_ID = 1; EOS_ID = 2\n",
        "\n",
        "def _has_repeat_ngram(ids, n=3):\n",
        "    if len(ids) < 2*n:\n",
        "        return False\n",
        "    last = tuple(ids[-n:])\n",
        "    for i in range(len(ids)-n*2+1):\n",
        "        if tuple(ids[i:i+n]) == last:\n",
        "            return True\n",
        "    return False\n",
        "\n",
        "def greedy_decode(model, batch, max_len=200, no_repeat_ngram=0):\n",
        "    \"\"\"\n",
        "    Returns a LongTensor [B, L] where each row is terminated by EOS (if produced)\n",
        "    and padded with EOS to equal length L.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        src = batch[\"src_ids\"].to(next(model.parameters()).device)\n",
        "        enc_outputs, (hn, cn) = model.encoder(src)\n",
        "        enc_mask = (src == PAD_ID)\n",
        "        dec_hc = model.bridge(hn, cn)\n",
        "        ctx = torch.zeros(src.size(0), enc_outputs.size(-1), device=src.device)\n",
        "        y_prev = torch.full((src.size(0),), BOS_ID, dtype=torch.long, device=src.device)\n",
        "\n",
        "        B = src.size(0)\n",
        "        finished = torch.zeros(B, dtype=torch.bool, device=src.device)\n",
        "        outputs = [[] for _ in range(B)]\n",
        "\n",
        "        for _ in range(max_len):\n",
        "            logits, dec_hc, ctx, _ = model.decoder(y_prev, dec_hc, enc_outputs, enc_mask, ctx)\n",
        "\n",
        "            # simple no-repeat-ngram guard (char/subword)\n",
        "            if no_repeat_ngram and no_repeat_ngram > 1:\n",
        "                for b in range(B):\n",
        "                    if not finished[b] and len(outputs[b]) >= no_repeat_ngram-1:\n",
        "                        # block last (n-1) history repeating as a new n-gram\n",
        "                        # (cheap heuristic: downweight the top-1 if it causes repeat)\n",
        "                        top1 = torch.argmax(logits[b])\n",
        "                        if _has_repeat_ngram(outputs[b] + [int(top1)], n=no_repeat_ngram):\n",
        "                            # pick 2nd best instead\n",
        "                            top2 = torch.topk(logits[b], k=2).indices[1]\n",
        "                            logits[b, top1] = -1e9\n",
        "                            logits[b, top2] += 1e-3  # tiny nudge\n",
        "\n",
        "            y_prev = torch.argmax(logits, dim=-1)\n",
        "\n",
        "            for b in range(B):\n",
        "                if finished[b]:\n",
        "                    continue\n",
        "                token = int(y_prev[b])\n",
        "                outputs[b].append(token)\n",
        "                if token == EOS_ID:\n",
        "                    finished[b] = True\n",
        "\n",
        "            if finished.all():\n",
        "                break\n",
        "\n",
        "        # pad with EOS to rectangular tensor\n",
        "        max_out = max(len(x) for x in outputs) if outputs else 1\n",
        "        out_tensor = torch.full((B, max_out), EOS_ID, dtype=torch.long, device=src.device)\n",
        "        for b, seq in enumerate(outputs):\n",
        "            if len(seq):\n",
        "                out_tensor[b, :len(seq)] = torch.tensor(seq, dtype=torch.long, device=src.device)\n",
        "        return out_tensor\n"
      ],
      "metadata": {
        "id": "hVMIX18dtHY4"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Colab Cell #6 — Load BEST ckpt → Test metrics + translate()\n",
        "#   (standalone; assumes Cells 3–5 are already run)\n",
        "# ============================================================\n",
        "import os, json, math, random\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch, sentencepiece as spm\n",
        "import sacrebleu\n",
        "from jiwer import cer as jiwer_cer\n",
        "\n",
        "# ---- paths / device\n",
        "cfg = json.load(open(\"/content/nmt_urdu_roman/project_config.json\",\"r\",encoding=\"utf-8\"))\n",
        "PROJECT_DIR = Path(cfg[\"project_dir\"])\n",
        "ARTI        = PROJECT_DIR / \"artifacts\"\n",
        "MODELS_DIR  = PROJECT_DIR / \"models\"\n",
        "DATA_DIR    = PROJECT_DIR / \"data\"\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def find_best_ckpt():\n",
        "    cands = sorted(MODELS_DIR.glob(\"bilstm4lstm_*_best.pt\"), key=os.path.getmtime, reverse=True)\n",
        "    assert cands, \"No *_best.pt found. Train first.\"\n",
        "    return cands[0]\n",
        "\n",
        "best_path = find_best_ckpt()\n",
        "ckpt = torch.load(best_path, map_location=DEVICE)\n",
        "exp_used = ckpt[\"exp\"]\n",
        "print(\"Loaded:\", best_path.name, \"| tokenization:\", ckpt.get(\"tokenization\"), \"| BLEU(val):\", ckpt.get(\"bleu\"))\n",
        "\n",
        "# ---- Seq2Seq & greedy_decode must exist from Cell #5\n",
        "assert \"Seq2Seq\" in globals() and \"greedy_decode\" in globals(), \"Please re-run Cell #5.\"\n",
        "\n",
        "# ---- vocab sizes + decode\n",
        "PAD_ID = 0; BOS_ID = 1; EOS_ID = 2; UNK_ID = 3\n",
        "\n",
        "if exp_used[\"tokenization\"] == \"char\":\n",
        "    v_src = json.load(open(ARTI / \"vocab_src_char.json\",\"r\",encoding=\"utf-8\"))\n",
        "    v_tgt = json.load(open(ARTI / \"vocab_tgt_char.json\",\"r\",encoding=\"utf-8\"))\n",
        "    SRC_V = len(v_src[\"itos\"]); TGT_V = len(v_tgt[\"itos\"])\n",
        "    itos_tgt = v_tgt[\"itos\"]\n",
        "    def decode_fn(ids):\n",
        "        if isinstance(ids, torch.Tensor): ids = ids.tolist()\n",
        "        return \"\".join(itos_tgt[i] for i in ids if i not in (PAD_ID,BOS_ID,EOS_ID)).strip()\n",
        "else:\n",
        "    sp_src = spm.SentencePieceProcessor(); sp_src.load(str(ARTI / \"spm_src.model\"))\n",
        "    sp_tgt = spm.SentencePieceProcessor(); sp_tgt.load(str(ARTI / \"spm_tgt.model\"))\n",
        "    SRC_V = sp_src.get_piece_size(); TGT_V = sp_tgt.get_piece_size()\n",
        "    def decode_fn(ids):\n",
        "        if isinstance(ids, torch.Tensor): ids = ids.tolist()\n",
        "        ids = [i for i in ids if i not in (PAD_ID,BOS_ID,EOS_ID)]\n",
        "        try: return sp_tgt.decode(ids).strip()\n",
        "        except: return sp_tgt.decode_pieces([sp_tgt.id_to_piece(i) for i in ids]).strip()\n",
        "\n",
        "# ---- rebuild model AFTER we know SRC_V/TGT_V\n",
        "model = Seq2Seq(SRC_V, TGT_V,\n",
        "                exp_used[\"embedding_dim\"], exp_used[\"hidden_size\"],\n",
        "                exp_used[\"enc_layers\"],   exp_used[\"dec_layers\"],\n",
        "                exp_used[\"dropout\"]).to(DEVICE)\n",
        "model.load_state_dict(ckpt[\"model_state\"])\n",
        "model.eval()\n",
        "\n",
        "# ---- test loader via make_loaders from Cell #4\n",
        "assert \"make_loaders\" in globals(), \"Please re-run Cell #4.\"\n",
        "_, _, _, _, _, dl_test = make_loaders(mode=exp_used[\"tokenization\"], batch_size=exp_used[\"batch_size\"])\n",
        "\n",
        "def eval_on_loader(model, dl):\n",
        "    preds_text, refs_text = [], []\n",
        "    total_loss, total_tokens = 0.0, 0\n",
        "    with torch.no_grad():\n",
        "        for batch in dl:\n",
        "            src = batch[\"src_ids\"].to(DEVICE)\n",
        "            tgt = batch[\"tgt_ids\"].to(DEVICE)\n",
        "\n",
        "            # loss (teacher-forced)\n",
        "            logits = model(src, tgt, teacher_forcing=1.0)\n",
        "            B, Tm1, V = logits.shape\n",
        "            gold = tgt[:,1:1+Tm1]\n",
        "            loss = torch.nn.functional.cross_entropy(\n",
        "                logits.reshape(B*Tm1, V),\n",
        "                gold.reshape(B*Tm1),\n",
        "                ignore_index=PAD_ID\n",
        "            )\n",
        "            ntok = (gold != PAD_ID).sum().item()\n",
        "            total_loss += loss.item() * ntok\n",
        "            total_tokens += ntok\n",
        "\n",
        "            # greedy decode\n",
        "            gen = greedy_decode(model, batch, max_len=tgt.size(1)-1)\n",
        "            preds_text.extend([decode_fn(row) for row in gen])\n",
        "            refs_text.extend([decode_fn(row) for row in tgt])\n",
        "\n",
        "    avg_nll = total_loss / max(total_tokens,1)\n",
        "    ppl = math.exp(avg_nll)\n",
        "    bleu = sacrebleu.corpus_bleu(preds_text, [refs_text]).score\n",
        "    cer_scores = [jiwer_cer(r, p) for p, r in zip(preds_text, refs_text)]\n",
        "    cer_mean = float(np.mean(cer_scores)) if cer_scores else 1.0\n",
        "    return ppl, bleu, cer_mean\n",
        "\n",
        "test_ppl, test_bleu, test_cer = eval_on_loader(model, dl_test)\n",
        "print(f\"\\n=== TEST ===\\nPPL: {test_ppl:.3f} | BLEU: {test_bleu:.2f} | CER: {test_cer:.3f}\")\n",
        "\n",
        "# ---- simple translator\n",
        "def translate(texts, max_len=200):\n",
        "    batch = {\"src_ids\": [], \"tgt_ids\": []}\n",
        "    if exp_used[\"tokenization\"] == \"char\":\n",
        "        stoi = json.load(open(ARTI / \"vocab_src_char.json\",\"r\",encoding=\"utf-8\"))[\"stoi\"]\n",
        "        for t in texts:\n",
        "            ids = [BOS_ID] + [stoi.get(ch, UNK_ID) for ch in t] + [EOS_ID]\n",
        "            batch[\"src_ids\"].append(torch.tensor(ids, dtype=torch.long))\n",
        "            batch[\"tgt_ids\"].append(torch.tensor([BOS_ID, EOS_ID], dtype=torch.long))\n",
        "    else:\n",
        "        for t in texts:\n",
        "            ids = [BOS_ID] + sp_src.encode(t, out_type=int) + [EOS_ID]\n",
        "            batch[\"src_ids\"].append(torch.tensor(ids, dtype=torch.long))\n",
        "            batch[\"tgt_ids\"].append(torch.tensor([BOS_ID, EOS_ID], dtype=torch.long))\n",
        "\n",
        "    def pad(lst):\n",
        "        m = max(len(x) for x in lst)\n",
        "        out = torch.full((len(lst), m), PAD_ID, dtype=torch.long)\n",
        "        for i,x in enumerate(lst): out[i,:len(x)] = x\n",
        "        return out\n",
        "\n",
        "    batch = {k: pad(v).to(DEVICE) for k,v in batch.items()}\n",
        "    gen = greedy_decode(model, batch, max_len=max_len)\n",
        "    return [decode_fn(row) for row in gen]\n",
        "\n",
        "# demo on a few test lines\n",
        "test_df = pd.read_parquet(DATA_DIR / \"pairs_test.parquet\")\n",
        "samps = test_df.sample(5, random_state=7)[[\"src_ur\",\"tgt_rom\"]].to_dict(\"records\")\n",
        "preds = translate([s[\"src_ur\"] for s in samps], max_len=120)  # shorter max_len\n",
        "\n",
        "\n",
        "print(\"\\n=== Qualitative (5 random from TEST) ===\")\n",
        "for i, s in enumerate(samps):\n",
        "    print(f\"{i+1:02d}. UR : {s['src_ur']}\")\n",
        "    print(f\"    GT : {s['tgt_rom']}\")\n",
        "    print(f\"    PR : {preds[i]}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PM_tvh7YbT29",
        "outputId": "77aa34d5-7fb6-4e93-8ad6-8af7c943e210"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded: bilstm4lstm_char_E256_H256_enc2_dec4_drop0.3_best.pt | tokenization: char | BLEU(val): 63.03771018975481\n",
            "\n",
            "=== TEST ===\n",
            "PPL: 1.137 | BLEU: 75.15 | CER: 0.049\n",
            "\n",
            "=== Qualitative (5 random from TEST) ===\n",
            "01. UR : ہو رہے گا کچہ نہ کچہ گہبراییں کیا\n",
            "    GT : ho rahegā kuchh na kuchh ghabrā.eñ kyā\n",
            "    PR : ho rahegā kuchh na kuchh ghabrā.eñ kyā\n",
            "02. UR : لگ گیی چپ حالیؔ رنجور کو\n",
            "    GT : lag ga.ī chup 'hālī'-e-ranjūr ko\n",
            "    PR : lag ga.ī chup 'hālī' ranjūr ko\n",
            "03. UR : مری طرح بہی کویی میرا غم گسار نہ ہو\n",
            "    GT : mirī tarah bhī koī merā ġham-gusār na ho\n",
            "    PR : mirī tarah bhī koī merā ġham-gusār na ho\n",
            "04. UR : سورج دماغ لوگ بہی ابلاغ فکر میں\n",
            "    GT : sūraj-dimāġh log bhī ablāġh-e-fikr meñ\n",
            "    PR : sūraj-e-dimāġh log bhī iblāġh-e-fikr meñ\n",
            "05. UR : مویے شیشہ دیدۂ ساغر کی مژگانی کرے\n",
            "    GT : mū-e-shīsha dīda-e-sāġhar kī mizhgānī kare\n",
            "    PR : mū-e-shīsha dīda-e-sāġhar kī mizhgānī kare\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Mini experiment sweep (robust to eval_on_loader signature)\n",
        "#   - Runs: char_H512, char_LR1e-3, spm_baseline (if SPM available)\n",
        "#   - Saves CSV + BLEU bar plot\n",
        "# ============================================================\n",
        "import os, json, time, math, numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "import torch, inspect\n",
        "\n",
        "# --- Expect Cell #4 and #5 already run\n",
        "assert 'train_model' in globals(), \"train_model not found. Run Cell #5.\"\n",
        "assert 'Seq2Seq' in globals(), \"Seq2Seq not found. Run Cell #5.\"\n",
        "assert 'DEVICE' in globals(), \"DEVICE not found. Run Cell #5.\"\n",
        "assert 'ARTI' in globals(), \"ARTI not found. Run Cell #5.\"\n",
        "assert 'get_loaders' in globals(), \"get_loaders not found. Run Cell #5.\"\n",
        "\n",
        "# ---- Shim eval_on_loader to support BOTH signatures\n",
        "assert 'eval_on_loader' in globals(), \"eval_on_loader not found. Run Cell #5.\"\n",
        "_orig_eval_on_loader = eval_on_loader  # keep original\n",
        "\n",
        "def eval_on_loader(*args, **kwargs):\n",
        "    \"\"\"\n",
        "    Wrapper that supports:\n",
        "      - old: eval_on_loader(model, dl)\n",
        "      - new: eval_on_loader(model, dl, decode_fn, mode)\n",
        "    \"\"\"\n",
        "    sig = inspect.signature(_orig_eval_on_loader)\n",
        "    if len(sig.parameters) == 2:\n",
        "        # old signature — just hand it model, dl\n",
        "        model, dl = args[:2]\n",
        "        return _orig_eval_on_loader(model, dl)\n",
        "    # new signature — pass through\n",
        "    return _orig_eval_on_loader(*args, **kwargs)\n",
        "\n",
        "# ---- Paths / base config\n",
        "cfg = json.load(open(\"/content/nmt_urdu_roman/project_config.json\",\"r\",encoding=\"utf-8\"))\n",
        "PROJECT_DIR = Path(cfg[\"project_dir\"])\n",
        "RUNS_DIR = PROJECT_DIR / \"runs\"; RUNS_DIR.mkdir(parents=True, exist_ok=True)\n",
        "MODELS_DIR = PROJECT_DIR / \"models\"\n",
        "\n",
        "base = json.load(open(ARTI / \"exp_default.json\",\"r\",encoding=\"utf-8\"))\n",
        "BASE_EPOCHS = 6\n",
        "BASE_BATCH  = base.get(\"batch_size\", 64)\n",
        "\n",
        "# SPM availability\n",
        "SPM_OK = (ARTI/'spm_src.model').exists() and (ARTI/'spm_tgt.model').exists()\n",
        "if not SPM_OK:\n",
        "    print(\"⚠️ SPM model files not found. The SPM config will be skipped.\")\n",
        "\n",
        "# ---- Sweep definitions\n",
        "sweep = [\n",
        "    {\"name\": \"char_H512\",   \"overrides\": {\"tokenization\": \"char\", \"hidden_size\": 512, \"epochs\": BASE_EPOCHS, \"batch_size\": BASE_BATCH}},\n",
        "    {\"name\": \"char_LR1e-3\", \"overrides\": {\"tokenization\": \"char\", \"learning_rate\": 1e-3, \"epochs\": BASE_EPOCHS, \"batch_size\": BASE_BATCH}},\n",
        "]\n",
        "if SPM_OK:\n",
        "    sweep.append({\"name\": \"spm_baseline\", \"overrides\": {\"tokenization\": \"spm\", \"epochs\": BASE_EPOCHS, \"batch_size\": BASE_BATCH}})\n",
        "\n",
        "# ---- Helper to get decode function only when needed\n",
        "def _decode_fn_for(tokenization):\n",
        "    if tokenization == \"char\":\n",
        "        v_tgt = json.load(open(ARTI / \"vocab_tgt_char.json\",\"r\",encoding=\"utf-8\"))\n",
        "        itos_tgt = v_tgt[\"itos\"]\n",
        "        PAD_ID, BOS_ID, EOS_ID = 0,1,2\n",
        "        def decode_char(ids):\n",
        "            return \"\".join(itos_tgt[i] for i in ids if i not in (PAD_ID,BOS_ID,EOS_ID) and 0<=i<len(itos_tgt)).strip()\n",
        "        return decode_char\n",
        "    else:\n",
        "        import sentencepiece as spm\n",
        "        PAD_ID, BOS_ID, EOS_ID = 0,1,2\n",
        "        sp_tgt = spm.SentencePieceProcessor(); sp_tgt.load(str(ARTI / \"spm_tgt.model\"))\n",
        "        def decode_spm(ids):\n",
        "            ids = [i for i in ids if i not in (PAD_ID,BOS_ID,EOS_ID)]\n",
        "            try: return sp_tgt.decode(ids).strip()\n",
        "            except: return sp_tgt.decode_pieces([sp_tgt.id_to_piece(i) for i in ids]).strip()\n",
        "        return decode_spm\n",
        "\n",
        "results = []\n",
        "\n",
        "for job in sweep:\n",
        "    name = job[\"name\"]\n",
        "    overrides = dict(base); overrides.update(job[\"overrides\"])\n",
        "    print(f\"\\n=== Running: {name} ===\")\n",
        "    t0 = time.time()\n",
        "    try:\n",
        "        ckpt_path = train_model(exp_overrides=overrides)\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Training failed for {name}: {e}\")\n",
        "        continue\n",
        "    dt = time.time() - t0\n",
        "\n",
        "    if not ckpt_path or not Path(ckpt_path).exists():\n",
        "        print(f\"❌ No checkpoint produced for {name}\")\n",
        "        continue\n",
        "\n",
        "    meta = torch.load(ckpt_path, map_location=\"cpu\")\n",
        "    best_bleu = float(meta.get(\"bleu\", float('nan')))\n",
        "    exp_used  = meta.get(\"exp\", overrides)\n",
        "    tokenization = exp_used[\"tokenization\"]\n",
        "\n",
        "    # Rebuild model to recompute val ppl/CER\n",
        "    if tokenization == \"char\":\n",
        "        v_src = json.load(open(ARTI / \"vocab_src_char.json\",\"r\",encoding=\"utf-8\"))\n",
        "        v_tgt = json.load(open(ARTI / \"vocab_tgt_char.json\",\"r\",encoding=\"utf-8\"))\n",
        "        src_v = len(v_src[\"itos\"]); tgt_v = len(v_tgt[\"itos\"])\n",
        "    else:\n",
        "        import sentencepiece as spm\n",
        "        sp_src = spm.SentencePieceProcessor(); sp_src.load(str(ARTI / \"spm_src.model\"))\n",
        "        sp_tgt = spm.SentencePieceProcessor(); sp_tgt.load(str(ARTI / \"spm_tgt.model\"))\n",
        "        src_v = sp_src.get_piece_size(); tgt_v = sp_tgt.get_piece_size()\n",
        "\n",
        "    model = Seq2Seq(\n",
        "        src_vsize=src_v, tgt_vsize=tgt_v,\n",
        "        emb_dim=exp_used[\"embedding_dim\"], hid_size=exp_used[\"hidden_size\"],\n",
        "        enc_layers=exp_used[\"enc_layers\"], dec_layers=exp_used[\"dec_layers\"],\n",
        "        dropout=exp_used[\"dropout\"]\n",
        "    ).to(DEVICE)\n",
        "    state = torch.load(ckpt_path, map_location=DEVICE)[\"model_state\"]\n",
        "    model.load_state_dict(state)\n",
        "\n",
        "    # loaders + eval on VAL (signature-agnostic)\n",
        "    _, _, _, dl_train_tmp, dl_val_tmp, _ = get_loaders(tokenization, batch=exp_used[\"batch_size\"])\n",
        "    decode_fn = _decode_fn_for(tokenization)\n",
        "    try:\n",
        "        val_nll, val_ppl, val_bleu2, val_cer, _, _ = eval_on_loader(model, dl_val_tmp, decode_fn, tokenization)\n",
        "    except TypeError:\n",
        "        val_nll, val_ppl, val_bleu2, val_cer, _, _ = eval_on_loader(model, dl_val_tmp)\n",
        "\n",
        "    results.append({\n",
        "        \"name\": name,\n",
        "        \"tokenization\": tokenization,\n",
        "        \"embedding_dim\": exp_used[\"embedding_dim\"],\n",
        "        \"hidden_size\": exp_used[\"hidden_size\"],\n",
        "        \"learning_rate\": exp_used[\"learning_rate\"],\n",
        "        \"epochs\": exp_used[\"epochs\"],\n",
        "        \"batch_size\": exp_used[\"batch_size\"],\n",
        "        \"best_bleu_val_ckpt\": float(best_bleu),\n",
        "        \"val_ppl_reval\": float(val_ppl),\n",
        "        \"val_cer_reval\": float(val_cer),\n",
        "        \"train_time_s\": round(dt, 1),\n",
        "        \"ckpt_path\": str(ckpt_path),\n",
        "    })\n",
        "\n",
        "# ---- Save/show table safely\n",
        "if results:\n",
        "    df = pd.DataFrame(results).sort_values(\"best_bleu_val_ckpt\", ascending=False)\n",
        "    out_csv = RUNS_DIR / \"exp_results_sweep.csv\"\n",
        "    df.to_csv(out_csv, index=False)\n",
        "    print(\"\\n✅ Saved sweep results:\", out_csv)\n",
        "    display(df)\n",
        "\n",
        "    # BLEU bar chart\n",
        "    plt.figure(figsize=(8,4))\n",
        "    x = np.arange(len(df))\n",
        "    plt.bar(x, df[\"best_bleu_val_ckpt\"])\n",
        "    plt.xticks(x, df[\"name\"], rotation=15, ha='right')\n",
        "    plt.ylabel(\"Best Val BLEU (ckpt meta)\")\n",
        "    plt.title(\"Mini Sweep — Best Validation BLEU\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"\\n⚠️ No successful runs to summarize (results list is empty).\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oo9Kdd3e0rtB",
        "outputId": "2108b107-09df-4e15-f361-5c1e322344ff"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Running: char_H512 ===\n",
            "❌ Training failed for char_H512: not enough values to unpack (expected 6, got 3)\n",
            "\n",
            "=== Running: char_LR1e-3 ===\n",
            "❌ Training failed for char_LR1e-3: not enough values to unpack (expected 6, got 3)\n",
            "\n",
            "=== Running: spm_baseline ===\n",
            "❌ Training failed for spm_baseline: list index out of range\n",
            "\n",
            "⚠️ No successful runs to summarize (results list is empty).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save everything you need to /MyDrive\n",
        "from pathlib import Path\n",
        "import shutil, json, torch\n",
        "\n",
        "ROOT = Path(\"/content\")\n",
        "PROJ = ROOT / \"nmt_urdu_roman\"\n",
        "GDR  = Path(\"/content/drive/MyDrive/nmt_urdu_roman_export\")\n",
        "GDR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "for p in [\"models\",\"artifacts\",\"exp_results.csv\"]:\n",
        "    src = (PROJ / p) if p.endswith(\".csv\") else (PROJ / \"runs\" / p if (PROJ/\"runs\"/p).exists() else PROJ / p)\n",
        "    if src.exists():\n",
        "        dst = GDR / p\n",
        "        if src.is_dir():\n",
        "            shutil.copytree(src, dst, dirs_exist_ok=True)\n",
        "        else:\n",
        "            shutil.copy2(src, dst)\n",
        "\n",
        "# small manifest\n",
        "ckpt = \"/content/nmt_urdu_roman/models/bilstm4lstm_char_E256_H256_enc2_dec4_drop0.3_best.pt\"\n",
        "meta = torch.load(ckpt, map_location=\"cpu\")\n",
        "(Path(GDR/\"MANIFEST.json\")).write_text(json.dumps({\n",
        "    \"best_ckpt\": ckpt,\n",
        "    \"tokenization\": meta.get(\"tokenization\"),\n",
        "    \"bleu_val\": float(meta.get(\"bleu\", -1)),\n",
        "}, indent=2), encoding=\"utf-8\")\n",
        "\n",
        "print(\"✅ Exported to:\", GDR)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZJi5HUZFrEt0",
        "outputId": "69afc2e3-c5bc-47be-b00a-8ef3ebad945a"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Exported to: /content/drive/MyDrive/nmt_urdu_roman_export\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Colab Cell #7 — Reusable inference (greedy or beam), batch API\n",
        "# ============================================================\n",
        "import json, torch\n",
        "from pathlib import Path\n",
        "import sentencepiece as spm\n",
        "\n",
        "# Paths\n",
        "cfg = json.load(open(\"/content/nmt_urdu_roman/project_config.json\",\"r\",encoding=\"utf-8\"))\n",
        "PROJECT_DIR = Path(cfg[\"project_dir\"])\n",
        "ARTI        = PROJECT_DIR / \"artifacts\"\n",
        "MODELS_DIR  = PROJECT_DIR / \"models\"\n",
        "\n",
        "# Load best checkpoint (adjust if you saved multiple)\n",
        "ckpts = sorted(MODELS_DIR.glob(\"bilstm4lstm_*_best.pt\"))\n",
        "assert ckpts, \"No best checkpoints found in models/.\"\n",
        "CKPT_PATH = ckpts[-1]\n",
        "ckpt = torch.load(CKPT_PATH, map_location=\"cpu\")\n",
        "exp_used = ckpt[\"exp\"]\n",
        "tokenization = ckpt.get(\"tokenization\", exp_used.get(\"tokenization\", \"char\"))\n",
        "print(f\"Using ckpt: {CKPT_PATH.name} | tokenization={tokenization}\")\n",
        "\n",
        "# Bring model defs from Cell #5\n",
        "assert \"Seq2Seq\" in globals(), \"Please re-run Cell #5 first (model classes).\"\n",
        "\n",
        "# Vocab / tokenizers\n",
        "PAD_ID=0; BOS_ID=1; EOS_ID=2; UNK_ID=3\n",
        "if tokenization == \"char\":\n",
        "    v_src = json.load(open(ARTI / \"vocab_src_char.json\",\"r\",encoding=\"utf-8\"))\n",
        "    v_tgt = json.load(open(ARTI / \"vocab_tgt_char.json\",\"r\",encoding=\"utf-8\"))\n",
        "    stoi_src, itos_tgt = v_src[\"stoi\"], v_tgt[\"itos\"]\n",
        "    SRC_V, TGT_V = len(v_src[\"itos\"]), len(v_tgt[\"itos\"])\n",
        "    def enc_src(s):\n",
        "        return [BOS_ID] + [stoi_src.get(ch, UNK_ID) for ch in s] + [EOS_ID]\n",
        "    def dec_tgt(ids):\n",
        "        return \"\".join(itos_tgt[i] for i in ids if i not in (PAD_ID,BOS_ID,EOS_ID)).strip()\n",
        "else:\n",
        "    sp_src = spm.SentencePieceProcessor(); sp_src.load(str(ARTI/\"spm_src.model\"))\n",
        "    sp_tgt = spm.SentencePieceProcessor(); sp_tgt.load(str(ARTI/\"spm_tgt.model\"))\n",
        "    SRC_V, TGT_V = sp_src.get_piece_size(), sp_tgt.get_piece_size()\n",
        "    def enc_src(s):\n",
        "        return [BOS_ID] + sp_src.encode(s, out_type=int) + [EOS_ID]\n",
        "    def dec_tgt(ids):\n",
        "        ids = [i for i in ids if i not in (PAD_ID,BOS_ID,EOS_ID)]\n",
        "        try: return sp_tgt.decode(ids).strip()\n",
        "        except: return sp_tgt.decode_pieces([sp_tgt.id_to_piece(i) for i in ids]).strip()\n",
        "\n",
        "# Rebuild model & load weights\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = Seq2Seq(SRC_V, TGT_V,\n",
        "                exp_used[\"embedding_dim\"], exp_used[\"hidden_size\"],\n",
        "                exp_used[\"enc_layers\"], exp_used[\"dec_layers\"],\n",
        "                exp_used[\"dropout\"]).to(DEVICE)\n",
        "model.load_state_dict(ckpt[\"model_state\"])\n",
        "model.eval()\n",
        "\n",
        "# --- Greedy (EOS-safe) from Cell 5.1 (assume it's already redefined) ---\n",
        "assert \"greedy_decode\" in globals(), \"Please run the EOS-safe greedy from earlier.\"\n",
        "\n",
        "# --- Lightweight Beam Search (beam=5) ---\n",
        "import heapq\n",
        "def beam_search(model, src_ids, beam_size=5, max_len=200, length_penalty=0.7):\n",
        "    \"\"\"\n",
        "    src_ids: 1D LongTensor on DEVICE including BOS/EOS around source.\n",
        "    returns: list[int] best hypothesis token ids (no BOS, ends at EOS if emitted).\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        src = src_ids.unsqueeze(0)  # [1, Tsrc]\n",
        "        enc_outputs, (hn, cn) = model.encoder(src)\n",
        "        enc_mask = (src == PAD_ID)\n",
        "        dec_hc = model.bridge(hn, cn)\n",
        "        ctx = torch.zeros(1, enc_outputs.size(-1), device=src.device)\n",
        "        # beams: list of tuples (-score, seq, dec_hc, ctx, last_token)\n",
        "        start = (0.0, [], dec_hc, ctx, torch.tensor([BOS_ID], device=src.device))\n",
        "        beams = [start]\n",
        "        finished = []\n",
        "\n",
        "        for _ in range(max_len):\n",
        "            new_beams = []\n",
        "            for score, seq, dec_hc, ctx, y_prev in beams:\n",
        "                if len(seq) and seq[-1] == EOS_ID:\n",
        "                    finished.append((score, seq))\n",
        "                    continue\n",
        "                logits, dec_hc_n, ctx_n, _ = model.decoder(y_prev, dec_hc, enc_outputs, enc_mask, ctx)\n",
        "                logp = torch.log_softmax(logits, dim=-1).squeeze(0)  # [V]\n",
        "                topk = torch.topk(logp, k=beam_size)\n",
        "                for k in range(beam_size):\n",
        "                    tok = int(topk.indices[k])\n",
        "                    sc = score - float(topk.values[k])  # negative log-prob\n",
        "                    new_beams.append((sc, seq+[tok], dec_hc_n, ctx_n, torch.tensor([tok], device=src.device)))\n",
        "            # keep top-K\n",
        "            beams = heapq.nsmallest(beam_size, new_beams, key=lambda x: x[0])\n",
        "            # early stop if we already have K finished with EOS\n",
        "            if len(finished) >= beam_size and all(seq and seq[-1]==EOS_ID for _,seq in finished[:beam_size]):\n",
        "                break\n",
        "\n",
        "        cand = finished if finished else beams\n",
        "        # length penalty: favor slightly longer but reasonable sequences\n",
        "        cand = [(sc / (len(seq) ** length_penalty if len(seq)>0 else 1.0), seq) for sc, seq in cand]\n",
        "        best = min(cand, key=lambda x: x[0])[1]\n",
        "        return best\n",
        "\n",
        "# --- Public functions ---\n",
        "def translate_one(text, decoder=\"greedy\", max_len=None, beam_size=5):\n",
        "    ids = enc_src(text)\n",
        "    if max_len is None:\n",
        "        max_len = max(30, len(ids) + 10)  # safe cap\n",
        "    src_tensor = torch.tensor(ids, dtype=torch.long, device=DEVICE)\n",
        "\n",
        "    if decoder == \"beam\":\n",
        "        out = beam_search(model, src_tensor, beam_size=beam_size, max_len=max_len)\n",
        "        return dec_tgt(out)\n",
        "    else:\n",
        "        batch = {\n",
        "            \"src_ids\": src_tensor.unsqueeze(0),\n",
        "            \"tgt_ids\": torch.tensor([[BOS_ID, EOS_ID]], dtype=torch.long, device=DEVICE)\n",
        "        }\n",
        "        gen = greedy_decode(model, batch, max_len=max_len)\n",
        "        return dec_tgt(gen[0].tolist())\n",
        "\n",
        "def translate_batch(texts, decoder=\"greedy\", max_len=None, beam_size=5):\n",
        "    outs = []\n",
        "    for t in texts:\n",
        "        outs.append(translate_one(t, decoder=decoder, max_len=max_len, beam_size=beam_size))\n",
        "    return outs\n",
        "\n",
        "print(\"✅ Inference ready. Example:\")\n",
        "print(\"UR  :\", \"مزے جہان کے اپنی نظر میں خاک نہیں\")\n",
        "print(\"ROM :\", translate_one(\"مزے جہان کے اپنی نظر میں خاک نہیں\", decoder=\"beam\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vmULxZDltkOk",
        "outputId": "9349cf3f-5c44-4f24-dd64-4c8f3b02b01f"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using ckpt: bilstm4lstm_char_E256_H256_enc2_dec4_drop0.3_best.pt | tokenization=char\n",
            "✅ Inference ready. Example:\n",
            "UR  : مزے جہان کے اپنی نظر میں خاک نہیں\n",
            "ROM : maze jahān ke apnī nazar meñ ḳhaak nahīñ\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Cell #8 (fixed) — Batch translate & export CSV\n",
        "#   * Robust beam search tuples (score, seq, h_c, ctx)\n",
        "#   * Works with tokenization=\"char\" or \"spm\"\n",
        "#   * Saves /content/nmt_urdu_roman/runs/preds_test.csv\n",
        "# ============================================================\n",
        "import os, json, math\n",
        "from pathlib import Path\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import pandas as pd\n",
        "\n",
        "# --------- Common constants / device ---------\n",
        "PAD_ID = 0; BOS_ID = 1; EOS_ID = 2; UNK_ID = 3\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# --------- Paths and config ---------\n",
        "cfg = json.load(open(\"/content/nmt_urdu_roman/project_config.json\",\"r\",encoding=\"utf-8\"))\n",
        "PROJECT_DIR = Path(cfg[\"project_dir\"])\n",
        "ARTI        = PROJECT_DIR / \"artifacts\"\n",
        "DATA_DIR    = PROJECT_DIR / \"data\"\n",
        "RUNS_DIR    = PROJECT_DIR / \"runs\"\n",
        "RUNS_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# --------- Helper: load checkpoint / model / tokenizers ---------\n",
        "def load_model_and_tok(ckpt_path=None):\n",
        "    # Try to reuse globals if they exist\n",
        "    global best_model, exp_used\n",
        "    if ckpt_path is None:\n",
        "        # Pick the best *.pt in models dir if not given\n",
        "        models = sorted((PROJECT_DIR / \"models\").glob(\"*.pt\"), key=os.path.getmtime, reverse=True)\n",
        "        assert models, \"No checkpoints found in /models. Train first.\"\n",
        "        ckpt_path = models[0]\n",
        "\n",
        "    ckpt = torch.load(ckpt_path, map_location=DEVICE)\n",
        "    exp_local = ckpt[\"exp\"]\n",
        "    tokenization = ckpt.get(\"tokenization\", exp_local.get(\"tokenization\", \"char\"))\n",
        "\n",
        "    # Build model skeleton (reuse class from Cell #5)\n",
        "    assert \"Seq2Seq\" in globals(), \"Seq2Seq not found. Re-run Cell #5.\"\n",
        "    # Get vocab sizes\n",
        "    if tokenization == \"char\":\n",
        "        v_src = json.load(open(ARTI / \"vocab_src_char.json\",\"r\",encoding=\"utf-8\"))\n",
        "        v_tgt = json.load(open(ARTI / \"vocab_tgt_char.json\",\"r\",encoding=\"utf-8\"))\n",
        "        src_v = len(v_src[\"itos\"]); tgt_v = len(v_tgt[\"itos\"])\n",
        "    else:\n",
        "        import sentencepiece as spm\n",
        "        sp_src = spm.SentencePieceProcessor(); sp_src.load(str(ARTI / \"spm_src.model\"))\n",
        "        sp_tgt = spm.SentencePieceProcessor(); sp_tgt.load(str(ARTI / \"spm_tgt.model\"))\n",
        "        src_v = sp_src.get_piece_size(); tgt_v = sp_tgt.get_piece_size()\n",
        "\n",
        "    model = Seq2Seq(src_v, tgt_v,\n",
        "                    exp_local[\"embedding_dim\"], exp_local[\"hidden_size\"],\n",
        "                    exp_local[\"enc_layers\"], exp_local[\"dec_layers\"],\n",
        "                    exp_local[\"dropout\"]).to(DEVICE)\n",
        "    model.load_state_dict(ckpt[\"model_state\"])\n",
        "    model.eval()\n",
        "\n",
        "    # Tokenizers & decode from Cell #5 utility\n",
        "    assert \"_load_tokenizers\" in globals(), \"_load_tokenizers not found. Re-run Cell #5.\"\n",
        "    tok = _load_tokenizers(tokenization)\n",
        "\n",
        "    # For char-mode encoding we need src stoi\n",
        "    if tokenization == \"char\":\n",
        "        v_src = json.load(open(ARTI / \"vocab_src_char.json\",\"r\",encoding=\"utf-8\"))\n",
        "        stoi_src = v_src[\"stoi\"]\n",
        "    else:\n",
        "        stoi_src = None  # we’ll use tok[\"sp_src\"]\n",
        "\n",
        "    return model, exp_local, tok, stoi_src, ckpt_path\n",
        "\n",
        "best_model, exp_used, tok, stoi_src_char, used_ckpt = load_model_and_tok()\n",
        "print(f\"Using ckpt: {Path(used_ckpt).name} | tokenization={exp_used['tokenization']}\")\n",
        "\n",
        "# --------- Encoding / decoding helpers ---------\n",
        "def encode_src_text(s: str, tokenization: str):\n",
        "    if tokenization == \"char\":\n",
        "        ids = [BOS_ID] + [stoi_src_char.get(ch, UNK_ID) for ch in s] + [EOS_ID]\n",
        "    else:\n",
        "        ids = [BOS_ID] + tok[\"sp_src\"].encode(s, out_type=int) + [EOS_ID]\n",
        "    t = torch.tensor(ids, dtype=torch.long, device=DEVICE).unsqueeze(0)  # [1,T]\n",
        "    return t\n",
        "\n",
        "def decode_ids(ids: torch.Tensor):\n",
        "    # ids: [L] on cpu or gpu\n",
        "    arr = ids.tolist() if isinstance(ids, torch.Tensor) else ids\n",
        "    return tok[\"decode\"](arr)\n",
        "\n",
        "# --------- Greedy (quick) ---------\n",
        "def greedy_one(model, src_ids, max_len=200):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        enc_out, (hn, cn) = model.encoder(src_ids)\n",
        "        enc_mask = (src_ids == PAD_ID)\n",
        "        dec_hc = model.bridge(hn, cn)\n",
        "        ctx = torch.zeros(src_ids.size(0), enc_out.size(-1), device=src_ids.device)\n",
        "        y_prev = torch.full((src_ids.size(0),), BOS_ID, dtype=torch.long, device=src_ids.device)\n",
        "        outs = []\n",
        "        for _ in range(max_len):\n",
        "            logits, dec_hc, ctx, _ = model.decoder(y_prev, dec_hc, enc_out, enc_mask, ctx)\n",
        "            y_prev = torch.argmax(logits, dim=-1)\n",
        "            outs.append(y_prev)\n",
        "            if (y_prev == EOS_ID).all(): break\n",
        "        return torch.stack(outs, dim=1).squeeze(0)  # [L]\n",
        "\n",
        "# --------- Beam search (fixed tuples) ---------\n",
        "def beam_one(model, src_ids, beam_size=5, max_len=200, length_penalty=0.8):\n",
        "    \"\"\"\n",
        "    Returns best seq ids (Tensor [L]).\n",
        "    Tuples in beams/finished are (neg_logprob, seq_list, (h,c), ctx)\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        # Encode once\n",
        "        enc_out, (hn, cn) = model.encoder(src_ids)\n",
        "        enc_mask = (src_ids == PAD_ID)\n",
        "        dec_hc0 = model.bridge(hn, cn)\n",
        "        ctx0 = torch.zeros(src_ids.size(0), enc_out.size(-1), device=src_ids.device)\n",
        "\n",
        "        # Init beams\n",
        "        beams = [(0.0, [BOS_ID], dec_hc0, ctx0)]  # negative log-prob score\n",
        "        finished = []\n",
        "\n",
        "        for _ in range(max_len):\n",
        "            new_beams = []\n",
        "            for score, seq, dec_hc, ctx in beams:\n",
        "                last = seq[-1]\n",
        "                if last == EOS_ID:\n",
        "                    finished.append((score, seq, dec_hc, ctx))\n",
        "                    continue\n",
        "                y_prev = torch.tensor([last], dtype=torch.long, device=src_ids.device)\n",
        "                logits, dec_hc_new, ctx_new, _ = model.decoder(y_prev, dec_hc, enc_out, enc_mask, ctx)\n",
        "                logp = F.log_softmax(logits, dim=-1).squeeze(0)  # [V]\n",
        "                topk = torch.topk(logp, beam_size)\n",
        "                for k in range(beam_size):\n",
        "                    tok_id = int(topk.indices[k].item())\n",
        "                    sc = float(-topk.values[k].item())  # negative log prob\n",
        "                    new_seq = seq + [tok_id]\n",
        "                    new_beams.append((score + sc, new_seq, dec_hc_new, ctx_new))\n",
        "\n",
        "            # prune\n",
        "            new_beams.sort(key=lambda x: x[0])\n",
        "            beams = new_beams[:beam_size]\n",
        "\n",
        "            # early stop if all finished\n",
        "            if len(finished) >= beam_size and all(b[-1] == EOS_ID for _, b, _, _ in finished[-beam_size:]):\n",
        "                break\n",
        "\n",
        "        # Candidate pool: prefer finished, else current beams\n",
        "        cand = finished if finished else beams\n",
        "\n",
        "        # Apply length penalty on a view with only (score, seq)\n",
        "        normed = []\n",
        "        for sc, seq, _, _ in cand:\n",
        "            L = max(1, len(seq))\n",
        "            normed.append((sc / (L ** length_penalty), seq))\n",
        "        best_seq = min(normed, key=lambda x: x[0])[1]\n",
        "\n",
        "        # strip leading BOS, truncate at EOS\n",
        "        if best_seq and best_seq[0] == BOS_ID:\n",
        "            best_seq = best_seq[1:]\n",
        "        if EOS_ID in best_seq:\n",
        "            best_seq = best_seq[:best_seq.index(EOS_ID)]\n",
        "        return torch.tensor(best_seq, dtype=torch.long)\n",
        "\n",
        "# --------- Public helpers ---------\n",
        "def translate_one(urdu_text: str, decoder=\"beam\", beam_size=5, max_len=200):\n",
        "    src_ids = encode_src_text(urdu_text, exp_used[\"tokenization\"])\n",
        "    if decoder == \"greedy\":\n",
        "        out = greedy_one(best_model, src_ids, max_len=max_len)\n",
        "    else:\n",
        "        out = beam_one(best_model, src_ids, beam_size=beam_size, max_len=max_len)\n",
        "    return decode_ids(out)\n",
        "\n",
        "def translate_batch(texts, decoder=\"beam\", beam_size=5, max_len=200):\n",
        "    outs = []\n",
        "    for s in texts:\n",
        "        outs.append(translate_one(str(s), decoder=decoder, beam_size=beam_size, max_len=max_len))\n",
        "    return outs\n",
        "\n",
        "# --------- Run on test split (or your own list) and export ---------\n",
        "test_df = pd.read_parquet(DATA_DIR / \"pairs_test.parquet\")[[\"src_ur\",\"tgt_rom\"]].copy()\n",
        "\n",
        "# Example for custom list instead:\n",
        "# custom = [\"مزے جہان کے اپنی نظر میں خاک نہیں\", \"اب کے ہم بچھڑے تو شاید کبھی خوابوں میں ملیں\"]\n",
        "# test_df = pd.DataFrame({\"src_ur\": custom, \"tgt_rom\": [\"\"]*len(custom)})\n",
        "\n",
        "preds = translate_batch(test_df[\"src_ur\"].tolist(), decoder=\"beam\", beam_size=5)\n",
        "out = test_df.assign(pred=preds)\n",
        "\n",
        "csv_path = RUNS_DIR / \"preds_test.csv\"\n",
        "out.to_csv(csv_path, index=False)\n",
        "print(\"✅ Saved:\", csv_path)\n",
        "out.head(10)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 591
        },
        "id": "fEtkl5s-uHP2",
        "outputId": "693313f1-3acf-4482-ffb0-c26c048142b8"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using ckpt: bilstm4lstm_char_E256_H256_enc2_dec4_drop0.3_best.pt | tokenization=char\n",
            "✅ Saved: /content/nmt_urdu_roman/runs/preds_test.csv\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                    src_ur  \\\n",
              "0    عاشقی میں میرؔ جیسے خواب مت دیکہا کرو   \n",
              "1       باولے ہو جاو گے مہتاب مت دیکہا کرو   \n",
              "2        جستہ جستہ پڑہ لیا کرنا مضامین وفا   \n",
              "3       پر کتاب عشق کا ہر باب مت دیکہا کرو   \n",
              "4    اس تماشے میں الٹ جاتی ہیں اکثر کشتیاں   \n",
              "5       ڈوبنے والوں کو زیر آب مت دیکہا کرو   \n",
              "6  مے کدے میں کیا تکلف مے کشی میں کیا حجاب   \n",
              "7       بزم ساقی میں ادب آداب مت دیکہا کرو   \n",
              "8  ہم سے درویشوں کے گہر آو تو یاروں کی طرح   \n",
              "9      ہر جگہ خس خانہ و برفاب مت دیکہا کرو   \n",
              "\n",
              "                                             tgt_rom  \\\n",
              "0        āshiqī meñ 'mīr' jaise ḳhvāb mat dekhā karo   \n",
              "1               bāvle ho jāoge mahtāb mat dekhā karo   \n",
              "2         jasta jasta paḌh liyā karnā mazāmīn-e-vafā   \n",
              "3        par kitāb-e-ishq kā har baab mat dekhā karo   \n",
              "4     is tamāshe meñ ulaT jaatī haiñ aksar kashtiyāñ   \n",
              "5             Dūbne vāloñ ko zer-e-āb mat dekhā karo   \n",
              "6  mai-kade meñ kyā takalluf mai-kashī meñ kyā hijāb   \n",
              "7           bazm-e-sāqī meñ adab ādāb mat dekhā karo   \n",
              "8     ham se durveshoñ ke ghar aao to yāroñ kī tarah   \n",
              "9       har jagah ḳhas-ḳhāna o barfāb mat dekhā karo   \n",
              "\n",
              "                                                pred  \n",
              "0        āshiqī meñ 'mīr' jaise ḳhvāb mat dekhā karo  \n",
              "1              bāvale ho jāoge mahtāb mat dekhā karo  \n",
              "2         jasta jasta paḌh liyā karnā mazāmīn-e-vafā  \n",
              "3        par kitāb-e-ishq kā har baab mat dekhā karo  \n",
              "4     is tamāshe meñ ulaT jaatī haiñ aksar kushtiyāñ  \n",
              "5             Dūbne vāloñ ko zer-e-āb mat dekhā karo  \n",
              "6  mai-kade meñ kyā takalluf-e-mai-kashī meñ kyā ...  \n",
              "7           bazm-e-sāqī meñ adab ādāb mat dekhā karo  \n",
              "8     ham se darveshoñ ke ghar aao to yāroñ kī tarah  \n",
              "9       har jagah ḳhas-ḳhāna-o-barfāb mat dekhā karo  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e903265f-8d83-4795-bfb3-66eeb4d3e56a\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>src_ur</th>\n",
              "      <th>tgt_rom</th>\n",
              "      <th>pred</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>عاشقی میں میرؔ جیسے خواب مت دیکہا کرو</td>\n",
              "      <td>āshiqī meñ 'mīr' jaise ḳhvāb mat dekhā karo</td>\n",
              "      <td>āshiqī meñ 'mīr' jaise ḳhvāb mat dekhā karo</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>باولے ہو جاو گے مہتاب مت دیکہا کرو</td>\n",
              "      <td>bāvle ho jāoge mahtāb mat dekhā karo</td>\n",
              "      <td>bāvale ho jāoge mahtāb mat dekhā karo</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>جستہ جستہ پڑہ لیا کرنا مضامین وفا</td>\n",
              "      <td>jasta jasta paḌh liyā karnā mazāmīn-e-vafā</td>\n",
              "      <td>jasta jasta paḌh liyā karnā mazāmīn-e-vafā</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>پر کتاب عشق کا ہر باب مت دیکہا کرو</td>\n",
              "      <td>par kitāb-e-ishq kā har baab mat dekhā karo</td>\n",
              "      <td>par kitāb-e-ishq kā har baab mat dekhā karo</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>اس تماشے میں الٹ جاتی ہیں اکثر کشتیاں</td>\n",
              "      <td>is tamāshe meñ ulaT jaatī haiñ aksar kashtiyāñ</td>\n",
              "      <td>is tamāshe meñ ulaT jaatī haiñ aksar kushtiyāñ</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>ڈوبنے والوں کو زیر آب مت دیکہا کرو</td>\n",
              "      <td>Dūbne vāloñ ko zer-e-āb mat dekhā karo</td>\n",
              "      <td>Dūbne vāloñ ko zer-e-āb mat dekhā karo</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>مے کدے میں کیا تکلف مے کشی میں کیا حجاب</td>\n",
              "      <td>mai-kade meñ kyā takalluf mai-kashī meñ kyā hijāb</td>\n",
              "      <td>mai-kade meñ kyā takalluf-e-mai-kashī meñ kyā ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>بزم ساقی میں ادب آداب مت دیکہا کرو</td>\n",
              "      <td>bazm-e-sāqī meñ adab ādāb mat dekhā karo</td>\n",
              "      <td>bazm-e-sāqī meñ adab ādāb mat dekhā karo</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>ہم سے درویشوں کے گہر آو تو یاروں کی طرح</td>\n",
              "      <td>ham se durveshoñ ke ghar aao to yāroñ kī tarah</td>\n",
              "      <td>ham se darveshoñ ke ghar aao to yāroñ kī tarah</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>ہر جگہ خس خانہ و برفاب مت دیکہا کرو</td>\n",
              "      <td>har jagah ḳhas-ḳhāna o barfāb mat dekhā karo</td>\n",
              "      <td>har jagah ḳhas-ḳhāna-o-barfāb mat dekhā karo</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e903265f-8d83-4795-bfb3-66eeb4d3e56a')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-e903265f-8d83-4795-bfb3-66eeb4d3e56a button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-e903265f-8d83-4795-bfb3-66eeb4d3e56a');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-e7c4cf32-13a5-4d21-b04c-ef14a2cad53a\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-e7c4cf32-13a5-4d21-b04c-ef14a2cad53a')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-e7c4cf32-13a5-4d21-b04c-ef14a2cad53a button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "out",
              "summary": "{\n  \"name\": \"out\",\n  \"rows\": 5255,\n  \"fields\": [\n    {\n      \"column\": \"src_ur\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5255,\n        \"samples\": [\n          \"\\u0632\\u06c1\\u0631 \\u0627\\u06af\\u0644\\u062a\\u06d2 \\u06c1\\u06cc\\u06ba \\u062c\\u0628 \\u0645\\u0644 \\u06a9\\u0631 \\u062f\\u0646\\u06cc\\u0627 \\u0648\\u0627\\u0644\\u06d2\",\n          \"\\u0632\\u0648\\u0631 \\u0646\\u0633\\u0628\\u062a \\u0645\\u06d2 \\u0633\\u06d2 \\u0631\\u06a9\\u06c1\\u062a\\u0627 \\u06c1\\u06d2 \\u0627\\u0636\\u0627\\u0631\\u0627 \\u06a9\\u0627 \\u0646\\u0645\\u06a9\",\n          \"\\u06a9\\u0628\\u06c1\\u06cc \\u067e\\u0631\\u06cc \\u0645\\u0631\\u06cc \\u062e\\u0644\\u0648\\u062a \\u0645\\u06cc\\u06ba \\u0622 \\u0646\\u06a9\\u0644\\u062a\\u06cc \\u06c1\\u06d2\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"tgt_rom\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5255,\n        \"samples\": [\n          \"zahr ugalte hai\\u00f1 jab mil kar duniy\\u0101 vaale\",\n          \"zor nisbat mai se rakht\\u0101 hai az\\u0101r\\u0101 k\\u0101 namak\",\n          \"kabh\\u012b par\\u012b mir\\u012b \\u1e33halvat me\\u00f1 aa nikalt\\u012b hai\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"pred\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5255,\n        \"samples\": [\n          \"zahr ugalte hai\\u00f1 jab mil kar duniy\\u0101 vaale\",\n          \"zor-e-nisbat mai se rakht\\u0101 hai az\\u0101r\\u0101 k\\u0101 namak\",\n          \"kabh\\u012b par\\u012b mir\\u012b \\u1e33halvat me\\u00f1 aa nikalt\\u012b hai\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Cell #9 — Quick error analysis + light post-processing\n",
        "#   - Shows top confusions\n",
        "#   - Normalizes hyphens/ezāfe/wa and a couple frequent typos\n",
        "#   - Re-scores BLEU/CER after cleanup and saves new CSV\n",
        "# ============================================================\n",
        "import re, pandas as pd, numpy as np, sacrebleu\n",
        "from jiwer import cer as jiwer_cer\n",
        "from pathlib import Path\n",
        "\n",
        "PROJECT_DIR = Path(\"/content/nmt_urdu_roman\")\n",
        "RUNS_DIR    = PROJECT_DIR / \"runs\"\n",
        "DATA_DIR    = PROJECT_DIR / \"data\"\n",
        "\n",
        "df = pd.read_csv(RUNS_DIR / \"preds_test.csv\", sep=\",\")\n",
        "print(\"Rows:\", len(df))\n",
        "\n",
        "# ---- quick error peek\n",
        "def token_diff_rows(df, n=10):\n",
        "    rows = []\n",
        "    for i, r in df.iterrows():\n",
        "        gt = str(r[\"tgt_rom\"]).split()\n",
        "        pr = str(r[\"pred\"]).split()\n",
        "        if gt != pr:\n",
        "            rows.append((i, r[\"src_ur\"], r[\"tgt_rom\"], r[\"pred\"]))\n",
        "        if len(rows) >= n: break\n",
        "    return rows\n",
        "\n",
        "print(\"\\nExamples with diffs (first 10):\")\n",
        "for i, ur, gt, pr in token_diff_rows(df, n=10):\n",
        "    print(f\"- {i:04d} UR: {ur}\\n   GT: {gt}\\n   PR: {pr}\\n\")\n",
        "\n",
        "# ---- light post-processing\n",
        "EZAFE = r\"(?:\\s*-\\s*e\\s*-\\s*|\\s+e\\s+|\\s*e\\s*-\\s*|\\s*-\\s*e\\s*)\"\n",
        "WA    = r\"(?:\\s*-\\s*o\\s*-\\s*|\\s+o\\s+|\\s*o\\s*-\\s*|\\s*-\\s*o\\s*)\"\n",
        "\n",
        "def tidy_roman(s: str) -> str:\n",
        "    if not isinstance(s, str): return \"\"\n",
        "    t = s\n",
        "\n",
        "    # unify ezāfe and wa to \"-e-\" / \"-o-\"\n",
        "    t = re.sub(EZAFE, \"-e-\", t)\n",
        "    t = re.sub(WA, \"-o-\", t)\n",
        "\n",
        "    # collapse multiple hyphens/spaces around hyphens\n",
        "    t = re.sub(r\"\\s*-\\s*\", \"-\", t)\n",
        "    t = re.sub(r\"\\s{2,}\", \" \", t).strip()\n",
        "\n",
        "    # common char fixes\n",
        "    # kashti vs kushti (if your ground truth prefers 'kashtiyāñ')\n",
        "    t = re.sub(r\"\\bkushti(yāñ|yān|yā|yān?)\\b\", r\"kashti\\1\", t, flags=re.I)\n",
        "\n",
        "    # normalize dotted 'ḳhaak' variants -> 'ḳhaak' / 'khāk' style harmonization is repo-specific; skip heavy changes\n",
        "    # small punctuation spacing\n",
        "    t = re.sub(r\"\\s+([,؛۔?!])\", r\"\\1\", t)\n",
        "\n",
        "    return t\n",
        "\n",
        "df[\"pred_clean\"] = df[\"pred\"].map(tidy_roman)\n",
        "\n",
        "# ---- score before/after\n",
        "preds_raw   = df[\"pred\"].tolist()\n",
        "preds_clean = df[\"pred_clean\"].tolist()\n",
        "refs        = df[\"tgt_rom\"].tolist()\n",
        "\n",
        "bleu_raw   = sacrebleu.corpus_bleu(preds_raw,   [refs]).score\n",
        "bleu_clean = sacrebleu.corpus_bleu(preds_clean, [refs]).score\n",
        "cer_raw    = float(np.mean([jiwer_cer(r, p) for p, r in zip(preds_raw, refs)]))\n",
        "cer_clean  = float(np.mean([jiwer_cer(r, p) for p, r in zip(preds_clean, refs)]))\n",
        "\n",
        "print(f\"\\nBLEU raw   : {bleu_raw:.2f} | CER raw   : {cer_raw:.3f}\")\n",
        "print(f\"BLEU clean : {bleu_clean:.2f} | CER clean : {cer_clean:.3f}\")\n",
        "\n",
        "# ---- save cleaned file\n",
        "out_path = RUNS_DIR / \"preds_test_clean.csv\"\n",
        "df[[\"src_ur\",\"tgt_rom\",\"pred_clean\"]].to_csv(out_path, index=False)\n",
        "print(\"✅ Saved cleaned preds:\", out_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a1itu53BuY98",
        "outputId": "dc92a261-b16e-44bd-e9df-ff3f47c58056"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rows: 5255\n",
            "\n",
            "Examples with diffs (first 10):\n",
            "- 0001 UR: باولے ہو جاو گے مہتاب مت دیکہا کرو\n",
            "   GT: bāvle ho jāoge mahtāb mat dekhā karo\n",
            "   PR: bāvale ho jāoge mahtāb mat dekhā karo\n",
            "\n",
            "- 0004 UR: اس تماشے میں الٹ جاتی ہیں اکثر کشتیاں\n",
            "   GT: is tamāshe meñ ulaT jaatī haiñ aksar kashtiyāñ\n",
            "   PR: is tamāshe meñ ulaT jaatī haiñ aksar kushtiyāñ\n",
            "\n",
            "- 0006 UR: مے کدے میں کیا تکلف مے کشی میں کیا حجاب\n",
            "   GT: mai-kade meñ kyā takalluf mai-kashī meñ kyā hijāb\n",
            "   PR: mai-kade meñ kyā takalluf-e-mai-kashī meñ kyā hijāb\n",
            "\n",
            "- 0008 UR: ہم سے درویشوں کے گہر آو تو یاروں کی طرح\n",
            "   GT: ham se durveshoñ ke ghar aao to yāroñ kī tarah\n",
            "   PR: ham se darveshoñ ke ghar aao to yāroñ kī tarah\n",
            "\n",
            "- 0009 UR: ہر جگہ خس خانہ و برفاب مت دیکہا کرو\n",
            "   GT: har jagah ḳhas-ḳhāna o barfāb mat dekhā karo\n",
            "   PR: har jagah ḳhas-ḳhāna-o-barfāb mat dekhā karo\n",
            "\n",
            "- 0010 UR: مانگے تانگے کی قباییں دیر تک رہتی نہیں\n",
            "   GT: māñge-tāñge kī qabā.eñ der tak rahtī nahīñ\n",
            "   PR: māñge tāñge kī qabā.eñ der tah rahtī nahīñ\n",
            "\n",
            "- 0011 UR: یار لوگوں کے لقب القاب مت دیکہا کرو\n",
            "   GT: yaar logoñ ke laqab-alqāb mat dekhā karo\n",
            "   PR: yaar logoñ ke laqb-e-ulqāb mat dekhā karo\n",
            "\n",
            "- 0012 UR: تشنگی میں لب بہگو لینا بہی کافی ہے فرازؔ\n",
            "   GT: tishnagī meñ lab bhigo lenā bhī kaafī hai 'farāz'\n",
            "   PR: tishnagī meñ lab bhugū lenā bhī kaafī hai 'farāz'\n",
            "\n",
            "- 0016 UR: یوں ہی موسم کی ادا دیکہ کے یاد آیا ہے\n",
            "   GT: yūñhī mausam kī adā dekh ke yaad aayā hai\n",
            "   PR: yuuñ hī mausam kī adā dekh ke yaad aayā hai\n",
            "\n",
            "- 0020 UR: دل یہ کہتا ہے کہ شاید ہے فسردہ تو بہی\n",
            "   GT: dil ye kahtā hai ki shāyad hai fasurda tū bhī\n",
            "   PR: dil ye kahtā hai ki shāyad hai fasrda to bhī\n",
            "\n",
            "\n",
            "BLEU raw   : 74.89 | CER raw   : 0.048\n",
            "BLEU clean : 74.64 | CER clean : 0.049\n",
            "✅ Saved cleaned preds: /content/nmt_urdu_roman/runs/preds_test_clean.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# If not already mounted:\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# Copy folders into Drive\n",
        "!cp -r /content/nmt_urdu_roman /content/drive/MyDrive/\n",
        "!cp -r /content/urdu_ghazals_rekhta /content/drive/MyDrive/\n",
        "\n",
        "print(\"Saved to MyDrive/nmt_urdu_roman and MyDrive/urdu_ghazals_rekhta\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j7-8yu1qzL5n",
        "outputId": "dccfab4f-e6ff-4308-fb7f-b457b6bb308a"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Saved to MyDrive/nmt_urdu_roman and MyDrive/urdu_ghazals_rekhta\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xDsKSMUu3mYT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}